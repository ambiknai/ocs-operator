
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>ocsinitialization: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/openshift/ocs-operator/controllers/ocsinitialization/ocsinitialization_controller.go (73.5%)</option>
				
				<option value="file1">github.com/openshift/ocs-operator/controllers/ocsinitialization/sccs.go (92.0%)</option>
				
				<option value="file2">github.com/openshift/ocs-operator/controllers/persistentvolume/persistentvolume_controller.go (61.1%)</option>
				
				<option value="file3">github.com/openshift/ocs-operator/controllers/persistentvolume/reconcile.go (75.6%)</option>
				
				<option value="file4">github.com/openshift/ocs-operator/controllers/storagecluster/cephblockpools.go (60.0%)</option>
				
				<option value="file5">github.com/openshift/ocs-operator/controllers/storagecluster/cephcluster.go (77.6%)</option>
				
				<option value="file6">github.com/openshift/ocs-operator/controllers/storagecluster/cephfilesystem.go (65.5%)</option>
				
				<option value="file7">github.com/openshift/ocs-operator/controllers/storagecluster/cephobjectstores.go (74.0%)</option>
				
				<option value="file8">github.com/openshift/ocs-operator/controllers/storagecluster/cephobjectstoreusers.go (76.8%)</option>
				
				<option value="file9">github.com/openshift/ocs-operator/controllers/storagecluster/exporter.go (82.6%)</option>
				
				<option value="file10">github.com/openshift/ocs-operator/controllers/storagecluster/external_resources.go (73.7%)</option>
				
				<option value="file11">github.com/openshift/ocs-operator/controllers/storagecluster/generate.go (100.0%)</option>
				
				<option value="file12">github.com/openshift/ocs-operator/controllers/storagecluster/kms_resources.go (81.0%)</option>
				
				<option value="file13">github.com/openshift/ocs-operator/controllers/storagecluster/noobaa_system_reconciler.go (65.7%)</option>
				
				<option value="file14">github.com/openshift/ocs-operator/controllers/storagecluster/placement.go (91.1%)</option>
				
				<option value="file15">github.com/openshift/ocs-operator/controllers/storagecluster/platform_detection.go (62.5%)</option>
				
				<option value="file16">github.com/openshift/ocs-operator/controllers/storagecluster/prometheus.go (57.4%)</option>
				
				<option value="file17">github.com/openshift/ocs-operator/controllers/storagecluster/quickstarts.go (54.4%)</option>
				
				<option value="file18">github.com/openshift/ocs-operator/controllers/storagecluster/reconcile.go (72.7%)</option>
				
				<option value="file19">github.com/openshift/ocs-operator/controllers/storagecluster/storageclasses.go (69.7%)</option>
				
				<option value="file20">github.com/openshift/ocs-operator/controllers/storagecluster/storagecluster_controller.go (20.0%)</option>
				
				<option value="file21">github.com/openshift/ocs-operator/controllers/storagecluster/topology.go (92.2%)</option>
				
				<option value="file22">github.com/openshift/ocs-operator/controllers/storagecluster/uninstall_reconciler.go (73.1%)</option>
				
				<option value="file23">github.com/openshift/ocs-operator/controllers/storagecluster/volumesnapshotterclasses.go (66.7%)</option>
				
				<option value="file24">github.com/openshift/ocs-operator/controllers/util/k8sutil.go (0.0%)</option>
				
				<option value="file25">github.com/openshift/ocs-operator/controllers/util/predicates.go (36.8%)</option>
				
				<option value="file26">github.com/openshift/ocs-operator/controllers/util/ready.go (0.0%)</option>
				
				<option value="file27">github.com/openshift/ocs-operator/controllers/util/status.go (0.0%)</option>
				
				<option value="file28">github.com/openshift/ocs-operator/metrics/internal/collectors/ceph-object-store.go (55.9%)</option>
				
				<option value="file29">github.com/openshift/ocs-operator/metrics/internal/collectors/registry.go (0.0%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package ocsinitialization

import (
        "context"
        "fmt"
        "os"
        "reflect"

        "github.com/go-logr/logr"
        secv1client "github.com/openshift/client-go/security/clientset/versioned/typed/security/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        "github.com/openshift/ocs-operator/controllers/util"
        appsv1 "k8s.io/api/apps/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/types"
        ctrl "sigs.k8s.io/controller-runtime"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/reconcile"
)

// watchNamespace is the namespace the operator is watching.
var watchNamespace string

const wrongNamespacedName = "Ignoring this resource. Only one should exist, and this one has the wrong name and/or namespace."

const (
        rookCephToolDeploymentName = "rook-ceph-tools"
        // This name is predefined by Rook
        rookCephOperatorConfigName = "rook-ceph-operator-config"
)

// InitNamespacedName returns a NamespacedName for the singleton instance that
// should exist.
func InitNamespacedName() types.NamespacedName <span class="cov8" title="1">{
        return types.NamespacedName{
                Name:      "ocsinit",
                Namespace: watchNamespace,
        }
}</span>

// OCSInitializationReconciler reconciles a OCSInitialization object
//nolint
type OCSInitializationReconciler struct {
        client.Client
        Log            logr.Logger
        Scheme         *runtime.Scheme
        SecurityClient secv1client.SecurityV1Interface
        RookImage      string
}

func newToolsDeployment(namespace string, rookImage string) *appsv1.Deployment <span class="cov8" title="1">{

        name := rookCephToolDeploymentName
        var replicaOne int32 = 1

        privilegedContainer := true
        return &amp;appsv1.Deployment{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      name,
                        Namespace: namespace,
                },
                Spec: appsv1.DeploymentSpec{
                        Replicas: &amp;replicaOne,
                        Selector: &amp;metav1.LabelSelector{
                                MatchLabels: map[string]string{
                                        "app": "rook-ceph-tools",
                                },
                        },
                        Template: corev1.PodTemplateSpec{
                                ObjectMeta: metav1.ObjectMeta{
                                        Labels: map[string]string{
                                                "app": "rook-ceph-tools",
                                        },
                                },
                                Spec: corev1.PodSpec{
                                        DNSPolicy: corev1.DNSClusterFirstWithHostNet,
                                        Containers: []corev1.Container{
                                                {
                                                        Name:    name,
                                                        Image:   rookImage,
                                                        Command: []string{"/tini"},
                                                        Args:    []string{"-g", "--", "/usr/local/bin/toolbox.sh"},
                                                        Env: []corev1.EnvVar{
                                                                {
                                                                        Name: "ROOK_CEPH_USERNAME",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                SecretKeyRef: &amp;corev1.SecretKeySelector{
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon"},
                                                                                        Key:                  "ceph-username",
                                                                                },
                                                                        },
                                                                },
                                                                {
                                                                        Name: "ROOK_CEPH_SECRET",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                SecretKeyRef: &amp;corev1.SecretKeySelector{
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon"},
                                                                                        Key:                  "ceph-secret",
                                                                                },
                                                                        },
                                                                },
                                                        },
                                                        SecurityContext: &amp;corev1.SecurityContext{
                                                                Privileged: &amp;privilegedContainer,
                                                        },
                                                        VolumeMounts: []corev1.VolumeMount{
                                                                {Name: "dev", MountPath: "/dev"},
                                                                {Name: "sysbus", MountPath: "/sys/bus"},
                                                                {Name: "libmodules", MountPath: "/lib/modules"},
                                                                {Name: "mon-endpoint-volume", MountPath: "/etc/rook"},
                                                        },
                                                },
                                        },
                                        Tolerations: []corev1.Toleration{
                                                {
                                                        Key:      defaults.NodeTolerationKey,
                                                        Operator: corev1.TolerationOpEqual,
                                                        Value:    "true",
                                                        Effect:   corev1.TaintEffectNoSchedule,
                                                },
                                        },
                                        // if hostNetwork: false, the "rbd map" command hangs, see https://github.com/rook/rook/issues/2021
                                        HostNetwork: true,
                                        Volumes: []corev1.Volume{
                                                {Name: "dev", VolumeSource: corev1.VolumeSource{HostPath: &amp;corev1.HostPathVolumeSource{Path: "/dev"}}},
                                                {Name: "sysbus", VolumeSource: corev1.VolumeSource{HostPath: &amp;corev1.HostPathVolumeSource{Path: "/sys/bus"}}},
                                                {Name: "libmodules", VolumeSource: corev1.VolumeSource{HostPath: &amp;corev1.HostPathVolumeSource{Path: "/lib/modules"}}},
                                                {Name: "mon-endpoint-volume", VolumeSource: corev1.VolumeSource{
                                                        ConfigMap: &amp;corev1.ConfigMapVolumeSource{LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon-endpoints"},
                                                                Items: []corev1.KeyToPath{
                                                                        {Key: "data", Path: "mon-endpoints"},
                                                                },
                                                        },
                                                },
                                                },
                                        },
                                },
                        },
                },
        }
}</span>

func (r *OCSInitializationReconciler) ensureToolsDeployment(initialData *ocsv1.OCSInitialization) error <span class="cov8" title="1">{

        var isFound bool
        namespace := initialData.Namespace

        toolsDeployment := newToolsDeployment(namespace, r.RookImage)
        foundToolsDeployment := &amp;appsv1.Deployment{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: rookCephToolDeploymentName, Namespace: namespace}, foundToolsDeployment)

        if err == nil </span><span class="cov8" title="1">{
                isFound = true
        }</span> else<span class="cov8" title="1"> if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                isFound = false
        }</span> else<span class="cov0" title="0"> {
                return err
        }</span>

        <span class="cov8" title="1">if initialData.Spec.EnableCephTools </span><span class="cov8" title="1">{
                // Create or Update if ceph tools is enabled.

                if !isFound </span><span class="cov8" title="1">{
                        return r.Client.Create(context.TODO(), toolsDeployment)
                }</span> else<span class="cov8" title="1"> if !reflect.DeepEqual(foundToolsDeployment.Spec, toolsDeployment.Spec) </span><span class="cov8" title="1">{

                        updateDeployment := foundToolsDeployment.DeepCopy()
                        updateDeployment.Spec = *toolsDeployment.Spec.DeepCopy()

                        return r.Client.Update(context.TODO(), updateDeployment)
                }</span>
        } else<span class="cov8" title="1"> if isFound </span><span class="cov8" title="1">{
                // delete if ceph tools exists and is disabled
                return r.Client.Delete(context.TODO(), foundToolsDeployment)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// +kubebuilder:rbac:groups=ocs.openshift.io,resources=*,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=security.openshift.io,resources=securitycontextconstraints,verbs=get;create;update
// +kubebuilder:rbac:groups=security.openshift.io,resourceNames=privileged,resources=securitycontextconstraints,verbs=get;create;update

// Reconcile reads that state of the cluster for a OCSInitialization object and makes changes based on the state read
// and what is in the OCSInitialization.Spec
// The Controller will requeue the Request to be processed again if the returned error is non-nil or
// Result.Requeue is true, otherwise upon completion it will remove the work from the queue.
func (r *OCSInitializationReconciler) Reconcile(request reconcile.Request) (reconcile.Result, error) <span class="cov8" title="1">{

        prevLogger := r.Log
        defer func() </span><span class="cov8" title="1">{ r.Log = prevLogger }</span>()
        <span class="cov8" title="1">r.Log = r.Log.WithValues("Request.Namespace", request.Namespace, "Request.Name", request.Name)

        r.Log.Info("Reconciling OCSInitialization")

        initNamespacedName := InitNamespacedName()
        instance := &amp;ocsv1.OCSInitialization{}
        if initNamespacedName.Name != request.Name || initNamespacedName.Namespace != request.Namespace </span><span class="cov8" title="1">{
                // Ignoring this resource because it has the wrong name or namespace
                r.Log.Info(wrongNamespacedName)
                err := r.Client.Get(context.TODO(), request.NamespacedName, instance)
                if err != nil </span><span class="cov8" title="1">{
                        // the resource probably got deleted
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                return reconcile.Result{}, nil
                        }</span>
                        <span class="cov0" title="0">return reconcile.Result{}, err</span>
                }

                <span class="cov8" title="1">instance.Status.Phase = util.PhaseIgnored
                err = r.Client.Status().Update(context.TODO(), instance)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "failed to update ignored resource")
                }</span>
                <span class="cov8" title="1">return reconcile.Result{}, err</span>
        }

        // Fetch the OCSInitialization instance
        <span class="cov8" title="1">err := r.Client.Get(context.TODO(), request.NamespacedName, instance)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        // Request object not found, could have been deleted after reconcile request.
                        // Recreating since we depend on this to exist. A user may delete it to
                        // induce a reset of all initial data.
                        r.Log.Info("recreating OCSInitialization resource")
                        return reconcile.Result{}, r.Client.Create(context.TODO(), &amp;ocsv1.OCSInitialization{
                                ObjectMeta: metav1.ObjectMeta{
                                        Name:      initNamespacedName.Name,
                                        Namespace: initNamespacedName.Namespace,
                                },
                        })
                }</span>
                // Error reading the object - requeue the request.
                <span class="cov0" title="0">return reconcile.Result{}, err</span>
        }

        <span class="cov8" title="1">if instance.Status.Conditions == nil </span><span class="cov8" title="1">{
                reason := ocsv1.ReconcileInit
                message := "Initializing OCSInitialization resource"
                util.SetProgressingCondition(&amp;instance.Status.Conditions, reason, message)

                instance.Status.Phase = util.PhaseProgressing
                err = r.Client.Status().Update(context.TODO(), instance)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to add conditions to status")
                        return reconcile.Result{}, err
                }</span>
        }

        <span class="cov8" title="1">err = r.ensureSCCs(instance)
        if err != nil </span><span class="cov0" title="0">{
                reason := ocsv1.ReconcileFailed
                message := fmt.Sprintf("Error while reconciling: %v", err)
                util.SetErrorCondition(&amp;instance.Status.Conditions, reason, message)

                instance.Status.Phase = util.PhaseError
                // don't want to overwrite the actual reconcile failure
                uErr := r.Client.Status().Update(context.TODO(), instance)
                if uErr != nil </span><span class="cov0" title="0">{
                        r.Log.Error(uErr, "Failed to update conditions")
                }</span>
                <span class="cov0" title="0">return reconcile.Result{}, err</span>
        }
        <span class="cov8" title="1">instance.Status.SCCsCreated = true

        err = r.Client.Status().Update(context.TODO(), instance)
        if err != nil </span><span class="cov0" title="0">{
                return reconcile.Result{}, err
        }</span>

        <span class="cov8" title="1">err = r.ensureToolsDeployment(instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Failed to process ceph tools deployment")
                return reconcile.Result{}, err
        }</span>

        <span class="cov8" title="1">if !instance.Status.RookCephOperatorConfigCreated </span><span class="cov8" title="1">{
                // if true, no need to ensure presence of ConfigMap
                // if false, ensure ConfigMap and update the status
                err = r.ensureRookCephOperatorConfig(instance)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to process %s Configmap", rookCephOperatorConfigName)
                        return reconcile.Result{}, err
                }</span>
                <span class="cov8" title="1">instance.Status.RookCephOperatorConfigCreated = true</span>
        }

        <span class="cov8" title="1">reason := ocsv1.ReconcileCompleted
        message := ocsv1.ReconcileCompletedMessage
        util.SetCompleteCondition(&amp;instance.Status.Conditions, reason, message)

        instance.Status.Phase = util.PhaseReady
        err = r.Client.Status().Update(context.TODO(), instance)

        return reconcile.Result{}, err</span>
}

// SetupWithManager sets up a controller with a manager
func (r *OCSInitializationReconciler) SetupWithManager(mgr ctrl.Manager) error <span class="cov0" title="0">{
        ns, err := util.GetWatchNamespace()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">watchNamespace = ns

        rookImage := os.Getenv("ROOK_CEPH_IMAGE")
        if rookImage == "" </span><span class="cov0" title="0">{
                return fmt.Errorf("No ROOK_CEPH_IMAGE environment variable set")
        }</span>
        <span class="cov0" title="0">r.RookImage = rookImage

        return ctrl.NewControllerManagedBy(mgr).
                For(&amp;ocsv1.OCSInitialization{}).
                Owns(&amp;appsv1.Deployment{}).
                Complete(r)</span>
}

// returns a ConfigMap with default settings for rook-ceph operator
func newRookCephOperatorConfig(namespace string) *corev1.ConfigMap <span class="cov8" title="1">{
        var defaultCSIToleration = `
- key: ` + defaults.NodeTolerationKey + `
  operator: Equal
  value: "true"
  effect: NoSchedule`

        config := &amp;corev1.ConfigMap{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      rookCephOperatorConfigName,
                        Namespace: namespace,
                },
        }
        data := make(map[string]string)
        data["CSI_PROVISIONER_TOLERATIONS"] = defaultCSIToleration
        data["CSI_PLUGIN_TOLERATIONS"] = defaultCSIToleration
        data["CSI_LOG_LEVEL"] = "5"
        config.Data = data

        return config
}</span>

func (r *OCSInitializationReconciler) ensureRookCephOperatorConfig(initialData *ocsv1.OCSInitialization) error <span class="cov8" title="1">{
        rookCephOperatorConfig := &amp;corev1.ConfigMap{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: rookCephOperatorConfigName, Namespace: initialData.Namespace}, rookCephOperatorConfig)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        // If it does not exist, create a ConfigMap with default settings
                        return r.Client.Create(context.TODO(), newRookCephOperatorConfig(initialData.Namespace))
                }</span>
                <span class="cov0" title="0">return err</span>
        }
        // If it already exists, do not update. It is up to the user to
        // update the ConfigMap as they see fit. Changes will be picked
        // up by rook operator and reconciled. We do not want to reconcile
        // this ConfigMap. If we do, user changes will be reset to defaults.
        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">package ocsinitialization

import (
        "context"
        "fmt"

        secv1 "github.com/openshift/api/security/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func (r *OCSInitializationReconciler) ensureSCCs(initialData *ocsv1.OCSInitialization) error <span class="cov8" title="1">{
        sccs := getAllSCCs(initialData.Namespace)
        for _, scc := range sccs </span><span class="cov8" title="1">{
                found, err := r.SecurityClient.SecurityContextConstraints().Get(context.TODO(), scc.Name, metav1.GetOptions{})

                if err != nil &amp;&amp; errors.IsNotFound(err) </span><span class="cov0" title="0">{
                        r.Log.Info(fmt.Sprintf("Creating %s SecurityContextConstraint", scc.Name))
                        _, err := r.SecurityClient.SecurityContextConstraints().Create(context.TODO(), scc, metav1.CreateOptions{})
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("unable to create SCC %+v: %v", scc, err)
                        }</span>
                } else<span class="cov8" title="1"> if err == nil </span><span class="cov8" title="1">{
                        scc.ObjectMeta = found.ObjectMeta
                        r.Log.Info(fmt.Sprintf("Updating %s SecurityContextConstraint", scc.Name))
                        _, err := r.SecurityClient.SecurityContextConstraints().Update(context.TODO(), scc, metav1.UpdateOptions{})
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("unable to update SCC %+v: %v", scc, err)
                        }</span>
                } else<span class="cov0" title="0"> {
                        return fmt.Errorf("something went wrong when checking for SCC %+v: %v", scc, err)
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

func getAllSCCs(namespace string) []*secv1.SecurityContextConstraints <span class="cov8" title="1">{
        return []*secv1.SecurityContextConstraints{
                newRookCephSCC(namespace),
                newRookCephCSISCC(namespace),
                newNooBaaSCC(namespace),
        }
}</span>

func blankSCC() *secv1.SecurityContextConstraints <span class="cov8" title="1">{
        return &amp;secv1.SecurityContextConstraints{
                TypeMeta: metav1.TypeMeta{
                        APIVersion: "security.openshift.io/v1",
                        Kind:       "SecurityContextConstraints",
                },
        }
}</span>

func newRookCephSCC(namespace string) *secv1.SecurityContextConstraints <span class="cov8" title="1">{
        scc := blankSCC()

        scc.Name = "rook-ceph"
        scc.AllowPrivilegedContainer = true
        scc.AllowHostNetwork = true
        scc.AllowHostDirVolumePlugin = true
        scc.AllowHostPorts = true
        scc.AllowHostPID = true
        scc.AllowHostIPC = true
        scc.ReadOnlyRootFilesystem = false
        scc.RequiredDropCapabilities = []corev1.Capability{}
        scc.DefaultAddCapabilities = []corev1.Capability{}
        scc.RunAsUser = secv1.RunAsUserStrategyOptions{
                Type: secv1.RunAsUserStrategyRunAsAny,
        }
        scc.SELinuxContext = secv1.SELinuxContextStrategyOptions{
                Type: secv1.SELinuxStrategyMustRunAs,
        }
        scc.FSGroup = secv1.FSGroupStrategyOptions{
                Type: secv1.FSGroupStrategyMustRunAs,
        }
        scc.SupplementalGroups = secv1.SupplementalGroupsStrategyOptions{
                Type: secv1.SupplementalGroupsStrategyRunAsAny,
        }
        scc.Volumes = []secv1.FSType{
                secv1.FSTypeConfigMap,
                secv1.FSTypeDownwardAPI,
                secv1.FSTypeEmptyDir,
                secv1.FSTypeHostPath,
                secv1.FSTypePersistentVolumeClaim,
                secv1.FSProjected,
                secv1.FSTypeSecret,
        }
        scc.Users = []string{
                fmt.Sprintf("system:serviceaccount:%s:rook-ceph-system", namespace),
                fmt.Sprintf("system:serviceaccount:%s:default", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-ceph-mgr", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-ceph-osd", namespace),
        }

        return scc
}</span>

func newRookCephCSISCC(namespace string) *secv1.SecurityContextConstraints <span class="cov8" title="1">{
        scc := blankSCC()

        scc.Name = "rook-ceph-csi"
        scc.AllowPrivilegedContainer = true
        scc.AllowHostNetwork = true
        scc.AllowHostDirVolumePlugin = true
        scc.AllowedCapabilities = []corev1.Capability{
                secv1.AllowAllCapabilities,
        }
        scc.AllowHostPorts = true
        scc.AllowHostPID = true
        scc.AllowHostIPC = true
        scc.ReadOnlyRootFilesystem = false
        scc.RequiredDropCapabilities = []corev1.Capability{}
        scc.DefaultAddCapabilities = []corev1.Capability{}
        scc.RunAsUser = secv1.RunAsUserStrategyOptions{
                Type: secv1.RunAsUserStrategyRunAsAny,
        }
        scc.SELinuxContext = secv1.SELinuxContextStrategyOptions{
                Type: secv1.SELinuxStrategyRunAsAny,
        }
        scc.FSGroup = secv1.FSGroupStrategyOptions{
                Type: secv1.FSGroupStrategyRunAsAny,
        }
        scc.SupplementalGroups = secv1.SupplementalGroupsStrategyOptions{
                Type: secv1.SupplementalGroupsStrategyRunAsAny,
        }
        scc.Volumes = []secv1.FSType{
                secv1.FSTypeAll,
        }
        scc.Users = []string{
                fmt.Sprintf("system:serviceaccount:%s:rook-csi-rbd-plugin-sa", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-csi-rbd-provisioner-sa", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-csi-rbd-attacher-sa", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-csi-cephfs-plugin-sa", namespace),
                fmt.Sprintf("system:serviceaccount:%s:rook-csi-cephfs-provisioner-sa", namespace),
        }

        return scc
}</span>

func newNooBaaSCC(namespace string) *secv1.SecurityContextConstraints <span class="cov8" title="1">{
        scc := blankSCC()

        scc.Name = "noobaa"
        allowPrivilegeEscalation := true
        scc.AllowHostDirVolumePlugin = false
        scc.AllowHostIPC = false
        scc.AllowHostNetwork = false
        scc.AllowHostPID = false
        scc.AllowPrivilegeEscalation = &amp;allowPrivilegeEscalation
        scc.AllowPrivilegedContainer = false
        scc.ReadOnlyRootFilesystem = false
        scc.AllowedCapabilities = []corev1.Capability{}
        scc.DefaultAddCapabilities = []corev1.Capability{}
        scc.RequiredDropCapabilities = []corev1.Capability{
                "KILL",
                "MKNOD",
                "SETUID",
                "SETGID",
        }
        scc.RunAsUser = secv1.RunAsUserStrategyOptions{
                Type: secv1.RunAsUserStrategyRunAsAny,
        }
        scc.SELinuxContext = secv1.SELinuxContextStrategyOptions{
                Type: secv1.SELinuxStrategyMustRunAs,
        }
        scc.FSGroup = secv1.FSGroupStrategyOptions{
                Type: secv1.FSGroupStrategyMustRunAs,
        }
        scc.SupplementalGroups = secv1.SupplementalGroupsStrategyOptions{
                Type: secv1.SupplementalGroupsStrategyRunAsAny,
        }
        scc.Volumes = []secv1.FSType{
                secv1.FSTypeConfigMap,
                secv1.FSTypeDownwardAPI,
                secv1.FSTypeEmptyDir,
                secv1.FSTypeHostPath,
                secv1.FSTypePersistentVolumeClaim,
                secv1.FSProjected,
                secv1.FSTypeSecret,
        }
        scc.Users = []string{
                fmt.Sprintf("system:serviceaccount:%s:noobaa", namespace),
        }

        return scc
}</span>
</pre>
		
		<pre class="file" id="file2" style="display: none">package persistentvolume

import (
        "strings"

        "github.com/openshift/ocs-operator/controllers/util"

        "github.com/go-logr/logr"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/runtime"
        ctrl "sigs.k8s.io/controller-runtime"
        "sigs.k8s.io/controller-runtime/pkg/builder"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/event"
        "sigs.k8s.io/controller-runtime/pkg/predicate"
)

const (
        csiExpansionSecretName      = "csi.storage.k8s.io/controller-expand-secret-name"
        csiExpansionSecretNamespace = "csi.storage.k8s.io/controller-expand-secret-namespace"
        csiRBDDriverSuffix          = ".rbd.csi.ceph.com"
        csiCephFSDriverSuffix       = ".cephfs.csi.ceph.com"
)

// reconcilePV determines whether we want to reconcile a given PV
func reconcilePV(obj runtime.Object) bool <span class="cov8" title="1">{
        pv, ok := obj.(*corev1.PersistentVolume)
        if !ok </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov8" title="1">drivers := []string{
                csiRBDDriverSuffix,
                csiCephFSDriverSuffix,
        }

        for _, driver := range drivers </span><span class="cov8" title="1">{
                if pv.Spec.CSI != nil &amp;&amp; strings.HasSuffix(pv.Spec.CSI.Driver, driver) </span><span class="cov8" title="1">{
                        if pv.Spec.StorageClassName != "" </span><span class="cov8" title="1">{
                                secretRef := pv.Spec.CSI.ControllerExpandSecretRef
                                if secretRef == nil || secretRef.Name == "" </span><span class="cov8" title="1">{
                                        return true
                                }</span>
                        }
                        <span class="cov8" title="1">return false</span>
                }
        }

        <span class="cov8" title="1">return false</span>
}

var pvPredicate = predicate.Funcs{
        CreateFunc: func(e event.CreateEvent) bool <span class="cov0" title="0">{
                return reconcilePV(e.Object)
        }</span>,
        DeleteFunc: func(e event.DeleteEvent) bool <span class="cov0" title="0">{
                return false
        }</span>,
        UpdateFunc: func(e event.UpdateEvent) bool <span class="cov0" title="0">{
                return reconcilePV(e.ObjectNew)
        }</span>,
        GenericFunc: func(e event.GenericEvent) bool <span class="cov0" title="0">{
                return reconcilePV(e.Object)
        }</span>,
}

// PersistentVolumeReconciler reconciles a PersistentVolume object
//nolint
type PersistentVolumeReconciler struct {
        client.Client
        Scheme *runtime.Scheme
        Log    logr.Logger
}

// SetupWithManager sets up a controller with a manager
func (r *PersistentVolumeReconciler) SetupWithManager(mgr ctrl.Manager) error <span class="cov0" title="0">{

        // Compose a predicate that is an OR of the specified predicates
        predicate := util.ComposePredicates(
                predicate.ResourceVersionChangedPredicate{},
                util.MetadataChangedPredicate{},
        )

        return ctrl.NewControllerManagedBy(mgr).
                For(&amp;corev1.PersistentVolume{}, builder.WithPredicates(predicate, pvPredicate)).
                Complete(r)
}</span>
</pre>
		
		<pre class="file" id="file3" style="display: none">package persistentvolume

import (
        "context"

        corev1 "k8s.io/api/core/v1"
        storagev1 "k8s.io/api/storage/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/reconcile"
)

// ensureFunc which encapsulate all the 'ensure*' type functions
type ensureFunc func(*corev1.PersistentVolume) error

// +kubebuilder:rbac:groups=core,resources=persistentvolumes,verbs=get;list;watch;update;patch
// +kubebuilder:rbac:groups=storage.k8s.io,resources=storageclasses,verbs=get

// Reconcile ...
func (r *PersistentVolumeReconciler) Reconcile(request reconcile.Request) (reconcile.Result, error) <span class="cov8" title="1">{

        prevLogger := r.Log
        defer func() </span><span class="cov8" title="1">{ r.Log = prevLogger }</span>()
        <span class="cov8" title="1">r.Log = r.Log.WithValues("Request.Namespace", request.Namespace, "Request.Name", request.Name)

        pv := &amp;corev1.PersistentVolume{}
        err := r.Client.Get(context.TODO(), request.NamespacedName, pv)
        if err != nil </span><span class="cov0" title="0">{
                if errors.IsNotFound(err) </span><span class="cov0" title="0">{
                        r.Log.Info("PersistentVolume not found")
                        return reconcile.Result{}, nil
                }</span>
                <span class="cov0" title="0">return reconcile.Result{}, err</span>
        }

        // Check GetDeletionTimestamp to determine if the object is under deletion
        <span class="cov8" title="1">if !pv.GetDeletionTimestamp().IsZero() </span><span class="cov0" title="0">{
                r.Log.Info("Object is terminated, skipping reconciliation")
                return reconcile.Result{}, nil
        }</span>

        <span class="cov8" title="1">r.Log.Info("Reconciling PersistentVolume")

        ensureFs := []ensureFunc{
                r.ensureExpansionSecret,
        }
        for _, f := range ensureFs </span><span class="cov8" title="1">{
                err = f(pv)
                if err != nil </span><span class="cov0" title="0">{
                        return reconcile.Result{}, err
                }</span>
        }

        <span class="cov8" title="1">return reconcile.Result{}, nil</span>
}

func (r *PersistentVolumeReconciler) ensureExpansionSecret(pv *corev1.PersistentVolume) error <span class="cov8" title="1">{
        scName := pv.Spec.StorageClassName
        if scName == "" </span><span class="cov8" title="1">{
                r.Log.Info("PersistentVolume has no associated StorageClass")
                return nil
        }</span>
        <span class="cov8" title="1">sc := &amp;storagev1.StorageClass{}

        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: scName}, sc)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Error getting StorageClass")
                return err
        }</span>

        <span class="cov8" title="1">patch := client.MergeFrom(pv)

        secretRef := &amp;corev1.SecretReference{}

        if secretName, ok := sc.Parameters[csiExpansionSecretName]; ok </span><span class="cov8" title="1">{
                secretRef.Name = secretName
        }</span>
        <span class="cov8" title="1">if secretNamespace, ok := sc.Parameters[csiExpansionSecretNamespace]; ok </span><span class="cov8" title="1">{
                secretRef.Namespace = secretNamespace
        }</span>

        <span class="cov8" title="1">newPV := &amp;corev1.PersistentVolume{}
        pv.DeepCopyInto(newPV)

        newPV.Spec.CSI.ControllerExpandSecretRef = secretRef
        err = r.Client.Patch(context.TODO(), newPV, patch)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Error patching PersistentVolume")
                return err
        }</span>

        <span class="cov8" title="1">r.Log.Info("PersistentVolume patched")
        return nil</span>
}
</pre>
		
		<pre class="file" id="file4" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsCephBlockPools struct{}

// newCephBlockPoolInstances returns the cephBlockPool instances that should be created
// on first run.
func (r *StorageClusterReconciler) newCephBlockPoolInstances(initData *ocsv1.StorageCluster) ([]*cephv1.CephBlockPool, error) <span class="cov8" title="1">{
        ret := []*cephv1.CephBlockPool{
                {
                        ObjectMeta: metav1.ObjectMeta{
                                Name:      generateNameForCephBlockPool(initData),
                                Namespace: initData.Namespace,
                        },
                        Spec: cephv1.PoolSpec{
                                FailureDomain:  determineFailureDomain(initData),
                                Replicated:     generateCephReplicatedSpec(initData, "data"),
                                EnableRBDStats: true,
                        },
                },
        }
        for _, obj := range ret </span><span class="cov8" title="1">{
                err := controllerutil.SetControllerReference(initData, obj, r.Scheme)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }
        <span class="cov8" title="1">return ret, nil</span>
}

// ensureCreated ensures that cephBlockPool resources exist in the desired
// state.
func (obj *ocsCephBlockPools) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephBlockPools.ReconcileStrategy)
        if reconcileStrategy == ReconcileStrategyIgnore </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov8" title="1">cephBlockPools, err := r.newCephBlockPoolInstances(instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">for _, cephBlockPool := range cephBlockPools </span><span class="cov8" title="1">{
                existing := cephv1.CephBlockPool{}
                err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephBlockPool.Name, Namespace: cephBlockPool.Namespace}, &amp;existing)

                switch </span>{
                case err == nil:<span class="cov0" title="0">
                        if reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                                return nil
                        }</span>
                        <span class="cov0" title="0">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                r.Log.Info(fmt.Sprintf("Unable to restore init object because %s is marked for deletion", existing.Name))
                                return fmt.Errorf("failed to restore initialization object %s because it is marked for deletion", existing.Name)
                        }</span>

                        <span class="cov0" title="0">r.Log.Info(fmt.Sprintf("Restoring original cephBlockPool %s", cephBlockPool.Name))
                        existing.ObjectMeta.OwnerReferences = cephBlockPool.ObjectMeta.OwnerReferences
                        cephBlockPool.ObjectMeta = existing.ObjectMeta
                        err = r.Client.Update(context.TODO(), cephBlockPool)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                case errors.IsNotFound(err):<span class="cov8" title="1">
                        r.Log.Info(fmt.Sprintf("Creating cephBlockPool %s", cephBlockPool.Name))
                        err = r.Client.Create(context.TODO(), cephBlockPool)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
        }

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the CephBlockPools owned by the StorageCluster
func (obj *ocsCephBlockPools) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        foundCephBlockPool := &amp;cephv1.CephBlockPool{}
        cephBlockPools, err := r.newCephBlockPoolInstances(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, cephBlockPool := range cephBlockPools </span><span class="cov8" title="1">{
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephBlockPool.Name, Namespace: sc.Namespace}, foundCephBlockPool)
                if err != nil </span><span class="cov0" title="0">{
                        if errors.IsNotFound(err) </span><span class="cov0" title="0">{
                                r.Log.Info("Uninstall: CephBlockPool not found", "CephBlockPool Name", cephBlockPool.Name)
                                continue</span>
                        }
                        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve cephBlockPool %v: %v", cephBlockPool.Name, err)</span>
                }

                <span class="cov8" title="1">if cephBlockPool.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: Deleting cephBlockPool", "CephBlockPool Name", cephBlockPool.Name)
                        err = r.Client.Delete(context.TODO(), foundCephBlockPool)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("Uninstall: Failed to delete cephBlockPool %v: %v", foundCephBlockPool.Name, err)
                        }</span>
                }

                <span class="cov8" title="1">err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephBlockPool.Name, Namespace: sc.Namespace}, foundCephBlockPool)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephBlockPool is deleted", "CephBlockPool Name", cephBlockPool.Name)
                                continue</span>
                        }
                }
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting for cephBlockPool %v to be deleted", cephBlockPool.Name)</span>

        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        "os"
        "reflect"
        "strconv"
        "strings"

        "github.com/go-logr/logr"
        objectreferencesv1 "github.com/openshift/custom-resource-status/objectreferences/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        statusutil "github.com/openshift/ocs-operator/controllers/util"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        rook "github.com/rook/rook/pkg/apis/rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        storagev1 "k8s.io/api/storage/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/api/resource"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/version"
        "k8s.io/client-go/tools/reference"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsCephCluster struct{}
type diskSpeed string

const (
        diskSpeedUnknown diskSpeed = "unknown"
        diskSpeedSlow    diskSpeed = "slow"
        diskSpeedFast    diskSpeed = "fast"
)

type knownDiskType struct {
        speed            diskSpeed
        provisioner      StorageClassProvisionerType
        storageClassType string
}

// These are known disk types where we can't correctly detect the type of the
// disk (rotational or ssd) automatically, so rook would apply wrong tunings.
// This list allows to specify disks from which storage classes to tune for fast
// or slow disk optimization.
var knownDiskTypes = []knownDiskType{
        {diskSpeedSlow, EBS, "gp2"},
        {diskSpeedSlow, EBS, "io1"},
        {diskSpeedFast, AzureDisk, "managed-premium"},
}

const (
        // Hardcoding networkProvider to multus and this can be changed later to accommodate other providers
        networkProvider           = "multus"
        publicNetworkSelectorKey  = "public"
        clusterNetworkSelectorKey = "cluster"
)

func arbiterEnabled(sc *ocsv1.StorageCluster) bool <span class="cov8" title="1">{
        return sc.Spec.Arbiter.Enable
}</span>

// ensureCreated ensures that a CephCluster resource exists with its Spec in
// the desired state.
func (obj *ocsCephCluster) ensureCreated(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        if sc.Spec.ExternalStorage.Enable &amp;&amp; len(sc.Spec.StorageDeviceSets) != 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("'StorageDeviceSets' should not be initialized in an external CephCluster")
        }</span>

        <span class="cov8" title="1">for i, ds := range sc.Spec.StorageDeviceSets </span><span class="cov0" title="0">{
                sc.Spec.StorageDeviceSets[i].Config.TuneSlowDeviceClass = false
                sc.Spec.StorageDeviceSets[i].Config.TuneFastDeviceClass = false

                diskSpeed, err := r.checkTuneStorageDevices(ds)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("Failed to check for known device types: %+v", err)
                }</span>
                <span class="cov0" title="0">switch diskSpeed </span>{
                case diskSpeedSlow:<span class="cov0" title="0">
                        sc.Spec.StorageDeviceSets[i].Config.TuneSlowDeviceClass = true</span>
                case diskSpeedFast:<span class="cov0" title="0">
                        sc.Spec.StorageDeviceSets[i].Config.TuneFastDeviceClass = true</span>
                default:<span class="cov0" title="0"></span>
                }
        }

        <span class="cov8" title="1">if isMultus(sc.Spec.Network) </span><span class="cov8" title="1">{
                err := validateMultusSelectors(sc.Spec.Network.Selectors)
                if err != nil </span><span class="cov8" title="1">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">var cephCluster *cephv1.CephCluster
        // Define a new CephCluster object
        if sc.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                cephCluster = newExternalCephCluster(sc, r.images.Ceph, r.monitoringIP, r.monitoringPort)
        }</span> else<span class="cov8" title="1"> {
                kmsConfigMap, err := getKMSConfigMap(sc, r.Client, reachKMSProvider)
                if err != nil </span><span class="cov8" title="1">{
                        r.Log.Error(err, "failed to procure KMS config")
                        return err
                }</span>
                <span class="cov8" title="1">cephCluster = newCephCluster(sc, r.images.Ceph, r.nodeCount, r.serverVersion, kmsConfigMap, r.Log)</span>
        }

        // Set StorageCluster instance as the owner and controller
        <span class="cov8" title="1">if err := controllerutil.SetControllerReference(sc, cephCluster, r.Scheme); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Check if this CephCluster already exists
        <span class="cov8" title="1">found := &amp;cephv1.CephCluster{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephCluster.Name, Namespace: cephCluster.Namespace}, found)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        if sc.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                                r.Log.Info("Creating external CephCluster")
                        }</span> else<span class="cov8" title="1"> {
                                r.Log.Info("Creating CephCluster")
                        }</span>
                        <span class="cov8" title="1">if err := r.Client.Create(context.TODO(), cephCluster); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        // Need to happen after the ceph cluster CR creation was confirmed
                        <span class="cov8" title="1">sc.Status.Images.Ceph.ActualImage = cephCluster.Spec.CephVersion.Image
                        return nil</span>
                }
                <span class="cov0" title="0">return err</span>
        }

        // Update the CephCluster if it is not in the desired state
        <span class="cov8" title="1">if !reflect.DeepEqual(cephCluster.Spec, found.Spec) </span><span class="cov8" title="1">{
                r.Log.Info("Updating spec for CephCluster")
                if !sc.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                        // Check if Cluster is Expanding
                        if len(found.Spec.Storage.StorageClassDeviceSets) &lt; len(cephCluster.Spec.Storage.StorageClassDeviceSets) </span><span class="cov0" title="0">{
                                r.phase = statusutil.PhaseClusterExpanding
                        }</span> else<span class="cov8" title="1"> if len(found.Spec.Storage.StorageClassDeviceSets) == len(cephCluster.Spec.Storage.StorageClassDeviceSets) </span><span class="cov8" title="1">{
                                for _, countInFoundSpec := range found.Spec.Storage.StorageClassDeviceSets </span><span class="cov0" title="0">{
                                        for _, countInCephClusterSpec := range cephCluster.Spec.Storage.StorageClassDeviceSets </span><span class="cov0" title="0">{
                                                if countInFoundSpec.Name == countInCephClusterSpec.Name &amp;&amp; countInCephClusterSpec.Count &gt; countInFoundSpec.Count </span><span class="cov0" title="0">{
                                                        r.phase = statusutil.PhaseClusterExpanding
                                                        break</span>
                                                }
                                        }
                                        <span class="cov0" title="0">if r.phase == statusutil.PhaseClusterExpanding </span><span class="cov0" title="0">{
                                                break</span>
                                        }
                                }
                        }
                }
                <span class="cov8" title="1">found.Spec = cephCluster.Spec
                if err := r.Client.Update(context.TODO(), found); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                // Need to happen after the ceph cluster CR update was confirmed
                <span class="cov8" title="1">sc.Status.Images.Ceph.ActualImage = cephCluster.Spec.CephVersion.Image
                return nil</span>
        }

        // Add it to the list of RelatedObjects if found
        <span class="cov8" title="1">objectRef, err := reference.GetReference(r.Scheme, found)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">err = objectreferencesv1.SetObjectReference(&amp;sc.Status.RelatedObjects, *objectRef)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Handle CephCluster resource status
        <span class="cov8" title="1">if found.Status.State == "" </span><span class="cov8" title="1">{
                r.Log.Info("CephCluster resource is not reporting status.")
                // What does this mean to OCS status? Assuming progress.
                reason := "CephClusterStatus"
                message := "CephCluster resource is not reporting status"
                statusutil.MapCephClusterNoConditions(&amp;r.conditions, reason, message)
        }</span> else<span class="cov8" title="1"> {
                // Interpret CephCluster status and set any negative conditions
                if sc.Spec.ExternalStorage.Enable </span><span class="cov0" title="0">{
                        statusutil.MapExternalCephClusterNegativeConditions(&amp;r.conditions, found)
                }</span> else<span class="cov8" title="1"> {
                        statusutil.MapCephClusterNegativeConditions(&amp;r.conditions, found)
                }</span>
        }

        // When phase is expanding, wait for CephCluster state to be updating
        // this means expansion is in progress and overall system is progressing
        // else expansion is not yet triggered
        <span class="cov8" title="1">if sc.Status.Phase == statusutil.PhaseClusterExpanding &amp;&amp;
                found.Status.State != cephv1.ClusterStateUpdating </span><span class="cov0" title="0">{
                r.phase = statusutil.PhaseClusterExpanding
        }</span>

        <span class="cov8" title="1">if sc.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                if found.Status.State == cephv1.ClusterStateConnecting </span><span class="cov0" title="0">{
                        sc.Status.Phase = statusutil.PhaseConnecting
                }</span> else<span class="cov8" title="1"> if found.Status.State == cephv1.ClusterStateConnected </span><span class="cov0" title="0">{
                        sc.Status.Phase = statusutil.PhaseReady
                }</span> else<span class="cov8" title="1"> {
                        sc.Status.Phase = statusutil.PhaseNotReady
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the CephCluster owned by the StorageCluster
func (obj *ocsCephCluster) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        cephCluster := &amp;cephv1.CephCluster{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: generateNameForCephCluster(sc), Namespace: sc.Namespace}, cephCluster)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: CephCluster not found")
                        return nil
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve cephCluster: %v", err)</span>
        }

        <span class="cov8" title="1">if cephCluster.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                r.Log.Info("Uninstall: Deleting cephCluster")
                err = r.Client.Delete(context.TODO(), cephCluster)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("Uninstall: Failed to delete cephCluster: %v", err)
                }</span>
        }

        <span class="cov8" title="1">err = r.Client.Get(context.TODO(), types.NamespacedName{Name: generateNameForCephCluster(sc), Namespace: sc.Namespace}, cephCluster)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: CephCluster is deleted")
                        return nil
                }</span>
        }
        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting for cephCluster to be deleted")</span>

}

// newCephCluster returns a CephCluster object.
func newCephCluster(sc *ocsv1.StorageCluster, cephImage string, nodeCount int, serverVersion *version.Info, kmsConfigMap *corev1.ConfigMap, reqLogger logr.Logger) *cephv1.CephCluster <span class="cov8" title="1">{
        labels := map[string]string{
                "app": sc.Name,
        }

        cephCluster := &amp;cephv1.CephCluster{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      generateNameForCephCluster(sc),
                        Namespace: sc.Namespace,
                        Labels:    labels,
                },
                Spec: cephv1.ClusterSpec{
                        CephVersion: cephv1.CephVersionSpec{
                                Image:            cephImage,
                                AllowUnsupported: allowUnsupportedCephVersion(),
                        },
                        Mon: generateMonSpec(sc, nodeCount),
                        Mgr: cephv1.MgrSpec{
                                Modules: []cephv1.Module{
                                        {Name: "pg_autoscaler", Enabled: true},
                                        {Name: "balancer", Enabled: true},
                                },
                        },
                        DataDirHostPath: "/var/lib/rook",
                        DisruptionManagement: cephv1.DisruptionManagementSpec{
                                ManagePodBudgets:                 true,
                                ManageMachineDisruptionBudgets:   false,
                                MachineDisruptionBudgetNamespace: "openshift-machine-api",
                        },
                        Network: cephv1.NetworkSpec{
                                HostNetwork: sc.Spec.HostNetwork,
                        },
                        Monitoring: cephv1.MonitoringSpec{
                                Enabled:        true,
                                RulesNamespace: "openshift-storage",
                        },
                        Storage: rook.StorageScopeSpec{
                                StorageClassDeviceSets: newStorageClassDeviceSets(sc, serverVersion),
                        },
                        Placement: rook.PlacementSpec{
                                "all":     getPlacement(sc, "all"),
                                "mon":     getPlacement(sc, "mon"),
                                "arbiter": getPlacement(sc, "arbiter"),
                        },
                        Resources: newCephDaemonResources(sc.Spec.Resources),
                        ContinueUpgradeAfterChecksEvenIfNotHealthy: true,
                        LogCollector: cephv1.LogCollectorSpec{
                                Enabled:     true,
                                Periodicity: "24h",
                        },
                },
        }
        monPVCTemplate := sc.Spec.MonPVCTemplate
        monDataDirHostPath := sc.Spec.MonDataDirHostPath
        // If the `monPVCTemplate` is provided, the mons will provisioned on the
        // provided `monPVCTemplate`.
        if monPVCTemplate != nil </span><span class="cov8" title="1">{
                cephCluster.Spec.Mon.VolumeClaimTemplate = monPVCTemplate
                // If the `monDataDirHostPath` is provided without the `monPVCTemplate`,
                // the mons will be provisioned on the provided `monDataDirHostPath`.
        }</span> else<span class="cov8" title="1"> if len(monDataDirHostPath) &gt; 0 </span><span class="cov8" title="1">{
                cephCluster.Spec.DataDirHostPath = monDataDirHostPath
                // If no `monPVCTemplate` and `monDataDirHostPath` is provided, the mons will
                // be provisioned using the PVC template of first StorageDeviceSets if present.
        }</span> else<span class="cov8" title="1"> if len(sc.Spec.StorageDeviceSets) &gt; 0 </span><span class="cov8" title="1">{
                ds := sc.Spec.StorageDeviceSets[0]
                cephCluster.Spec.Mon.VolumeClaimTemplate = &amp;corev1.PersistentVolumeClaim{
                        Spec: corev1.PersistentVolumeClaimSpec{
                                StorageClassName: ds.DataPVCTemplate.Spec.StorageClassName,
                                Resources: corev1.ResourceRequirements{
                                        Requests: corev1.ResourceList{
                                                corev1.ResourceStorage: resource.MustParse("10Gi"),
                                        },
                                },
                        },
                }
        }</span> else<span class="cov8" title="1"> {
                reqLogger.Info(fmt.Sprintf("No monDataDirHostPath, monPVCTemplate or storageDeviceSets configured for storageCluster %s", sc.GetName()))
        }</span>
        <span class="cov8" title="1">if isMultus(sc.Spec.Network) </span><span class="cov8" title="1">{
                cephCluster.Spec.Network.NetworkSpec = *sc.Spec.Network
        }</span>
        // if kmsConfig is not 'nil', add the KMS details to CephCluster spec
        <span class="cov8" title="1">if kmsConfigMap != nil </span><span class="cov8" title="1">{
                cephCluster.Spec.Security.KeyManagementService.ConnectionDetails = kmsConfigMap.Data
                cephCluster.Spec.Security.KeyManagementService.TokenSecretName = KMSTokenSecretName
        }</span>
        <span class="cov8" title="1">return cephCluster</span>
}

func isMultus(nwSpec *rook.NetworkSpec) bool <span class="cov8" title="1">{
        if nwSpec != nil </span><span class="cov8" title="1">{
                return nwSpec.IsMultus()
        }</span>
        <span class="cov8" title="1">return false</span>
}

func validateMultusSelectors(selectors map[string]string) error <span class="cov8" title="1">{
        publicNetwork, validPublicNetworkKey := selectors[publicNetworkSelectorKey]
        clusterNetwork, validClusterNetworkKey := selectors[clusterNetworkSelectorKey]
        if !validPublicNetworkKey &amp;&amp; !validClusterNetworkKey </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid value of the keys for the network selectors. keys should be public and cluster only")
        }</span>
        <span class="cov8" title="1">if publicNetwork == "" &amp;&amp; clusterNetwork == "" </span><span class="cov8" title="1">{
                return fmt.Errorf("Both public and cluster network selector values can't be empty")
        }</span>
        <span class="cov8" title="1">if publicNetwork == "" </span><span class="cov8" title="1">{
                return fmt.Errorf("public network selector values can't be empty")
        }</span>
        <span class="cov8" title="1">return nil</span>
}

func newExternalCephCluster(sc *ocsv1.StorageCluster, cephImage, monitoringIP, monitoringPort string) *cephv1.CephCluster <span class="cov8" title="1">{
        labels := map[string]string{
                "app": sc.Name,
        }

        var monitoringSpec = cephv1.MonitoringSpec{Enabled: false}

        if monitoringIP != "" </span><span class="cov8" title="1">{
                monitoringSpec.Enabled = true
                monitoringSpec.RulesNamespace = sc.Namespace
                monitoringSpec.ExternalMgrEndpoints = []corev1.EndpointAddress{{IP: monitoringIP}}
                if monitoringPort != "" </span><span class="cov8" title="1">{
                        if uint16Val, err := strconv.ParseUint(monitoringPort, 10, 16); err == nil </span><span class="cov8" title="1">{
                                monitoringSpec.ExternalMgrPrometheusPort = uint16(uint16Val)
                        }</span>
                }
        }

        <span class="cov8" title="1">externalCephCluster := &amp;cephv1.CephCluster{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      generateNameForCephCluster(sc),
                        Namespace: sc.Namespace,
                        Labels:    labels,
                },
                Spec: cephv1.ClusterSpec{
                        External: cephv1.ExternalSpec{
                                Enable: true,
                        },
                        CrashCollector: cephv1.CrashCollectorSpec{
                                Disable: true,
                        },
                        DisruptionManagement: cephv1.DisruptionManagementSpec{
                                ManagePodBudgets:               false,
                                ManageMachineDisruptionBudgets: false,
                        },
                        Monitoring: monitoringSpec,
                },
        }

        return externalCephCluster</span>
}

func getMinDeviceSetReplica(sc *ocsv1.StorageCluster) int <span class="cov8" title="1">{
        if arbiterEnabled(sc) </span><span class="cov0" title="0">{
                return defaults.ArbiterModeDeviceSetReplica
        }</span>
        <span class="cov8" title="1">return defaults.DeviceSetReplica</span>
}

func getReplicasPerFailureDomain(sc *ocsv1.StorageCluster) int <span class="cov8" title="1">{
        if arbiterEnabled(sc) </span><span class="cov0" title="0">{
                return defaults.ArbiterReplicasPerFailureDomain
        }</span>
        <span class="cov8" title="1">return defaults.ReplicasPerFailureDomain</span>
}

// getCephPoolReplicatedSize returns the default replica per cluster count for a
// StorageCluster type
func getCephPoolReplicatedSize(sc *ocsv1.StorageCluster) uint <span class="cov8" title="1">{
        if arbiterEnabled(sc) </span><span class="cov0" title="0">{
                return uint(4)
        }</span>
        <span class="cov8" title="1">return uint(3)</span>
}

// getMinimumNodes returns the minimum number of nodes that are required for the Storage Cluster of various configurations
func getMinimumNodes(sc *ocsv1.StorageCluster) int <span class="cov8" title="1">{
        // Case 1: When replicasPerFailureDomain is 1.
        // A node is the smallest failure domain that is possible. We definitely
        // want the devices in the same device set to be in different failure
        // domains which also means different nodes. In this case, minimum number of
        // nodes is getMinDeviceSetReplica() * 1.

        // Case 2: When replicasPerFailureDomain is greater than 1
        // In certain scenarios, it may be valid to place one or more replicas
        // within the failure domain on the same node but it does not make much
        // sense. It provides protection only against disk failures and not node
        // failures.
        // For example:
        // a. FailureDomain=node, replicasPerFailureDomain&gt;1
        // b. FailureDomain=rack, numNodesInTheRack=2, replicasPerFailureDomain&gt;2
        // c. FailureDomain=zone, numNodesInTheZone=2, replicasPerFailureDomain&gt;2
        // Therefore, we make an assumption that the replicas must be placed on
        // different nodes. This logic gets us to the equation below. If the user
        // needs to override this assumption, we can provide a flag (like
        // allowReplicasOnSameNode) in the future.

        maxReplica := getMinDeviceSetReplica(sc)
        for _, deviceSet := range sc.Spec.StorageDeviceSets </span><span class="cov8" title="1">{
                if deviceSet.Replica &gt; maxReplica </span><span class="cov8" title="1">{
                        maxReplica = deviceSet.Replica
                }</span>
        }

        <span class="cov8" title="1">return maxReplica * getReplicasPerFailureDomain(sc)</span>
}

func getMonCount(nodeCount int, arbiter bool) int <span class="cov8" title="1">{
        // return static value if overridden
        override := os.Getenv(monCountOverrideEnvVar)
        if override != "" </span><span class="cov0" title="0">{
                count, err := strconv.Atoi(override)
                if err != nil </span><span class="cov0" title="0">{
                        log.Error(err, "could not decode env var %s", monCountOverrideEnvVar)
                }</span> else<span class="cov0" title="0"> {
                        return count
                }</span>
        }

        <span class="cov8" title="1">if arbiter </span><span class="cov0" title="0">{
                return defaults.ArbiterModeMonCount
        }</span>
        <span class="cov8" title="1">return defaults.DefaultMonCount</span>
}

// newStorageClassDeviceSets converts a list of StorageDeviceSets into a list of Rook StorageClassDeviceSets
func newStorageClassDeviceSets(sc *ocsv1.StorageCluster, serverVersion *version.Info) []rook.StorageClassDeviceSet <span class="cov8" title="1">{
        storageDeviceSets := sc.Spec.StorageDeviceSets
        topologyMap := sc.Status.NodeTopologies

        var storageClassDeviceSets []rook.StorageClassDeviceSet

        // For kube server version 1.19 and above, topology spread constraints are used for OSD placements.
        // For kube server version below 1.19, NodeAffinity and PodAntiAffinity are used for OSD placements.
        supportTSC := serverVersion.Major &gt;= defaults.KubeMajorTopologySpreadConstraints &amp;&amp; serverVersion.Minor &gt;= defaults.KubeMinorTopologySpreadConstraints

        for _, ds := range storageDeviceSets </span><span class="cov8" title="1">{
                resources := ds.Resources
                if resources.Requests == nil &amp;&amp; resources.Limits == nil </span><span class="cov8" title="1">{
                        resources = defaults.DaemonResources["osd"]
                }</span>

                <span class="cov8" title="1">portable := ds.Portable

                topologyKey := ds.TopologyKey
                topologyKeyValues := []string{}

                noPlacement := ds.Placement.NodeAffinity == nil &amp;&amp; ds.Placement.PodAffinity == nil &amp;&amp; ds.Placement.PodAntiAffinity == nil
                noPreparePlacement := ds.PreparePlacement.NodeAffinity == nil &amp;&amp; ds.PreparePlacement.PodAffinity == nil &amp;&amp; ds.PreparePlacement.PodAntiAffinity == nil

                if supportTSC </span><span class="cov8" title="1">{
                        noPlacement = noPlacement &amp;&amp; ds.Placement.TopologySpreadConstraints == nil
                        noPreparePlacement = noPreparePlacement &amp;&amp; ds.PreparePlacement.TopologySpreadConstraints == nil
                }</span>

                <span class="cov8" title="1">if noPlacement </span><span class="cov8" title="1">{
                        if topologyKey == "" </span><span class="cov8" title="1">{
                                topologyKey = determineFailureDomain(sc)
                        }</span>

                        <span class="cov8" title="1">if topologyKey == "host" </span><span class="cov0" title="0">{
                                portable = false
                        }</span>

                        <span class="cov8" title="1">if topologyMap != nil </span><span class="cov8" title="1">{
                                topologyKey, topologyKeyValues = topologyMap.GetKeyValues(topologyKey)
                        }</span>
                }

                <span class="cov8" title="1">count := ds.Count
                replica := ds.Replica
                if replica == 0 </span><span class="cov8" title="1">{
                        replica = defaults.DeviceSetReplica

                        // This is a temporary hack in place due to limitations
                        // in the current implementation of the OCP console.
                        // The console is hardcoded to create a StorageCluster
                        // with a Count of 3, as made sense for the previous
                        // behavior, but it cannot be updated until the next
                        // z-stream release of OCP 4.2. This workaround is to
                        // enable the new behavior while the console is waiting
                        // to be updated.
                        // TODO: Remove this behavior when OCP console is updated
                        count = count / 3
                }</span>

                <span class="cov8" title="1">for i := 0; i &lt; replica; i++ </span><span class="cov8" title="1">{
                        placement := rook.Placement{}
                        preparePlacement := rook.Placement{}

                        if noPlacement </span><span class="cov8" title="1">{
                                if supportTSC </span><span class="cov8" title="1">{
                                        in := getPlacement(sc, "osd-tsc")
                                        (&amp;in).DeepCopyInto(&amp;placement)

                                        if noPreparePlacement </span><span class="cov8" title="1">{
                                                in := getPlacement(sc, "osd-prepare-tsc")
                                                (&amp;in).DeepCopyInto(&amp;preparePlacement)

                                                if len(topologyKeyValues) &gt;= replica </span><span class="cov8" title="1">{
                                                        // If topologyKey is not host, append additional topology spread constarint to the
                                                        // default preparePlacement. This serves even distribution at the host level
                                                        // within a failure domain (zone/rack).
                                                        if topologyKey != corev1.LabelHostname </span><span class="cov8" title="1">{
                                                                preparePlacement.TopologySpreadConstraints = append(preparePlacement.TopologySpreadConstraints, preparePlacement.TopologySpreadConstraints[0])
                                                        }</span>
                                                        <span class="cov8" title="1">preparePlacement.TopologySpreadConstraints[0].TopologyKey = topologyKey</span>
                                                }
                                        }
                                } else<span class="cov8" title="1"> {
                                        in := getPlacement(sc, "osd")
                                        (&amp;in).DeepCopyInto(&amp;placement)

                                        if noPreparePlacement </span><span class="cov8" title="1">{
                                                in := getPlacement(sc, "osd-prepare")
                                                (&amp;in).DeepCopyInto(&amp;preparePlacement)
                                        }</span>

                                        <span class="cov8" title="1">if len(topologyKeyValues) &gt;= replica </span><span class="cov8" title="1">{
                                                topologyIndex := i % len(topologyKeyValues)
                                                setTopologyForAffinity(&amp;placement, topologyKeyValues[topologyIndex], topologyKey)
                                                if noPreparePlacement </span><span class="cov8" title="1">{
                                                        setTopologyForAffinity(&amp;preparePlacement, topologyKeyValues[topologyIndex], topologyKey)
                                                }</span>
                                        }
                                }

                                <span class="cov8" title="1">if !noPreparePlacement </span><span class="cov0" title="0">{
                                        preparePlacement = ds.PreparePlacement
                                }</span>
                        } else<span class="cov0" title="0"> if !noPlacement &amp;&amp; noPreparePlacement </span><span class="cov0" title="0">{
                                preparePlacement = ds.Placement
                                placement = ds.Placement
                        }</span> else<span class="cov0" title="0"> {
                                preparePlacement = ds.PreparePlacement
                                placement = ds.Placement
                        }</span>

                        // Annotation crushDeviceClass ensures osd with different CRUSH device class than the one detected by Ceph
                        <span class="cov8" title="1">annotations := map[string]string{
                                "crushDeviceClass": ds.DeviceType,
                        }
                        ds.DataPVCTemplate.Annotations = annotations

                        set := rook.StorageClassDeviceSet{
                                Name:                 fmt.Sprintf("%s-%d", ds.Name, i),
                                Count:                count,
                                Resources:            resources,
                                Placement:            placement,
                                PreparePlacement:     &amp;preparePlacement,
                                Config:               ds.Config.ToMap(),
                                VolumeClaimTemplates: []corev1.PersistentVolumeClaim{ds.DataPVCTemplate},
                                Portable:             portable,
                                TuneSlowDeviceClass:  ds.Config.TuneSlowDeviceClass,
                                TuneFastDeviceClass:  ds.Config.TuneFastDeviceClass,
                                Encrypted:            sc.Spec.Encryption.Enable,
                        }

                        if ds.MetadataPVCTemplate != nil </span><span class="cov8" title="1">{
                                ds.MetadataPVCTemplate.ObjectMeta.Name = metadataPVCName
                                set.VolumeClaimTemplates = append(set.VolumeClaimTemplates, *ds.MetadataPVCTemplate)
                        }</span>
                        <span class="cov8" title="1">if ds.WalPVCTemplate != nil </span><span class="cov8" title="1">{
                                ds.WalPVCTemplate.ObjectMeta.Name = walPVCName
                                set.VolumeClaimTemplates = append(set.VolumeClaimTemplates, *ds.WalPVCTemplate)
                        }</span>

                        <span class="cov8" title="1">storageClassDeviceSets = append(storageClassDeviceSets, set)</span>
                }
        }

        <span class="cov8" title="1">return storageClassDeviceSets</span>
}

func newCephDaemonResources(custom map[string]corev1.ResourceRequirements) map[string]corev1.ResourceRequirements <span class="cov8" title="1">{
        resources := map[string]corev1.ResourceRequirements{
                "mon": defaults.GetDaemonResources("mon", custom),
                "mgr": defaults.GetDaemonResources("mgr", custom),
        }

        for k := range resources </span><span class="cov8" title="1">{
                if r, ok := custom[k]; ok </span><span class="cov0" title="0">{
                        resources[k] = r
                }</span>
        }

        <span class="cov8" title="1">return resources</span>
}

// The checkTuneStorageDevices function checks whether devices from the given
// storage class are a known type that should expclitly be tuned for fast or
// slow access.
func (r *StorageClusterReconciler) checkTuneStorageDevices(ds ocsv1.StorageDeviceSet) (diskSpeed, error) <span class="cov8" title="1">{
        deviceType := ds.DeviceType

        if DeviceTypeHDD == strings.ToLower(deviceType) </span><span class="cov8" title="1">{
                return diskSpeedSlow, nil
        }</span>

        <span class="cov8" title="1">if DeviceTypeSSD == strings.ToLower(deviceType) || DeviceTypeNVMe == strings.ToLower(deviceType) </span><span class="cov8" title="1">{
                return diskSpeedFast, nil
        }</span>

        <span class="cov8" title="1">storageClassName := *ds.DataPVCTemplate.Spec.StorageClassName
        storageClass := &amp;storagev1.StorageClass{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Namespace: "", Name: storageClassName}, storageClass)
        if err != nil </span><span class="cov0" title="0">{
                return diskSpeedUnknown, fmt.Errorf("failed to retrieve StorageClass %q. %+v", storageClassName, err)
        }</span>

        <span class="cov8" title="1">for _, dt := range knownDiskTypes </span><span class="cov8" title="1">{
                if string(dt.provisioner) != storageClass.Provisioner </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">if dt.storageClassType != storageClass.Parameters["type"] </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">return dt.speed, nil</span>
        }

        <span class="cov8" title="1">tuneFastDevices, err := r.DevicesDefaultToFastForThisPlatform()
        if err != nil </span><span class="cov0" title="0">{
                return diskSpeedUnknown, err
        }</span>
        <span class="cov8" title="1">if tuneFastDevices </span><span class="cov8" title="1">{
                return diskSpeedFast, nil
        }</span>

        // not a known disk type, don't tune
        <span class="cov8" title="1">return diskSpeedUnknown, nil</span>
}

func allowUnsupportedCephVersion() bool <span class="cov8" title="1">{
        return defaults.IsUnsupportedCephVersionAllowed == "allowed"
}</span>

func generateStretchClusterSpec(sc *ocsv1.StorageCluster) *cephv1.StretchClusterSpec <span class="cov0" title="0">{
        var zones []string
        stretchClusterSpec := cephv1.StretchClusterSpec{}
        stretchClusterSpec.FailureDomainLabel, zones = sc.Status.NodeTopologies.GetKeyValues(determineFailureDomain(sc))

        for _, zone := range zones </span><span class="cov0" title="0">{
                if zone == sc.Spec.NodeTopologies.ArbiterLocation </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">stretchClusterSpec.Zones = append(stretchClusterSpec.Zones, cephv1.StretchClusterZoneSpec{
                        Name:    zone,
                        Arbiter: false,
                })</span>
        }

        <span class="cov0" title="0">arbiterZoneSpec := cephv1.StretchClusterZoneSpec{
                Name:    sc.Spec.NodeTopologies.ArbiterLocation,
                Arbiter: true,
        }
        if sc.Spec.Arbiter.ArbiterMonPVCTemplate != nil </span><span class="cov0" title="0">{
                arbiterZoneSpec.VolumeClaimTemplate = sc.Spec.Arbiter.ArbiterMonPVCTemplate
        }</span>
        <span class="cov0" title="0">stretchClusterSpec.Zones = append(stretchClusterSpec.Zones, arbiterZoneSpec)

        return &amp;stretchClusterSpec</span>
}

func generateMonSpec(sc *ocsv1.StorageCluster, nodeCount int) cephv1.MonSpec <span class="cov8" title="1">{
        if arbiterEnabled(sc) </span><span class="cov0" title="0">{
                return cephv1.MonSpec{
                        Count:                getMonCount(nodeCount, true),
                        AllowMultiplePerNode: false,
                        StretchCluster:       generateStretchClusterSpec(sc),
                }
        }</span>

        <span class="cov8" title="1">return cephv1.MonSpec{
                Count:                getMonCount(nodeCount, false),
                AllowMultiplePerNode: false,
        }</span>
}

func getCephObjectStoreGatewayInstances(sc *ocsv1.StorageCluster) int32 <span class="cov8" title="1">{
        if arbiterEnabled(sc) </span><span class="cov0" title="0">{
                return int32(defaults.ArbiterCephObjectStoreGatewayInstances)
        }</span>
        <span class="cov8" title="1">return int32(defaults.CephObjectStoreGatewayInstances)</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsCephFilesystems struct{}

// newCephFilesystemInstances returns the cephFilesystem instances that should be created
// on first run.
func (r *StorageClusterReconciler) newCephFilesystemInstances(initData *ocsv1.StorageCluster) ([]*cephv1.CephFilesystem, error) <span class="cov8" title="1">{
        ret := []*cephv1.CephFilesystem{
                {
                        ObjectMeta: metav1.ObjectMeta{
                                Name:      generateNameForCephFilesystem(initData),
                                Namespace: initData.Namespace,
                        },
                        Spec: cephv1.FilesystemSpec{
                                MetadataPool: cephv1.PoolSpec{
                                        Replicated:    generateCephReplicatedSpec(initData, "metadata"),
                                        FailureDomain: initData.Status.FailureDomain,
                                },
                                DataPools: []cephv1.PoolSpec{
                                        {
                                                Replicated:    generateCephReplicatedSpec(initData, "data"),
                                                FailureDomain: initData.Status.FailureDomain,
                                        },
                                },
                                MetadataServer: cephv1.MetadataServerSpec{
                                        ActiveCount:   1,
                                        ActiveStandby: true,
                                        Placement:     getPlacement(initData, "mds"),
                                        Resources:     defaults.GetDaemonResources("mds", initData.Spec.Resources),
                                },
                        },
                },
        }
        for _, obj := range ret </span><span class="cov8" title="1">{
                err := controllerutil.SetControllerReference(initData, obj, r.Scheme)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }
        <span class="cov8" title="1">return ret, nil</span>
}

// ensureCreated ensures that cephFilesystem resources exist in the desired
// state.
func (obj *ocsCephFilesystems) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephFilesystems.ReconcileStrategy)
        if reconcileStrategy == ReconcileStrategyIgnore </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov8" title="1">cephFilesystems, err := r.newCephFilesystemInstances(instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">for _, cephFilesystem := range cephFilesystems </span><span class="cov8" title="1">{
                existing := cephv1.CephFilesystem{}
                err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephFilesystem.Name, Namespace: cephFilesystem.Namespace}, &amp;existing)
                switch </span>{
                case err == nil:<span class="cov0" title="0">
                        if reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                                return nil
                        }</span>
                        <span class="cov0" title="0">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                r.Log.Info(fmt.Sprintf("Unable to restore init object because %s is marked for deletion", existing.Name))
                                return fmt.Errorf("failed to restore initialization object %s because it is marked for deletion", existing.Name)
                        }</span>

                        <span class="cov0" title="0">r.Log.Info(fmt.Sprintf("Restoring original cephFilesystem %s", cephFilesystem.Name))
                        existing.ObjectMeta.OwnerReferences = cephFilesystem.ObjectMeta.OwnerReferences
                        cephFilesystem.ObjectMeta = existing.ObjectMeta
                        err = r.Client.Update(context.TODO(), cephFilesystem)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                case errors.IsNotFound(err):<span class="cov8" title="1">
                        r.Log.Info(fmt.Sprintf("Creating cephFilesystem %s", cephFilesystem.Name))
                        err = r.Client.Create(context.TODO(), cephFilesystem)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
        }

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the CephFilesystems owned by the StorageCluster
func (obj *ocsCephFilesystems) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        foundCephFilesystem := &amp;cephv1.CephFilesystem{}
        cephFilesystems, err := r.newCephFilesystemInstances(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, cephFilesystem := range cephFilesystems </span><span class="cov8" title="1">{
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephFilesystem.Name, Namespace: sc.Namespace}, foundCephFilesystem)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephFilesystem not found", "CephFilesystem Name", cephFilesystem.Name)
                                continue</span>
                        }
                        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve cephFilesystem %v: %v", cephFilesystem.Name, err)</span>
                }

                <span class="cov8" title="1">if cephFilesystem.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: Deleting cephFilesystem", "CephFilesystem Name", cephFilesystem.Name)
                        err = r.Client.Delete(context.TODO(), foundCephFilesystem)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("Uninstall: Failed to delete cephFilesystem %v: %v", foundCephFilesystem.Name, err)
                        }</span>
                }

                <span class="cov8" title="1">err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephFilesystem.Name, Namespace: sc.Namespace}, foundCephFilesystem)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephFilesystem is deleted", "CephFilesystem Name", cephFilesystem.Name)
                                continue</span>
                        }
                }
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting for cephFilesystem %v to be deleted", cephFilesystem.Name)</span>

        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file7" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsCephObjectStores struct{}

// ensureCreated ensures that CephObjectStore resources exist in the desired
// state.
func (obj *ocsCephObjectStores) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephObjectStores.ReconcileStrategy)
        if reconcileStrategy == ReconcileStrategyIgnore </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">platform, err := r.platform.GetPlatform(r.Client)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">isIBMWithSecret, err := isIBMPlatformWithCosSecret(platform, instance.Namespace, r.Client)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if avoidObjectStore(platform) || isIBMWithSecret </span><span class="cov8" title="1">{
                r.Log.Info(fmt.Sprintf("not creating a CephObjectStore because the platform is '%s'", platform))
                return nil
        }</span>

        <span class="cov8" title="1">cephObjectStores, err := r.newCephObjectStoreInstances(instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">err = r.createCephObjectStores(cephObjectStores, instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "could not create CephObjectStores")
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the CephObjectStores owned by the StorageCluster
func (obj *ocsCephObjectStores) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        foundCephObjectStore := &amp;cephv1.CephObjectStore{}
        cephObjectStores, err := r.newCephObjectStoreInstances(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, cephObjectStore := range cephObjectStores </span><span class="cov8" title="1">{
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStore.Name, Namespace: sc.Namespace}, foundCephObjectStore)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephObjectStore not found", "CephObjectStore Name", cephObjectStore.Name)
                                continue</span>
                        }
                        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve cephObjectStore %v: %v", cephObjectStore.Name, err)</span>
                }

                <span class="cov8" title="1">if cephObjectStore.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: Deleting cephObjectStore", "CephObjectStore Name", cephObjectStore.Name)
                        err = r.Client.Delete(context.TODO(), foundCephObjectStore)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("Uninstall: Failed to delete cephObjectStore %v: %v", foundCephObjectStore.Name, err)
                        }</span>
                }

                <span class="cov8" title="1">err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStore.Name, Namespace: sc.Namespace}, foundCephObjectStore)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephObjectStore is deleted", "CephObjectStore Name", cephObjectStore.Name)
                                continue</span>
                        }
                }
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting for cephObjectStore %v to be deleted", cephObjectStore.Name)</span>

        }
        <span class="cov8" title="1">return nil</span>
}

// createCephObjectStore creates CephObjectStore in the desired state
func (r *StorageClusterReconciler) createCephObjectStores(cephObjectStores []*cephv1.CephObjectStore, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        for _, cephObjectStore := range cephObjectStores </span><span class="cov8" title="1">{
                existing := cephv1.CephObjectStore{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStore.Name, Namespace: cephObjectStore.Namespace}, &amp;existing)
                switch </span>{
                case err == nil:<span class="cov8" title="1">
                        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephObjectStores.ReconcileStrategy)
                        if reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                                return nil
                        }</span>
                        <span class="cov8" title="1">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                err := fmt.Errorf("failed to restore cephobjectstore object %s because it is marked for deletion", existing.Name)
                                r.Log.Info("cephobjectstore restore failed")
                                return err
                        }</span>

                        <span class="cov8" title="1">r.Log.Info(fmt.Sprintf("Restoring original cephObjectStore %s", cephObjectStore.Name))
                        existing.ObjectMeta.OwnerReferences = cephObjectStore.ObjectMeta.OwnerReferences
                        cephObjectStore.ObjectMeta = existing.ObjectMeta
                        err = r.Client.Update(context.TODO(), cephObjectStore)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("failed to update CephObjectStore Object: %s", cephObjectStore.Name))
                                return err
                        }</span>
                case errors.IsNotFound(err):<span class="cov8" title="1">
                        r.Log.Info(fmt.Sprintf("creating CephObjectStore %s", cephObjectStore.Name))
                        err = r.Client.Create(context.TODO(), cephObjectStore)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("failed to create CephObjectStore object: %s", cephObjectStore.Name))
                                return err
                        }</span>
                }
        }
        <span class="cov8" title="1">return nil</span>
}

// newCephObjectStoreInstances returns the cephObjectStore instances that should be created
// on first run.
func (r *StorageClusterReconciler) newCephObjectStoreInstances(initData *ocsv1.StorageCluster) ([]*cephv1.CephObjectStore, error) <span class="cov8" title="1">{
        gatewayInstances := initData.Spec.ManagedResources.CephObjectStores.GatewayInstances
        if gatewayInstances == 0 </span><span class="cov8" title="1">{
                gatewayInstances = getCephObjectStoreGatewayInstances(initData)
        }</span>
        <span class="cov8" title="1">ret := []*cephv1.CephObjectStore{
                {
                        ObjectMeta: metav1.ObjectMeta{
                                Name:      generateNameForCephObjectStore(initData),
                                Namespace: initData.Namespace,
                        },
                        Spec: cephv1.ObjectStoreSpec{
                                PreservePoolsOnDelete: false,
                                DataPool: cephv1.PoolSpec{
                                        FailureDomain: initData.Status.FailureDomain,
                                        Replicated:    generateCephReplicatedSpec(initData, "data"),
                                },
                                MetadataPool: cephv1.PoolSpec{
                                        FailureDomain: initData.Status.FailureDomain,
                                        Replicated:    generateCephReplicatedSpec(initData, "metadata"),
                                },
                                Gateway: cephv1.GatewaySpec{
                                        Port:      80,
                                        Instances: gatewayInstances,
                                        Placement: getPlacement(initData, "rgw"),
                                        Resources: defaults.GetDaemonResources("rgw", initData.Spec.Resources),
                                },
                        },
                },
        }
        for _, obj := range ret </span><span class="cov8" title="1">{
                err := controllerutil.SetControllerReference(initData, obj, r.Scheme)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Failed to set ControllerReference to %s", obj.Name))
                        return nil, err
                }</span>
        }
        <span class="cov8" title="1">return ret, nil</span>
}
</pre>
		
		<pre class="file" id="file8" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsCephObjectStoreUsers struct{}

// newCephObjectStoreUserInstances returns the cephObjectStoreUser instances that should be created
// on first run.
func (r *StorageClusterReconciler) newCephObjectStoreUserInstances(initData *ocsv1.StorageCluster) ([]*cephv1.CephObjectStoreUser, error) <span class="cov8" title="1">{
        ret := []*cephv1.CephObjectStoreUser{
                {
                        ObjectMeta: metav1.ObjectMeta{
                                Name:      generateNameForCephObjectStoreUser(initData),
                                Namespace: initData.Namespace,
                        },
                        Spec: cephv1.ObjectStoreUserSpec{
                                DisplayName: initData.Name,
                                Store:       generateNameForCephObjectStore(initData),
                        },
                },
        }
        for _, obj := range ret </span><span class="cov8" title="1">{
                err := controllerutil.SetControllerReference(initData, obj, r.Scheme)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }
        <span class="cov8" title="1">return ret, nil</span>
}

// ensureCreated ensures that cephObjectStoreUser resources exist in the desired
// state.
func (obj *ocsCephObjectStoreUsers) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephObjectStoreUsers.ReconcileStrategy)
        if reconcileStrategy == ReconcileStrategyIgnore </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">platform, err := r.platform.GetPlatform(r.Client)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">isIBMWithSecret, err := isIBMPlatformWithCosSecret(platform, instance.Namespace, r.Client)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if avoidObjectStore(platform) || isIBMWithSecret </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">cephObjectStoreUsers, err := r.newCephObjectStoreUserInstances(instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">err = r.createCephObjectStoreUsers(cephObjectStoreUsers, instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "could not create CephObjectStoresUsers")
                return err
        }</span>

        <span class="cov8" title="1">return err</span>
}

// ensureDeleted deletes the CephObjectStoreUsers owned by the StorageCluster
func (obj *ocsCephObjectStoreUsers) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        foundCephObjectStoreUser := &amp;cephv1.CephObjectStoreUser{}
        cephObjectStoreUsers, err := r.newCephObjectStoreUserInstances(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, cephObjectStoreUser := range cephObjectStoreUsers </span><span class="cov8" title="1">{
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStoreUser.Name, Namespace: sc.Namespace}, foundCephObjectStoreUser)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephObjectStoreUser not found", "CephObjectStoreUser Name", cephObjectStoreUser.Name)
                                continue</span>
                        }
                        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve cephObjectStoreUser %v: %v", cephObjectStoreUser.Name, err)</span>
                }

                <span class="cov8" title="1">if cephObjectStoreUser.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: Deleting cephObjectStoreUser", "CephObjectStoreUser Name", cephObjectStoreUser.Name)
                        err = r.Client.Delete(context.TODO(), foundCephObjectStoreUser)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("Uninstall: Failed to delete cephObjectStoreUser %v: %v", foundCephObjectStoreUser.Name, err)
                        }</span>
                }

                <span class="cov8" title="1">err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStoreUser.Name, Namespace: sc.Namespace}, foundCephObjectStoreUser)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                r.Log.Info("Uninstall: CephObjectStoreUser is deleted", "CephObjectStoreUser Name", cephObjectStoreUser.Name)
                                continue</span>
                        }
                }
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting for cephObjectStoreUser %v to be deleted", cephObjectStoreUser.Name)</span>

        }
        <span class="cov8" title="1">return nil</span>
}

// createCephObjectStoreUsers creates CephObjectStoreUsers in the desired state
func (r *StorageClusterReconciler) createCephObjectStoreUsers(cephObjectStoreUsers []*cephv1.CephObjectStoreUser, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        for _, cephObjectStoreUser := range cephObjectStoreUsers </span><span class="cov8" title="1">{
                existing := cephv1.CephObjectStoreUser{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: cephObjectStoreUser.Name, Namespace: cephObjectStoreUser.Namespace}, &amp;existing)
                switch </span>{
                case err == nil:<span class="cov8" title="1">
                        reconcileStrategy := ReconcileStrategy(instance.Spec.ManagedResources.CephObjectStoreUsers.ReconcileStrategy)
                        if reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                                return nil
                        }</span>
                        <span class="cov8" title="1">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                r.Log.Info(fmt.Sprintf("Unable to restore init object because %s is marked for deletion", existing.Name))
                                return fmt.Errorf("failed to restore initialization object %s because it is marked for deletion", existing.Name)
                        }</span>

                        <span class="cov8" title="1">r.Log.Info(fmt.Sprintf("Restoring original cephObjectStoreUser %s", cephObjectStoreUser.Name))
                        existing.ObjectMeta.OwnerReferences = cephObjectStoreUser.ObjectMeta.OwnerReferences
                        cephObjectStoreUser.ObjectMeta = existing.ObjectMeta
                        err = r.Client.Update(context.TODO(), cephObjectStoreUser)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                case errors.IsNotFound(err):<span class="cov8" title="1">
                        r.Log.Info(fmt.Sprintf("Creating cephObjectStoreUser %s", cephObjectStoreUser.Name))
                        err = r.Client.Create(context.TODO(), cephObjectStoreUser)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file9" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
        corev1 "k8s.io/api/core/v1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/intstr"
)

const (
        exporterName   = "ocs-metrics-exporter"
        portMetrics    = "metrics"
        portExporter   = "exporter"
        metricsPath    = "/metrics"
        scrapeInterval = "1m"
)

var exporterLabels = map[string]string{
        "app.kubernetes.io/component": exporterName,
        "app.kubernetes.io/name":      exporterName,
}

// enableMetricsExporter is a wrapper around CreateOrUpdateService()
// and CreateOrUpdateServiceMonitor()
func (r *StorageClusterReconciler) enableMetricsExporter(instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        _, err := CreateOrUpdateService(r, instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">_, err = CreateOrUpdateServiceMonitor(r, instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">return nil</span>
}

func getMetricsExporterService(instance *ocsv1.StorageCluster) *corev1.Service <span class="cov8" title="1">{
        service := &amp;corev1.Service{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      exporterName,
                        Namespace: instance.Namespace,
                        Labels:    exporterLabels,
                        OwnerReferences: []metav1.OwnerReference{
                                {
                                        APIVersion: instance.APIVersion,
                                        Kind:       instance.Kind,
                                        Name:       instance.Name,
                                        UID:        instance.UID,
                                },
                        },
                },
                Spec: corev1.ServiceSpec{
                        Type: corev1.ServiceTypeClusterIP,
                        Ports: []corev1.ServicePort{
                                {
                                        Name:     portMetrics,
                                        Port:     int32(8080),
                                        Protocol: corev1.ProtocolTCP,
                                        TargetPort: intstr.IntOrString{
                                                Type:   intstr.Int,
                                                IntVal: int32(8080),
                                                StrVal: "8080",
                                        },
                                },
                                {
                                        Name:     portExporter,
                                        Port:     int32(8081),
                                        Protocol: corev1.ProtocolTCP,
                                        TargetPort: intstr.IntOrString{
                                                Type:   intstr.Int,
                                                IntVal: int32(8081),
                                                StrVal: "8081",
                                        },
                                },
                        },
                        Selector: exporterLabels,
                },
        }
        return service
}</span>

// CreateOrUpdateService creates service object or an error
func CreateOrUpdateService(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) (*corev1.Service, error) <span class="cov8" title="1">{
        service := getMetricsExporterService(instance)
        namespacedName := types.NamespacedName{Namespace: service.GetNamespace(), Name: service.GetName()}

        r.Log.Info("Reconciling metrics exporter service", "NamespacedName", namespacedName)

        oldService := &amp;corev1.Service{}
        err := r.Client.Get(context.TODO(), namespacedName, oldService)
        if err != nil </span><span class="cov8" title="1">{
                if apierrors.IsNotFound(err) </span><span class="cov8" title="1">{
                        err = r.Client.Create(context.TODO(), service)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to create metrics exporter service %v. %v", namespacedName, err)
                        }</span>
                        <span class="cov8" title="1">return service, nil</span>
                }
                <span class="cov0" title="0">return nil, fmt.Errorf("failed to retrieve metrics exporter service %v. %v", namespacedName, err)</span>
        }
        <span class="cov8" title="1">service.ResourceVersion = oldService.ResourceVersion
        service.Spec.ClusterIP = oldService.Spec.ClusterIP
        err = r.Client.Update(context.TODO(), service)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to update service %v. %v", namespacedName, err)
        }</span>
        <span class="cov8" title="1">return service, nil</span>
}

func getMetricsExporterServiceMonitor(instance *ocsv1.StorageCluster) *monitoringv1.ServiceMonitor <span class="cov8" title="1">{
        serviceMonitor := &amp;monitoringv1.ServiceMonitor{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      exporterName,
                        Namespace: instance.Namespace,
                        Labels:    exporterLabels,
                        OwnerReferences: []metav1.OwnerReference{
                                {
                                        APIVersion: instance.APIVersion,
                                        Kind:       instance.Kind,
                                        Name:       instance.Name,
                                        UID:        instance.UID,
                                },
                        },
                },
                Spec: monitoringv1.ServiceMonitorSpec{
                        NamespaceSelector: monitoringv1.NamespaceSelector{
                                MatchNames: []string{instance.Namespace},
                        },
                        Selector: metav1.LabelSelector{
                                MatchLabels: exporterLabels,
                        },
                        Endpoints: []monitoringv1.Endpoint{
                                {
                                        Port:     portMetrics,
                                        Path:     metricsPath,
                                        Interval: scrapeInterval,
                                },
                                {
                                        Port:     portExporter,
                                        Path:     metricsPath,
                                        Interval: scrapeInterval,
                                },
                        },
                },
        }
        return serviceMonitor
}</span>

// CreateOrUpdateServiceMonitor creates serviceMonitor object or an error
func CreateOrUpdateServiceMonitor(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) (*monitoringv1.ServiceMonitor, error) <span class="cov8" title="1">{
        serviceMonitor := getMetricsExporterServiceMonitor(instance)
        namespacedName := types.NamespacedName{Name: serviceMonitor.Name, Namespace: serviceMonitor.Namespace}

        r.Log.Info("Reconciling metrics exporter service monitor", "NamespacedName", namespacedName)

        oldSm := &amp;monitoringv1.ServiceMonitor{}
        err := r.Client.Get(context.TODO(), namespacedName, oldSm)
        if err != nil </span><span class="cov8" title="1">{
                if apierrors.IsNotFound(err) </span><span class="cov8" title="1">{
                        err = r.Client.Create(context.TODO(), serviceMonitor)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to create metrics exporter servicemonitor %v. %v", namespacedName, err)
                        }</span>
                        <span class="cov8" title="1">return serviceMonitor, nil</span>
                }
                <span class="cov0" title="0">return nil, fmt.Errorf("failed to retrieve metrics exporter servicemonitor %v. %v", namespacedName, err)</span>
        }
        <span class="cov8" title="1">serviceMonitor.ResourceVersion = oldSm.ResourceVersion
        err = r.Client.Update(context.TODO(), serviceMonitor)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to update metrics exporter servicemonitor %v. %v", namespacedName, err)
        }</span>
        <span class="cov8" title="1">return serviceMonitor, nil</span>
}
</pre>
		
		<pre class="file" id="file10" style="display: none">package storagecluster

import (
        "context"
        "crypto/sha512"
        "encoding/json"
        "fmt"
        "net"
        "regexp"
        "strconv"
        "strings"
        "time"

        "github.com/go-logr/logr"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
)

const (
        externalClusterDetailsSecret = "rook-ceph-external-cluster-details"
        externalClusterDetailsKey    = "external_cluster_details"
        cephFsStorageClassName       = "cephfs"
        cephRbdStorageClassName      = "ceph-rbd"
        cephRgwStorageClassName      = "ceph-rgw"
        externalCephRgwEndpointKey   = "endpoint"
)

const (
        rookCephOperatorConfigName = "rook-ceph-operator-config"
        rookEnableCephFSCSIKey     = "ROOK_CSI_ENABLE_CEPHFS"
)

// ExternalResource contains a list of External Cluster Resources
type ExternalResource struct {
        Kind string            `json:"kind"`
        Data map[string]string `json:"data"`
        Name string            `json:"name"`
}

type ocsExternalResources struct{}

// setRookCSICephFS function enables or disables the 'ROOK_CSI_ENABLE_CEPHFS' key
func (r *StorageClusterReconciler) setRookCSICephFS(
        enableDisableFlag bool, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        rookCephOperatorConfig := &amp;corev1.ConfigMap{}
        err := r.Client.Get(context.TODO(),
                types.NamespacedName{Name: rookCephOperatorConfigName, Namespace: instance.ObjectMeta.Namespace},
                rookCephOperatorConfig)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, fmt.Sprintf("Unable to get '%s' config", rookCephOperatorConfigName))
                return err
        }</span>
        <span class="cov8" title="1">enableDisableFlagStr := fmt.Sprintf("%v", enableDisableFlag)
        // if the current state of 'ROOK_CSI_ENABLE_CEPHFS' flag is same, just return
        if rookCephOperatorConfig.Data[rookEnableCephFSCSIKey] == enableDisableFlagStr </span><span class="cov8" title="1">{
                return nil
        }</span>
        <span class="cov8" title="1">rookCephOperatorConfig.Data[rookEnableCephFSCSIKey] = enableDisableFlagStr
        return r.Client.Update(context.TODO(), rookCephOperatorConfig)</span>
}

func checkEndpointReachable(endpoint string, timeout time.Duration) error <span class="cov8" title="1">{
        rxp := regexp.MustCompile(`^http[s]?://`)
        // remove any http or https protocols from the endpoint string
        endpoint = rxp.ReplaceAllString(endpoint, "")
        con, err := net.DialTimeout("tcp", endpoint, timeout)
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>
        <span class="cov8" title="1">defer con.Close()
        return nil</span>
}

func sha512sum(tobeHashed []byte) (string, error) <span class="cov8" title="1">{
        h := sha512.New()
        if _, err := h.Write(tobeHashed); err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov8" title="1">return fmt.Sprintf("%x", h.Sum(nil)), nil</span>
}

func (r *StorageClusterReconciler) externalSecretDataChecksum(instance *ocsv1.StorageCluster) (string, error) <span class="cov8" title="1">{
        found, err := r.retrieveSecret(externalClusterDetailsSecret, instance)
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>
        <span class="cov8" title="1">return sha512sum(found.Data[externalClusterDetailsKey])</span>
}

func (r *StorageClusterReconciler) sameExternalSecretData(instance *ocsv1.StorageCluster) bool <span class="cov8" title="1">{
        extSecretChecksum, err := r.externalSecretDataChecksum(instance)
        if err != nil </span><span class="cov0" title="0">{
                return false
        }</span>
        // if the 'ExternalSecretHash' and fetched hash are same, then return true
        <span class="cov8" title="1">if instance.Status.ExternalSecretHash == extSecretChecksum </span><span class="cov8" title="1">{
                return true
        }</span>
        // at this point the checksums are different, so update it
        <span class="cov8" title="1">instance.Status.ExternalSecretHash = extSecretChecksum
        return false</span>
}

// retrieveSecret function retrieves the secret object with the specified name
func (r *StorageClusterReconciler) retrieveSecret(secretName string, instance *ocsv1.StorageCluster) (*corev1.Secret, error) <span class="cov8" title="1">{
        found := &amp;corev1.Secret{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      secretName,
                        Namespace: instance.Namespace,
                },
        }
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: found.Name, Namespace: found.Namespace}, found)
        return found, err
}</span>

// retrieveExternalSecretData function retrieves the external secret and returns the data it contains
func (r *StorageClusterReconciler) retrieveExternalSecretData(
        instance *ocsv1.StorageCluster) ([]ExternalResource, error) <span class="cov8" title="1">{
        found, err := r.retrieveSecret(externalClusterDetailsSecret, instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "could not find the external secret resource")
                return nil, err
        }</span>
        <span class="cov8" title="1">var data []ExternalResource
        err = json.Unmarshal(found.Data[externalClusterDetailsKey], &amp;data)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "could not parse json blob")
                return nil, err
        }</span>
        <span class="cov8" title="1">return data, nil</span>
}

func newExternalGatewaySpec(rgwEndpoint string, reqLogger logr.Logger) (*cephv1.GatewaySpec, error) <span class="cov8" title="1">{
        var gateWay cephv1.GatewaySpec
        hostIP, portStr, err := net.SplitHostPort(rgwEndpoint)
        if err != nil </span><span class="cov0" title="0">{
                reqLogger.Error(err,
                        fmt.Sprintf("invalid rgw endpoint provided: %s", rgwEndpoint))
                return nil, err
        }</span>
        <span class="cov8" title="1">if hostIP == "" </span><span class="cov0" title="0">{
                err := fmt.Errorf("An empty rgw host 'IP' address found")
                reqLogger.Error(err, "Host IP should not be empty in rgw endpoint")
                return nil, err
        }</span>
        <span class="cov8" title="1">gateWay.ExternalRgwEndpoints = []corev1.EndpointAddress{{IP: hostIP}}
        var portInt64 int64
        if portInt64, err = strconv.ParseInt(portStr, 10, 32); err != nil </span><span class="cov0" title="0">{
                reqLogger.Error(err,
                        fmt.Sprintf("invalid rgw 'port' provided: %s", portStr))
                return nil, err
        }</span>
        <span class="cov8" title="1">gateWay.Port = int32(portInt64)
        return &amp;gateWay, nil</span>
}

// newExternalCephObjectStoreInstances returns a set of CephObjectStores
// needed for external cluster mode
func (r *StorageClusterReconciler) newExternalCephObjectStoreInstances(
        initData *ocsv1.StorageCluster, rgwEndpoint string) ([]*cephv1.CephObjectStore, error) <span class="cov8" title="1">{
        // check whether the provided rgw endpoint is empty
        if rgwEndpoint = strings.TrimSpace(rgwEndpoint); rgwEndpoint == "" </span><span class="cov0" title="0">{
                r.Log.Info("WARNING: Empty RGW Endpoint specified, external CephObjectStore won't be created")
                return nil, nil
        }</span>
        <span class="cov8" title="1">gatewaySpec, err := newExternalGatewaySpec(rgwEndpoint, r.Log)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // enable bucket healthcheck
        <span class="cov8" title="1">healthCheck := cephv1.BucketHealthCheckSpec{
                Bucket: cephv1.HealthCheckSpec{
                        Disabled: false,
                        Interval: "60s",
                },
        }
        retObj := &amp;cephv1.CephObjectStore{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      generateNameForCephObjectStore(initData),
                        Namespace: initData.Namespace,
                },
                Spec: cephv1.ObjectStoreSpec{
                        Gateway:     *gatewaySpec,
                        HealthCheck: healthCheck,
                },
        }
        retArrObj := []*cephv1.CephObjectStore{
                retObj,
        }
        return retArrObj, nil</span>
}

// ensureCreated ensures that requested resources for the external cluster
// being created
func (obj *ocsExternalResources) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        if r.sameExternalSecretData(instance) </span><span class="cov8" title="1">{
                return nil
        }</span>
        <span class="cov8" title="1">err := r.createExternalStorageClusterResources(instance)
        if err != nil </span><span class="cov8" title="1">{
                r.Log.Error(err, "could not create ExternalStorageClusterResource")
                return err
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted is dummy func for the ocsExternalResources
func (obj *ocsExternalResources) ensureDeleted(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov0" title="0">{
        return nil
}</span>

// createExternalStorageClusterResources creates external cluster resources
func (r *StorageClusterReconciler) createExternalStorageClusterResources(instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        ownerRef := metav1.OwnerReference{
                UID:        instance.UID,
                APIVersion: instance.APIVersion,
                Kind:       instance.Kind,
                Name:       instance.Name,
        }
        // this flag sets the 'ROOK_CSI_ENABLE_CEPHFS' flag
        enableRookCSICephFS := false
        // this stores only the StorageClasses specified in the Secret
        availableSCCs := []StorageClassConfiguration{}
        data, err := r.retrieveExternalSecretData(instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed to retrieve external resources")
                return err
        }</span>
        <span class="cov8" title="1">var extCephObjectStores []*cephv1.CephObjectStore
        for _, d := range data </span><span class="cov8" title="1">{
                objectMeta := metav1.ObjectMeta{
                        Name:            d.Name,
                        Namespace:       instance.Namespace,
                        OwnerReferences: []metav1.OwnerReference{ownerRef},
                }
                objectKey := types.NamespacedName{Name: d.Name, Namespace: instance.Namespace}
                switch d.Kind </span>{
                case "CephCluster":<span class="cov8" title="1">
                        monitoringIP, ok := d.Data["MonitoringEndpoint"]
                        if !ok || monitoringIP == "" </span><span class="cov0" title="0">{
                                err := fmt.Errorf(
                                        "Monitoring Endpoint not present in the external cluster secret %s",
                                        externalClusterDetailsSecret)
                                r.Log.Error(err, "Failed to get Monitoring IP.")
                                return err
                        }</span>
                        <span class="cov8" title="1">monitoringPort := d.Data["MonitoringPort"]
                        if monitoringPort != "" </span><span class="cov8" title="1">{
                                err := checkEndpointReachable(net.JoinHostPort(monitoringIP, monitoringPort), 5*time.Second)
                                if err != nil </span><span class="cov8" title="1">{
                                        r.Log.Error(err, "Monitoring validation failed")
                                        return err
                                }</span>
                                <span class="cov8" title="1">r.monitoringPort = monitoringPort</span>
                        }
                        <span class="cov8" title="1">r.Log.Info("Monitoring Information found. Monitoring will be enabled on the external cluster")
                        r.monitoringIP = monitoringIP</span>
                case "ConfigMap":<span class="cov8" title="1">
                        cm := &amp;corev1.ConfigMap{
                                ObjectMeta: objectMeta,
                                Data:       d.Data,
                        }
                        found := &amp;corev1.ConfigMap{ObjectMeta: objectMeta}
                        err := r.createExternalStorageClusterConfigMap(cm, found, objectKey)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, "could not create ExternalStorageClusterConfigMap")
                                return err
                        }</span>
                case "Secret":<span class="cov8" title="1">
                        sec := &amp;corev1.Secret{
                                ObjectMeta: objectMeta,
                                Data:       make(map[string][]byte),
                        }
                        for k, v := range d.Data </span><span class="cov8" title="1">{
                                sec.Data[k] = []byte(v)
                        }</span>
                        <span class="cov8" title="1">found := &amp;corev1.Secret{ObjectMeta: objectMeta}
                        err := r.createExternalStorageClusterSecret(sec, found, objectKey)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, "could not create ExternalStorageClusterSecret")
                                return err
                        }</span>
                case "StorageClass":<span class="cov8" title="1">
                        var scc StorageClassConfiguration
                        if d.Name == cephFsStorageClassName </span><span class="cov8" title="1">{
                                scc = newCephFilesystemStorageClassConfiguration(instance)
                                enableRookCSICephFS = true
                        }</span> else<span class="cov8" title="1"> if d.Name == cephRbdStorageClassName </span><span class="cov8" title="1">{
                                scc = newCephBlockPoolStorageClassConfiguration(instance)
                        }</span> else<span class="cov8" title="1"> if d.Name == cephRgwStorageClassName </span><span class="cov8" title="1">{
                                rgwEndpoint := d.Data[externalCephRgwEndpointKey]
                                if err := checkEndpointReachable(rgwEndpoint, 5*time.Second); err != nil </span><span class="cov0" title="0">{
                                        r.Log.Error(err, fmt.Sprintf("RGW endpoint, %q, is not reachable", rgwEndpoint))
                                        return err
                                }</span>
                                <span class="cov8" title="1">extCephObjectStores, err = r.newExternalCephObjectStoreInstances(instance, rgwEndpoint)
                                if err != nil </span><span class="cov0" title="0">{
                                        return err
                                }</span>
                                // rgw-endpoint is no longer needed in the 'd.Data' dictionary,
                                // and can be deleted
                                // created an issue in rook to add `CephObjectStore` type directly in the JSON output
                                // https://github.com/rook/rook/issues/6165
                                <span class="cov8" title="1">delete(d.Data, externalCephRgwEndpointKey)

                                scc = newCephOBCStorageClassConfiguration(instance)</span>
                        }
                        // now sc is pointing to appropriate StorageClass,
                        // whose parameters have to be updated
                        <span class="cov8" title="1">for k, v := range d.Data </span><span class="cov8" title="1">{
                                scc.storageClass.Parameters[k] = v
                        }</span>
                        <span class="cov8" title="1">availableSCCs = append(availableSCCs, scc)</span>
                }
        }
        // creating only the available storageClasses
        <span class="cov8" title="1">err = r.createStorageClasses(availableSCCs)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed to create needed StorageClasses")
                return err
        }</span>
        <span class="cov8" title="1">if err = r.setRookCSICephFS(enableRookCSICephFS, instance); err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err,
                        fmt.Sprintf("failed to set '%s' to %v", rookEnableCephFSCSIKey, enableRookCSICephFS))
                return err
        }</span>
        <span class="cov8" title="1">if extCephObjectStores != nil </span><span class="cov8" title="1">{
                if err = r.createCephObjectStores(extCephObjectStores, instance); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// createExternalStorageClusterConfigMap creates configmap for external cluster
func (r *StorageClusterReconciler) createExternalStorageClusterConfigMap(cm *corev1.ConfigMap, found *corev1.ConfigMap, objectKey types.NamespacedName) error <span class="cov8" title="1">{
        err := r.Client.Get(context.TODO(), objectKey, found)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info(fmt.Sprintf("creating configmap: %s", cm.Name))
                        err = r.Client.Create(context.TODO(), cm)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, "creation of configmap failed")
                                return err
                        }</span>
                } else<span class="cov0" title="0"> {
                        r.Log.Error(err, "unable the get the configmap")
                        return err
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// createExternalStorageClusterSecret creates secret for external cluster
func (r *StorageClusterReconciler) createExternalStorageClusterSecret(sec *corev1.Secret, found *corev1.Secret, objectKey types.NamespacedName) error <span class="cov8" title="1">{
        err := r.Client.Get(context.TODO(), objectKey, found)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info(fmt.Sprintf("creating secret: %s", sec.Name))
                        err = r.Client.Create(context.TODO(), sec)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, "creation of secret failed")
                                return err
                        }</span>
                } else<span class="cov0" title="0"> {
                        r.Log.Error(err, "unable the get the secret")
                        return err
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file11" style="display: none">package storagecluster

import (
        "fmt"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
)

func generateNameForCephCluster(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return generateNameForCephClusterFromString(initData.Name)
}</span>

func generateNameForCephClusterFromString(name string) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-cephcluster", name)
}</span>

func generateNameForCephFilesystem(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-cephfilesystem", initData.Name)
}</span>

func generateNameForCephObjectStoreUser(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-cephobjectstoreuser", initData.Name)
}</span>

func generateNameForCephBlockPool(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-cephblockpool", initData.Name)
}</span>

func generateNameForCephObjectStore(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-%s", initData.Name, "cephobjectstore")
}</span>

func generateNameForCephRgwSC(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-ceph-rgw", initData.Name)
}</span>

func generateNameForCephFilesystemSC(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-cephfs", initData.Name)
}</span>

func generateNameForCephBlockPoolSC(initData *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-ceph-rbd", initData.Name)
}</span>

// generateNameForSnapshotClass function generates 'SnapshotClass' name.
// 'snapshotType' can be: 'rbdSnapshotter' or 'cephfsSnapshotter'
func generateNameForSnapshotClass(initData *ocsv1.StorageCluster, snapshotType SnapshotterType) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s-%splugin-snapclass", initData.Name, snapshotType)
}</span>

func generateNameForSnapshotClassDriver(initData *ocsv1.StorageCluster, snapshotType SnapshotterType) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s.%s.csi.ceph.com", initData.Namespace, snapshotType)
}</span>

func generateNameForSnapshotClassSecret(snapshotType SnapshotterType) string <span class="cov8" title="1">{
        return fmt.Sprintf("rook-csi-%s-provisioner", snapshotType)
}</span>

// generateCephReplicatedSpec returns the ReplicatedSpec for the cephCluster
// based on the StorageCluster configuration
func generateCephReplicatedSpec(initData *ocsv1.StorageCluster, poolType string) cephv1.ReplicatedSpec <span class="cov8" title="1">{
        crs := cephv1.ReplicatedSpec{}

        crs.Size = getCephPoolReplicatedSize(initData)
        crs.ReplicasPerFailureDomain = uint(getReplicasPerFailureDomain(initData))
        if "data" == poolType </span><span class="cov8" title="1">{
                crs.TargetSizeRatio = .49
        }</span>

        <span class="cov8" title="1">return crs</span>
}
</pre>
		
		<pre class="file" id="file12" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        "time"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        corev1 "k8s.io/api/core/v1"
        "sigs.k8s.io/controller-runtime/pkg/client"

        "k8s.io/apimachinery/pkg/types"
)

const (
        // KMSConfigMapName is the name configmap which has KMS config details
        KMSConfigMapName = "ocs-kms-connection-details"
        // KMSTokenSecretName is the name of the secret which has KMS token details
        KMSTokenSecretName = "ocs-kms-token"
        // KMSProviderKey is the key in config map to get the KMS provider name
        KMSProviderKey = "KMS_PROVIDER"
        // VaultKMSProvider a constant to represent 'vault' KMS provider
        VaultKMSProvider = "vault"
)

var (
        // currently supported KMS providers mapped to their address key
        kmsProviderAddressKeyMap = map[string]string{
                VaultKMSProvider: "VAULT_ADDR",
        }
)

// kmsConfigMapValidateFunc is a functional type,
// which validates the given configmap and returns an error if any
type kmsConfigMapValidateFunc func(*corev1.ConfigMap) error

var (
        // just to make sure 'isKMSProviderReachable' function follows the validate function type
        _ kmsConfigMapValidateFunc = reachKMSProvider
)

// getKMSConfigMap function try to return a KMS ConfigMap.
// if 'kmsValidateFunc' function is present it try to validate the retrieved config map.
func getKMSConfigMap(instance *ocsv1.StorageCluster, client client.Client, kmsValidateFunc kmsConfigMapValidateFunc) (*corev1.ConfigMap, error) <span class="cov8" title="1">{
        // if 'KMS' is not enabled, nothing to fetch
        if !instance.Spec.Encryption.KeyManagementService.Enable </span><span class="cov8" title="1">{
                return nil, nil
        }</span>
        <span class="cov8" title="1">kmsConfigMap := corev1.ConfigMap{}
        err := client.Get(context.TODO(),
                types.NamespacedName{
                        Name:      KMSConfigMapName,
                        Namespace: instance.ObjectMeta.Namespace,
                },
                &amp;kmsConfigMap,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // if validation function is provided, use it
        <span class="cov8" title="1">if kmsValidateFunc != nil </span><span class="cov8" title="1">{
                err = kmsValidateFunc(&amp;kmsConfigMap)
        }</span>
        <span class="cov8" title="1">return &amp;kmsConfigMap, err</span>
}

// reachKMSProvider function checks whether the provided address is reachable or not.
// This function won't validate any other cases and only returns an error if the provided
// KMS provider address is not reachable. All other validations will be done on rook side.
func reachKMSProvider(kmsConfigMap *corev1.ConfigMap) error <span class="cov8" title="1">{
        if kmsConfigMap == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("please provide a valid config map")
        }</span>
        <span class="cov8" title="1">kmsProviderName, ok := kmsConfigMap.Data[KMSProviderKey]
        if !ok </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">var kmsProviderAddressKey string
        if kmsProviderAddressKey, ok = kmsProviderAddressKeyMap[kmsProviderName]; !ok </span><span class="cov8" title="1">{
                // cannot find an address key specific for this KMS provider
                // nothing to validate
                return nil
        }</span>
        <span class="cov8" title="1">kmsAddress, ok := kmsConfigMap.Data[kmsProviderAddressKey]
        if !ok </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov8" title="1">return checkEndpointReachable(kmsAddress, 5*time.Second)</span>
}
</pre>
		
		<pre class="file" id="file13" style="display: none">package storagecluster

import (
        "context"
        "fmt"

        nbv1 "github.com/noobaa/noobaa-operator/v2/pkg/apis/noobaa/v1alpha1"
        objectreferencesv1 "github.com/openshift/custom-resource-status/objectreferences/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        statusutil "github.com/openshift/ocs-operator/controllers/util"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/client-go/tools/reference"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

type ocsNoobaaSystem struct{}

func (obj *ocsNoobaaSystem) ensureCreated(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        // Everything other than ReconcileStrategyIgnore means we reconcile
        if sc.Spec.MultiCloudGateway != nil </span><span class="cov8" title="1">{
                reconcileStrategy := ReconcileStrategy(sc.Spec.MultiCloudGateway.ReconcileStrategy)
                if reconcileStrategy == ReconcileStrategyIgnore || reconcileStrategy == ReconcileStrategyStandalone </span><span class="cov8" title="1">{
                        return nil
                }</span>
        }

        // find cephCluster
        <span class="cov8" title="1">foundCeph := &amp;cephv1.CephCluster{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: generateNameForCephCluster(sc), Namespace: sc.Namespace}, foundCeph)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("Waiting on ceph cluster to be created before starting noobaa")
                        return nil
                }</span>
                <span class="cov0" title="0">return err</span>
        }
        <span class="cov8" title="1">if !sc.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                if foundCeph.Status.State != cephv1.ClusterStateCreated </span><span class="cov8" title="1">{
                        r.Log.Info("Waiting on ceph cluster to initialize before starting noobaa")
                        return nil
                }</span>
        } else<span class="cov8" title="1"> {
                if foundCeph.Status.State != cephv1.ClusterStateConnected </span><span class="cov8" title="1">{
                        r.Log.Info("Waiting for the external ceph cluster to be connected before starting noobaa")
                        return nil
                }</span>
        }

        // Take ownership over the noobaa object
        <span class="cov8" title="1">nb := &amp;nbv1.NooBaa{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "NooBaa",
                        APIVersion: "noobaa.io/v1alpha1'",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      "noobaa",
                        Namespace: sc.Namespace,
                },
        }
        err = controllerutil.SetControllerReference(sc, nb, r.Scheme)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Reconcile the noobaa state, creating or updating if needed
        <span class="cov8" title="1">_, err = controllerutil.CreateOrUpdate(context.TODO(), r.Client, nb, func() error </span><span class="cov8" title="1">{
                return r.setNooBaaDesiredState(nb, sc)
        }</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Failed to create or update NooBaa system")
                return err
        }</span>
        // Need to happen after the noobaa CR update was confirmed
        <span class="cov8" title="1">sc.Status.Images.NooBaaCore.ActualImage = *nb.Spec.Image
        sc.Status.Images.NooBaaDB.ActualImage = *nb.Spec.DBImage

        objectRef, err := reference.GetReference(r.Scheme, nb)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">err = objectreferencesv1.SetObjectReference(&amp;sc.Status.RelatedObjects, *objectRef)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">statusutil.MapNoobaaNegativeConditions(&amp;r.conditions, nb)
        return nil</span>
}

func (r *StorageClusterReconciler) setNooBaaDesiredState(nb *nbv1.NooBaa, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        storageClassName := generateNameForCephBlockPoolSC(sc)
        coreResources := defaults.GetDaemonResources("noobaa-core", sc.Spec.Resources)
        dbResources := defaults.GetDaemonResources("noobaa-db", sc.Spec.Resources)
        dBVolumeResources := defaults.GetDaemonResources("noobaa-db-vol", sc.Spec.Resources)
        endpointResources := defaults.GetDaemonResources("noobaa-endpoint", sc.Spec.Resources)

        nb.Labels = map[string]string{
                "app": "noobaa",
        }
        nb.Spec.DBStorageClass = &amp;storageClassName
        nb.Spec.PVPoolDefaultStorageClass = &amp;storageClassName
        nb.Spec.CoreResources = &amp;coreResources
        nb.Spec.DBResources = &amp;dbResources
        placement := getPlacement(sc, "noobaa-core")
        nb.Spec.Tolerations = placement.Tolerations
        nb.Spec.Affinity = &amp;corev1.Affinity{NodeAffinity: placement.NodeAffinity}
        nb.Spec.DBVolumeResources = &amp;dBVolumeResources
        nb.Spec.Image = &amp;r.images.NooBaaCore
        nb.Spec.DBImage = &amp;r.images.NooBaaDB

        // Default endpoint spec.
        nb.Spec.Endpoints = &amp;nbv1.EndpointsSpec{
                MinCount:               1,
                MaxCount:               2,
                AdditionalVirtualHosts: []string{},

                // TODO: After spec.resources["noobaa-endpoint"] is decleared obesolete this
                // definition should hold a constant value. and should not be read from
                // GetDaemonResources()
                Resources: &amp;endpointResources,
        }

        // Override with MCG options specified in the storage cluster spec
        if sc.Spec.MultiCloudGateway != nil </span><span class="cov8" title="1">{
                if sc.Spec.MultiCloudGateway.Endpoints != nil </span><span class="cov0" title="0">{
                        epSpec := sc.Spec.MultiCloudGateway.Endpoints

                        nb.Spec.Endpoints.MinCount = epSpec.MinCount
                        nb.Spec.Endpoints.MaxCount = epSpec.MaxCount
                        if epSpec.AdditionalVirtualHosts != nil </span><span class="cov0" title="0">{
                                nb.Spec.Endpoints.AdditionalVirtualHosts = epSpec.AdditionalVirtualHosts
                        }</span>
                        <span class="cov0" title="0">if epSpec.Resources != nil </span><span class="cov0" title="0">{
                                nb.Spec.Endpoints.Resources = epSpec.Resources
                        }</span>
                }
        }

        // Add KMS details to Noobaa spec, if available
        // no need to pass any validation function,
        // as validation already done during cephcluster creation time.
        <span class="cov8" title="1">if kmsConfig, err := getKMSConfigMap(sc, r.Client, nil); err != nil </span><span class="cov0" title="0">{
                return err
        }</span> else<span class="cov8" title="1"> if kmsConfig != nil </span><span class="cov8" title="1">{
                nb.Spec.Security.KeyManagementService.ConnectionDetails = kmsConfig.Data
                nb.Spec.Security.KeyManagementService.TokenSecretName = KMSTokenSecretName
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted Delete noobaa system in the namespace
func (obj *ocsNoobaaSystem) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        // Delete only if this is being managed by the OCS operator
        if sc.Spec.MultiCloudGateway != nil </span><span class="cov0" title="0">{
                reconcileStrategy := ReconcileStrategy(sc.Spec.MultiCloudGateway.ReconcileStrategy)
                if reconcileStrategy == ReconcileStrategyIgnore || reconcileStrategy == ReconcileStrategyStandalone </span><span class="cov0" title="0">{
                        return nil
                }</span>
        }
        <span class="cov8" title="1">noobaa := &amp;nbv1.NooBaa{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: "noobaa", Namespace: sc.Namespace}, noobaa)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        pvcs := &amp;corev1.PersistentVolumeClaimList{}
                        opts := []client.ListOption{
                                client.InNamespace(sc.Namespace),
                                client.MatchingLabels(map[string]string{"noobaa-core": "noobaa"}),
                        }
                        err = r.Client.List(context.TODO(), pvcs, opts...)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov8" title="1">if len(pvcs.Items) &gt; 0 </span><span class="cov0" title="0">{
                                return fmt.Errorf("Uninstall: Waiting on NooBaa PVCs to be deleted")
                        }</span>
                        <span class="cov8" title="1">r.Log.Info("Uninstall: NooBaa and noobaa-db PVC not found.")
                        return nil</span>
                }
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Failed to retrieve NooBaa system: %v", err)</span>
        }

        <span class="cov0" title="0">isOwned := false
        for _, ref := range noobaa.GetOwnerReferences() </span><span class="cov0" title="0">{
                if ref.Name == sc.Name &amp;&amp; ref.Kind == sc.Kind </span><span class="cov0" title="0">{
                        isOwned = true
                        break</span>
                }
        }
        <span class="cov0" title="0">if !isOwned </span><span class="cov0" title="0">{
                // if the noobaa found is not owned by our storagecluster, we skip it from deletion.
                r.Log.Info("Uninstall: NooBaa object %v found, but ownerReference not set to storagecluster. Skipping", noobaa.ObjectMeta.Name)
                return nil
        }</span>

        <span class="cov0" title="0">if noobaa.GetDeletionTimestamp().IsZero() </span><span class="cov0" title="0">{
                r.Log.Info("Uninstall: Deleting NooBaa system %v", noobaa.ObjectMeta.Name)
                err = r.Client.Delete(context.TODO(), noobaa)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Uninstall: Failed to delete NooBaa system %v", noobaa.ObjectMeta.Name)
                        return fmt.Errorf("Uninstall: Failed to delete NooBaa system %v : %v", noobaa.ObjectMeta.Name, err)
                }</span>
        }
        <span class="cov0" title="0">return fmt.Errorf("Uninstall: Waiting on NooBaa system %v to be deleted", noobaa.ObjectMeta.Name)</span>
}
</pre>
		
		<pre class="file" id="file14" style="display: none">package storagecluster

import (
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        rookv1 "github.com/rook/rook/pkg/apis/rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/labels"
        "sigs.k8s.io/controller-runtime/pkg/client"
)

// getPlacement returns placement configuration for ceph components with appropriate topology
func getPlacement(sc *ocsv1.StorageCluster, component string) rookv1.Placement <span class="cov8" title="1">{
        placement := rookv1.Placement{}
        in, ok := sc.Spec.Placement[rookv1.KeyType(component)]
        if ok </span><span class="cov8" title="1">{
                (&amp;in).DeepCopyInto(&amp;placement)
        }</span> else<span class="cov8" title="1"> {
                in := defaults.DaemonPlacements[component]
                (&amp;in).DeepCopyInto(&amp;placement)
        }</span>

        <span class="cov8" title="1">if component == "arbiter" </span><span class="cov8" title="1">{
                if !sc.Spec.Arbiter.DisableMasterNodeToleration </span><span class="cov8" title="1">{
                        placement.Tolerations = append(placement.Tolerations, corev1.Toleration{
                                Key:      "node-role.kubernetes.io/master",
                                Operator: corev1.TolerationOpExists,
                                Effect:   corev1.TaintEffectNoSchedule,
                        })
                }</span>
                <span class="cov8" title="1">return placement</span>
        }

        // If no placement is specified for the given component and the
        // StorageCluster has no label selector, set the default node
        // affinity.
        <span class="cov8" title="1">if placement.NodeAffinity == nil &amp;&amp; sc.Spec.LabelSelector == nil </span><span class="cov8" title="1">{
                placement.NodeAffinity = defaults.DefaultNodeAffinity
        }</span>

        // If the StorageCluster specifies a label selector, append it to the
        // node affinity, creating it if it doesn't exist.
        <span class="cov8" title="1">if sc.Spec.LabelSelector != nil </span><span class="cov8" title="1">{
                reqs := convertLabelToNodeSelectorRequirements(*sc.Spec.LabelSelector)
                if len(reqs) != 0 </span><span class="cov8" title="1">{
                        appendNodeRequirements(&amp;placement, reqs...)
                }</span>
        }

        <span class="cov8" title="1">topologyMap := sc.Status.NodeTopologies
        if topologyMap == nil </span><span class="cov8" title="1">{
                return placement
        }</span>

        <span class="cov8" title="1">topologyKey := determineFailureDomain(sc)
        topologyKey, _ = topologyMap.GetKeyValues(topologyKey)
        if component == "mon" || component == "mds" || (component == "rgw" &amp;&amp; getCephObjectStoreGatewayInstances(sc) &gt; 1) </span><span class="cov8" title="1">{
                if placement.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil </span><span class="cov8" title="1">{
                        for i := range placement.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution </span><span class="cov8" title="1">{
                                placement.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i].PodAffinityTerm.TopologyKey = topologyKey
                        }</span>
                }
                <span class="cov8" title="1">if placement.PodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution != nil </span><span class="cov8" title="1">{
                        for i := range placement.PodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution </span><span class="cov8" title="1">{
                                placement.PodAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution[i].TopologyKey = topologyKey
                        }</span>
                }
        }

        <span class="cov8" title="1">return placement</span>
}

//convertLabelToNodeSelectorRequirements returns a NodeSelectorRequirement list from a given LabelSelector
func convertLabelToNodeSelectorRequirements(labelSelector metav1.LabelSelector) []corev1.NodeSelectorRequirement <span class="cov8" title="1">{
        reqs := []corev1.NodeSelectorRequirement{}
        for key, value := range labelSelector.MatchLabels </span><span class="cov0" title="0">{
                req := corev1.NodeSelectorRequirement{}
                req.Key = key
                req.Operator = corev1.NodeSelectorOpIn
                req.Values = append(req.Values, value)
                reqs = append(reqs, req)
        }</span>
        <span class="cov8" title="1">numIter := len(labelSelector.MatchExpressions)
        for i := 0; i &lt; numIter; i++ </span><span class="cov8" title="1">{
                req := corev1.NodeSelectorRequirement{}
                req.Key = labelSelector.MatchExpressions[i].Key
                req.Operator = corev1.NodeSelectorOperator(labelSelector.MatchExpressions[i].Operator)
                req.Values = labelSelector.MatchExpressions[i].Values
                reqs = append(reqs, req)
        }</span>
        <span class="cov8" title="1">return reqs</span>
}

func appendNodeRequirements(placement *rookv1.Placement, reqs ...corev1.NodeSelectorRequirement) <span class="cov8" title="1">{
        if placement.NodeAffinity == nil </span><span class="cov8" title="1">{
                placement.NodeAffinity = &amp;corev1.NodeAffinity{}
        }</span>
        <span class="cov8" title="1">if placement.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil </span><span class="cov8" title="1">{
                placement.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution = &amp;corev1.NodeSelector{}
        }</span>
        <span class="cov8" title="1">nodeSelector := placement.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution
        if len(nodeSelector.NodeSelectorTerms) == 0 </span><span class="cov8" title="1">{
                nodeSelector.NodeSelectorTerms = append(nodeSelector.NodeSelectorTerms, corev1.NodeSelectorTerm{})
        }</span>
        <span class="cov8" title="1">nodeSelector.NodeSelectorTerms[0].MatchExpressions = append(nodeSelector.NodeSelectorTerms[0].MatchExpressions, reqs...)</span>
}

// MatchingLabelsSelector filters the list/delete operation on the given label
// selector (or index in the case of cached lists). A struct is used because
// labels.Selector is an interface, which cannot be aliased.
type MatchingLabelsSelector struct {
        labels.Selector
}

// ApplyToList applies this configuration to the given list options.
//This is implemented by MatchingLabelsSelector which implements ListOption interface.
func (m MatchingLabelsSelector) ApplyToList(opts *client.ListOptions) <span class="cov8" title="1">{
        opts.LabelSelector = m
}</span>

// setTopologyForAffinity assigns topology related values to the affinity placements
func setTopologyForAffinity(placement *rookv1.Placement, selectorValue string, topologyKey string) <span class="cov8" title="1">{
        placement.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution[0].PodAffinityTerm.TopologyKey = topologyKey

        nodeZoneSelector := corev1.NodeSelectorRequirement{
                Key:      topologyKey,
                Operator: corev1.NodeSelectorOpIn,
                Values:   []string{selectorValue},
        }
        appendNodeRequirements(placement, nodeZoneSelector)
}</span>
</pre>
		
		<pre class="file" id="file15" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        configv1 "github.com/openshift/api/config/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "strings"
        "sync"
)

//IBMCloud secret name
const ibmCloudCosSecretName = "ibm-cloud-cos-creds"

// AvoidObjectStorePlatforms is a list of all PlatformTypes where CephObjectStores will not be deployed.
var AvoidObjectStorePlatforms = []configv1.PlatformType{
        configv1.AWSPlatformType,
        configv1.GCPPlatformType,
        configv1.AzurePlatformType,
}

// TuneFastPlatforms is a list of all PlatformTypes where TuneFastDeviceClass has to be set True.
var TuneFastPlatforms = []configv1.PlatformType{
        configv1.OvirtPlatformType,
        configv1.IBMCloudPlatformType,
        configv1.AzurePlatformType,
}

// Platform is used to get the CloudPlatformType of the running cluster in a thread-safe manner
type Platform struct {
        platform configv1.PlatformType
        mux      sync.Mutex
}

// GetPlatform is used to get the CloudPlatformType of the running cluster
func (p *Platform) GetPlatform(c client.Client) (configv1.PlatformType, error) <span class="cov8" title="1">{
        // if 'platform' is already set just return it
        if p.platform != "" </span><span class="cov8" title="1">{
                return p.platform, nil
        }</span>
        <span class="cov0" title="0">p.mux.Lock()
        defer p.mux.Unlock()

        return p.getPlatform(c)</span>
}

func (p *Platform) getPlatform(c client.Client) (configv1.PlatformType, error) <span class="cov0" title="0">{
        infrastructure := &amp;configv1.Infrastructure{ObjectMeta: metav1.ObjectMeta{Name: "cluster"}}
        err := c.Get(context.TODO(), types.NamespacedName{Name: infrastructure.ObjectMeta.Name}, infrastructure)
        if err != nil </span><span class="cov0" title="0">{
                return "", fmt.Errorf("could not get infrastructure details to determine cloud platform: %v", err)
        }</span>

        <span class="cov0" title="0">p.platform = infrastructure.Status.Platform //nolint:staticcheck
        return p.platform, nil</span>
}

func avoidObjectStore(p configv1.PlatformType) bool <span class="cov8" title="1">{
        for _, platform := range AvoidObjectStorePlatforms </span><span class="cov8" title="1">{
                if p == platform </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

func isIBMPlatformWithCosSecret(p configv1.PlatformType, namespace string, c client.Client) (bool, error) <span class="cov8" title="1">{
        if p == configv1.IBMCloudPlatformType </span><span class="cov8" title="1">{
                isIBM, err := IsIBMPlatform(p, c)
                if err != nil </span><span class="cov0" title="0">{
                        return false, err
                }</span>
                <span class="cov8" title="1">isSecretPresent, err := IsCosSecretPresent(namespace, c)
                if err != nil </span><span class="cov0" title="0">{
                        return false, err
                }</span>
                <span class="cov8" title="1">if isIBM &amp;&amp; isSecretPresent </span><span class="cov0" title="0">{
                        return true, nil
                }</span>
        }
        <span class="cov8" title="1">return false, nil</span>
}

// IsIBMPlatform returns true if this cluster is running on IBM Cloud
// IBM Satellite clusters are not considered as IBMCloudPlatform
func IsIBMPlatform(p configv1.PlatformType, c client.Client) (bool, error) <span class="cov8" title="1">{
        isIBM := true
        nodeList := &amp;corev1.NodeList{}
        err := c.List(context.TODO(), nodeList)
        if err != nil </span><span class="cov0" title="0">{
                return isIBM, fmt.Errorf("failed to list kubernetes nodes: %s", err)
        }</span>
        // Incase of Satellite, cluster is deployed in user provided infrastructure
        <span class="cov8" title="1">if strings.Contains(nodeList.Items[0].Spec.ProviderID, "/sat-") </span><span class="cov0" title="0">{
                isIBM = false
        }</span>

        <span class="cov8" title="1">return isIBM, nil</span>
}

// Check for ibm-cos-cred secret in the concerned namespace
// if platform is IBMCloud, enable CephObjectStore only if ibm-cloud-cos-creds secret is not present
// in the target namespace
func IsCosSecretPresent(namespace string, c client.Client) (bool, error) <span class="cov8" title="1">{
        foundSecret := &amp;corev1.Secret{}
        err := c.Get(context.TODO(), types.NamespacedName{Name: ibmCloudCosSecretName, Namespace: namespace}, foundSecret)
        if err != nil &amp;&amp; errors.IsNotFound(err) </span><span class="cov8" title="1">{
                return false, nil
        }</span> else<span class="cov0" title="0"> if err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>
        // Secret is present.
        <span class="cov0" title="0">return true, nil</span>
}

func (r *StorageClusterReconciler) DevicesDefaultToFastForThisPlatform() (bool, error) <span class="cov8" title="1">{
        c := r.Client
        platform, err := r.platform.GetPlatform(c)
        if err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>

        <span class="cov8" title="1">for _, tfplatform := range TuneFastPlatforms </span><span class="cov8" title="1">{
                if platform == tfplatform </span><span class="cov8" title="1">{
                        return true, nil
                }</span>
        }

        <span class="cov8" title="1">return false, nil</span>
}
</pre>
		
		<pre class="file" id="file16" style="display: none">package storagecluster

import (
        "bytes"
        "context"
        "fmt"
        "io/ioutil"
        "os"
        "path/filepath"

        monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        k8sYAML "k8s.io/apimachinery/pkg/util/yaml"
)

const (
        internalPrometheusRuleFilepath = "/ocs-prometheus-rules/prometheus-ocs-rules.yaml"
        externalPrometheusRuleFilepath = "/ocs-prometheus-rules/prometheus-ocs-rules-external.yaml"
        ruleName                       = "ocs-prometheus-rules"
        ruleNamespace                  = "openshift-storage"
)

// enablePrometheusRules is a wrapper around CreateOrUpdatePrometheusRule()
func (r *StorageClusterReconciler) enablePrometheusRules(isExternal bool) error <span class="cov8" title="1">{
        rule, err := getPrometheusRules(isExternal)
        if err != nil </span><span class="cov8" title="1">{
                r.Log.Error(err, "prometheus rules file not found")
        }</span>
        <span class="cov8" title="1">err = r.CreateOrUpdatePrometheusRules(rule)
        if err != nil </span><span class="cov8" title="1">{
                r.Log.Error(err, "unable to deploy Prometheus rules")
        }</span>
        <span class="cov8" title="1">return nil</span>
}

func getPrometheusRules(isExternal bool) (*monitoringv1.PrometheusRule, error) <span class="cov8" title="1">{
        rule := &amp;monitoringv1.PrometheusRule{
                TypeMeta: metav1.TypeMeta{
                        Kind:       monitoringv1.PrometheusRuleKind,
                        APIVersion: monitoringv1.SchemeGroupVersion.String(),
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:      ruleName,
                        Namespace: ruleNamespace,
                },
        }
        var err error
        var ruleSpec *monitoringv1.PrometheusRuleSpec
        if isExternal </span><span class="cov8" title="1">{
                ruleSpec, err = getPrometheusRuleSpecFrom(externalPrometheusRuleFilepath)
                if err != nil </span><span class="cov8" title="1">{
                        return nil, err
                }</span>
        } else<span class="cov8" title="1"> {
                ruleSpec, err = getPrometheusRuleSpecFrom(internalPrometheusRuleFilepath)
                if err != nil </span><span class="cov8" title="1">{
                        return nil, err
                }</span>
        }
        <span class="cov0" title="0">rule.Spec = *ruleSpec
        return rule, nil</span>
}

func getPrometheusRuleSpecFrom(filePath string) (*monitoringv1.PrometheusRuleSpec, error) <span class="cov8" title="1">{
        if err := CheckFileExists(filePath); err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>
        <span class="cov0" title="0">fileContent, err := ioutil.ReadFile(filepath.Clean(filePath))
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("'%s' not readable", filePath)
        }</span>
        <span class="cov0" title="0">ruleSpec := monitoringv1.PrometheusRuleSpec{}
        if err := k8sYAML.NewYAMLOrJSONDecoder(bytes.NewBufferString(string(fileContent)), 1000).Decode(&amp;ruleSpec); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">return &amp;ruleSpec, nil</span>
}

// CheckFileExists checks for existence of file in given filepath
func CheckFileExists(filePath string) error <span class="cov8" title="1">{
        _, err := os.Stat(filePath)
        if err != nil </span><span class="cov8" title="1">{
                if os.IsNotExist(err) </span><span class="cov8" title="1">{
                        return fmt.Errorf("'%s' not found", filePath)
                }</span>
                <span class="cov0" title="0">return err</span>
        }
        <span class="cov0" title="0">return nil</span>
}

// CreateOrUpdatePrometheusRules creates or updates Prometheus Rule
func (r *StorageClusterReconciler) CreateOrUpdatePrometheusRules(rule *monitoringv1.PrometheusRule) error <span class="cov8" title="1">{
        err := r.Client.Create(context.TODO(), rule)
        if err != nil </span><span class="cov8" title="1">{
                if apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                        oldRule := &amp;monitoringv1.PrometheusRule{}
                        err = r.Client.Get(context.TODO(), types.NamespacedName{Name: rule.Name, Namespace: rule.Namespace}, oldRule)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed while fetching PrometheusRule: %v", err)
                        }</span>
                        <span class="cov0" title="0">oldRule.Spec = rule.Spec
                        err := r.Client.Update(context.TODO(), oldRule)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed while updating PrometheusRule: %v", err)
                        }</span>
                } else<span class="cov8" title="1"> {
                        return fmt.Errorf("failed while creating PrometheusRule: %v", err)
                }</span>
        }
        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file17" style="display: none">package storagecluster

import (
        "context"

        "github.com/ghodss/yaml"
        consolev1 "github.com/openshift/api/console/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        extv1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/types"
)

type ocsQuickStarts struct{}

func (obj *ocsQuickStarts) ensureCreated(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        qscrd := extv1.CustomResourceDefinition{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: "consolequickstarts.console.openshift.io", Namespace: ""}, &amp;qscrd)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.V(2).Info("No custom resource definition found for consolequickstart. Skipping quickstart initialization")
                        return nil
                }</span>
                <span class="cov0" title="0">return err</span>
        }
        <span class="cov8" title="1">if len(AllQuickStarts) == 0 </span><span class="cov0" title="0">{
                r.Log.Info("No quickstarts found")
                return nil
        }</span>
        <span class="cov8" title="1">for _, qs := range AllQuickStarts </span><span class="cov8" title="1">{
                cqs := consolev1.ConsoleQuickStart{}
                err := yaml.Unmarshal(qs, &amp;cqs)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to unmarshal ConsoleQuickStart", "ConsoleQuickStartString", string(qs))
                        continue</span>
                }
                <span class="cov8" title="1">found := consolev1.ConsoleQuickStart{}
                err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cqs.Name, Namespace: cqs.Namespace}, &amp;found)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                err = r.Client.Create(context.TODO(), &amp;cqs)
                                if err != nil </span><span class="cov0" title="0">{
                                        r.Log.Error(err, "Failed to create quickstart", "Name", cqs.Name, "Namespace", cqs.Namespace)
                                        return nil
                                }</span>
                                <span class="cov8" title="1">r.Log.Info("Creating quickstarts", "Name", cqs.Name, "Namespace", cqs.Namespace)
                                continue</span>
                        }
                        <span class="cov0" title="0">r.Log.Error(err, "Error has occurred when fetching quickstarts")
                        return nil</span>
                }
                <span class="cov0" title="0">found.Spec = cqs.Spec
                err = r.Client.Update(context.TODO(), &amp;found)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to update quickstart", "Name", cqs.Name, "Namespace", cqs.Namespace)
                        return nil
                }</span>
                <span class="cov0" title="0">r.Log.Info("Updating quickstarts", "Name", cqs.Name, "Namespace", cqs.Namespace)</span>
        }
        <span class="cov8" title="1">return nil</span>
}

func (obj *ocsQuickStarts) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        if len(AllQuickStarts) == 0 </span><span class="cov0" title="0">{
                r.Log.Info("No quickstarts found")
                return nil
        }</span>
        <span class="cov8" title="1">for _, qs := range AllQuickStarts </span><span class="cov8" title="1">{
                cqs := consolev1.ConsoleQuickStart{}
                err := yaml.Unmarshal(qs, &amp;cqs)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to unmarshal ConsoleQuickStart", "ConsoleQuickStartString", string(qs))
                        continue</span>
                }
                <span class="cov8" title="1">found := consolev1.ConsoleQuickStart{}
                err = r.Client.Get(context.TODO(), types.NamespacedName{Name: cqs.Name, Namespace: cqs.Namespace}, &amp;found)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                continue</span>
                        }
                        <span class="cov0" title="0">r.Log.Error(err, "Uninstall: Failed to get QuickStart %s", "Name", cqs.Name, "Namespace", cqs.Namespace)
                        return nil</span>
                }
                <span class="cov0" title="0">err = r.Client.Delete(context.TODO(), &amp;found)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Uninstall: Failed to delete QuickStart %s", "Name", cqs.Name, "Namespace", cqs.Namespace)
                        return nil
                }</span>
                <span class="cov0" title="0">r.Log.Info("Uninstall: Deleting QuickStart", "Name", cqs.Name, "Namespace", cqs.Namespace)</span>
        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file18" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        "os"
        "strings"
        "time"

        "github.com/blang/semver"
        "github.com/go-logr/logr"
        openshiftv1 "github.com/openshift/api/template/v1"
        conditionsv1 "github.com/openshift/custom-resource-status/conditions/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        statusutil "github.com/openshift/ocs-operator/controllers/util"
        "github.com/openshift/ocs-operator/version"
        batchv1 "k8s.io/api/batch/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/types"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
        "sigs.k8s.io/controller-runtime/pkg/reconcile"
)

// ReconcileStrategy is a string representing how we want to reconcile
// (or not) a particular resource
type ReconcileStrategy string

// StorageClassProvisionerType is a string representing StorageClass Provisioner. E.g: aws-ebs
type StorageClassProvisionerType string

type resourceManager interface {
        ensureCreated(*StorageClusterReconciler, *ocsv1.StorageCluster) error
        ensureDeleted(*StorageClusterReconciler, *ocsv1.StorageCluster) error
}

type ocsCephConfig struct{}
type ocsJobTemplates struct{}

const (
        rookConfigMapName = "rook-config-override"
        rookConfigData    = `
[global]
mon_osd_full_ratio = .85
mon_osd_backfillfull_ratio = .8
mon_osd_nearfull_ratio = .75
mon_max_pg_per_osd = 600
[osd]
osd_memory_target_cgroup_limit_ratio = 0.5
`
        monCountOverrideEnvVar = "MON_COUNT_OVERRIDE"

        // Name of MetadataPVCTemplate
        metadataPVCName = "metadata"
        // Name of WalPVCTemplate
        walPVCName = "wal"

        // ReconcileStrategyUnknown is the same as default
        ReconcileStrategyUnknown ReconcileStrategy = ""
        // ReconcileStrategyInit means reconcile once and ignore if it exists
        ReconcileStrategyInit ReconcileStrategy = "init"
        // ReconcileStrategyIgnore means never reconcile
        ReconcileStrategyIgnore ReconcileStrategy = "ignore"
        // ReconcileStrategyManage means always reconcile
        ReconcileStrategyManage ReconcileStrategy = "manage"
        // ReconcileStrategyStandalone also means never reconcile (NooBaa)
        ReconcileStrategyStandalone ReconcileStrategy = "standalone"

        // DeviceTypeSSD represents the DeviceType SSD
        DeviceTypeSSD = "ssd"

        // DeviceTypeHDD represents the DeviceType HDD
        DeviceTypeHDD = "hdd"

        // DeviceTypeNVMe represents the DeviceType NVMe
        DeviceTypeNVMe = "nvme"

        // AzureDisk represents Azure Premium Managed Disks provisioner for StorageClass
        AzureDisk StorageClassProvisionerType = "kubernetes.io/azure-disk"

        // EBS represents AWS EBS provisioner for StorageClass
        EBS StorageClassProvisionerType = "kubernetes.io/aws-ebs"
)

var storageClusterFinalizer = "storagecluster.ocs.openshift.io"

var validTopologyLabelKeys = []string{
        "failure-domain.beta.kubernetes.io",
        "failure-domain.kubernetes.io",
        "kubernetes.io/hostname",
        "topology.rook.io",
}

// +kubebuilder:rbac:groups=ocs.openshift.io,resources=*,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=ceph.rook.io,resources=cephclusters;cephblockpools;cephfilesystems;cephobjectstores;cephobjectstoreusers,verbs=*
// +kubebuilder:rbac:groups=noobaa.io,resources=noobaas,verbs=*
// +kubebuilder:rbac:groups=storage.k8s.io,resources=storageclasses,verbs=*
// +kubebuilder:rbac:groups=core,resources=pods;services;endpoints;persistentvolumeclaims;events;configmaps;secrets;nodes,verbs=*
// +kubebuilder:rbac:groups=core,resources=namespaces,verbs=get
// +kubebuilder:rbac:groups=apps,resources=deployments;daemonsets;replicasets;statefulsets,verbs=*
// +kubebuilder:rbac:groups=monitoring.coreos.com,resources=servicemonitors;prometheusrules,verbs=get;list;watch;create;update
// +kubebuilder:rbac:groups=snapshot.storage.k8s.io,resources=volumesnapshots;volumesnapshotclasses,verbs=*
// +kubebuilder:rbac:groups=template.openshift.io,resources=templates,verbs=*
// +kubebuilder:rbac:groups=config.openshift.io,resources=infrastructures,verbs=get;list;watch
// +kubebuilder:rbac:groups=console.openshift.io,resources=consolequickstarts,verbs=*
// +kubebuilder:rbac:groups=apiextensions.k8s.io,resources=customresourcedefinitions,verbs=get;list;watch;create;update

// Reconcile reads that state of the cluster for a StorageCluster object and makes changes based on the state read
// and what is in the StorageCluster.Spec
// Note:
// The Controller will requeue the Request to be processed again if the returned error is non-nil or
// Result.Requeue is true, otherwise upon completion it will remove the work from the queue.
func (r *StorageClusterReconciler) Reconcile(request reconcile.Request) (reconcile.Result, error) <span class="cov8" title="1">{

        prevLogger := r.Log
        defer func() </span><span class="cov8" title="1">{ r.Log = prevLogger }</span>()
        <span class="cov8" title="1">r.Log = r.Log.WithValues("Request.Namespace", request.Namespace, "Request.Name", request.Name)

        // Fetch the StorageCluster instance
        sc := &amp;ocsv1.StorageCluster{}
        if err := r.Client.Get(context.TODO(), request.NamespacedName, sc); err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("No StorageCluster resource")
                        // Request object not found, could have been deleted after reconcile request.
                        // Owned objects are automatically garbage collected. For additional cleanup logic use finalizers.
                        // Return and don't requeue
                        return reconcile.Result{}, nil
                }</span>
                // Error reading the object - requeue the request.
                <span class="cov0" title="0">return reconcile.Result{}, err</span>
        }

        <span class="cov8" title="1">if err := r.validateStorageClusterSpec(sc, request); err != nil </span><span class="cov0" title="0">{
                return reconcile.Result{}, err
        }</span>

        // Reconcile changes to the cluster
        <span class="cov8" title="1">result, reconcileError := r.reconcilePhases(sc, request)

        // Apply status changes to the storagecluster
        statusError := r.Client.Status().Update(context.TODO(), sc)
        if statusError != nil </span><span class="cov0" title="0">{
                r.Log.Info("Status Update Error", "StatusUpdateErr", "Could not update storagecluster status")
        }</span>

        // Reconcile errors have higher priority than status update errors
        <span class="cov8" title="1">if reconcileError != nil </span><span class="cov8" title="1">{
                return result, reconcileError
        }</span> else<span class="cov8" title="1"> if statusError != nil </span><span class="cov0" title="0">{
                return result, statusError
        }</span> else<span class="cov8" title="1"> {
                return result, nil
        }</span>
}

func (r *StorageClusterReconciler) initializeImagesStatus(sc *ocsv1.StorageCluster) <span class="cov8" title="1">{
        images := &amp;sc.Status.Images
        if images.Ceph == nil </span><span class="cov8" title="1">{
                images.Ceph = &amp;ocsv1.ComponentImageStatus{}
        }</span>
        <span class="cov8" title="1">images.Ceph.DesiredImage = r.images.Ceph

        if images.NooBaaCore == nil </span><span class="cov8" title="1">{
                images.NooBaaCore = &amp;ocsv1.ComponentImageStatus{}
        }</span>
        <span class="cov8" title="1">images.NooBaaCore.DesiredImage = r.images.Ceph

        if images.NooBaaDB == nil </span><span class="cov8" title="1">{
                images.NooBaaDB = &amp;ocsv1.ComponentImageStatus{}
        }</span>
        <span class="cov8" title="1">images.NooBaaDB.DesiredImage = r.images.NooBaaDB</span>
}

// validateStorageClusterSpec must be called before reconciling. Any syntactic and sematic errors in the CR must be caught here.
func (r *StorageClusterReconciler) validateStorageClusterSpec(instance *ocsv1.StorageCluster, request reconcile.Request) error <span class="cov8" title="1">{
        if err := versionCheck(instance, r.Log); err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Failed to validate version")
                r.recorder.Event(instance, statusutil.EventTypeWarning, statusutil.EventReasonValidationFailed, err.Error())
                return err
        }</span>

        <span class="cov8" title="1">if !instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                if err := r.validateStorageDeviceSets(instance); err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to validate StorageDeviceSets")
                        r.recorder.Event(instance, statusutil.EventTypeWarning, statusutil.EventReasonValidationFailed, err.Error())
                        return err
                }</span>
        }

        <span class="cov8" title="1">if err := validateArbiterSpec(instance, r.Log); err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "Failed to validate ArbiterSpec")
                r.recorder.Event(instance, statusutil.EventTypeWarning, statusutil.EventReasonValidationFailed, err.Error())
                return err
        }</span>
        <span class="cov8" title="1">return nil</span>
}

func (r *StorageClusterReconciler) reconcilePhases(
        instance *ocsv1.StorageCluster,
        request reconcile.Request) (reconcile.Result, error) <span class="cov8" title="1">{

        if instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                r.Log.Info("Reconciling external StorageCluster")
        }</span> else<span class="cov8" title="1"> {
                r.Log.Info("Reconciling StorageCluster")
        }</span>

        // Initialize the StatusImages section of the storageclsuter CR
        <span class="cov8" title="1">r.initializeImagesStatus(instance)

        // Check for active StorageCluster only if Create request is made
        // and ignore it if there's another active StorageCluster
        // If Update request is made and StorageCluster is PhaseIgnored, no need to
        // proceed further
        if instance.Status.Phase == "" </span><span class="cov8" title="1">{
                isActive, err := r.isActiveStorageCluster(instance)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "StorageCluster could not be reconciled. Retrying")
                        return reconcile.Result{}, err
                }</span>
                <span class="cov8" title="1">if !isActive </span><span class="cov0" title="0">{
                        instance.Status.Phase = statusutil.PhaseIgnored
                        return reconcile.Result{}, nil
                }</span>
        } else<span class="cov8" title="1"> if instance.Status.Phase == statusutil.PhaseIgnored </span><span class="cov0" title="0">{
                return reconcile.Result{}, nil
        }</span>

        <span class="cov8" title="1">if instance.Status.Phase != statusutil.PhaseReady &amp;&amp;
                instance.Status.Phase != statusutil.PhaseClusterExpanding &amp;&amp;
                instance.Status.Phase != statusutil.PhaseDeleting &amp;&amp;
                instance.Status.Phase != statusutil.PhaseConnecting </span><span class="cov8" title="1">{
                instance.Status.Phase = statusutil.PhaseProgressing
        }</span>

        // Add conditions if there are none
        <span class="cov8" title="1">if instance.Status.Conditions == nil </span><span class="cov8" title="1">{
                reason := ocsv1.ReconcileInit
                message := "Initializing StorageCluster"
                statusutil.SetProgressingCondition(&amp;instance.Status.Conditions, reason, message)
        }</span>

        // Check GetDeletionTimestamp to determine if the object is under deletion
        <span class="cov8" title="1">if instance.GetDeletionTimestamp().IsZero() </span><span class="cov8" title="1">{
                if !contains(instance.GetFinalizers(), storageClusterFinalizer) </span><span class="cov8" title="1">{
                        r.Log.Info("Finalizer not found for storagecluster. Adding finalizer")
                        instance.ObjectMeta.Finalizers = append(instance.ObjectMeta.Finalizers, storageClusterFinalizer)
                        if err := r.Client.Update(context.TODO(), instance); err != nil </span><span class="cov0" title="0">{
                                r.Log.Info("Update Error", "MetaUpdateErr", "Failed to update storagecluster with finalizer")
                                return reconcile.Result{}, err
                        }</span>
                }

                <span class="cov8" title="1">if err := r.reconcileUninstallAnnotations(instance); err != nil </span><span class="cov0" title="0">{
                        return reconcile.Result{}, err
                }</span>

        } else<span class="cov8" title="1"> {
                // The object is marked for deletion
                instance.Status.Phase = statusutil.PhaseDeleting

                if contains(instance.GetFinalizers(), storageClusterFinalizer) </span><span class="cov8" title="1">{
                        if err := r.deleteResources(instance); err != nil </span><span class="cov0" title="0">{
                                r.Log.Info("Uninstall in progress", "Status", err)
                                return reconcile.Result{RequeueAfter: time.Second * time.Duration(1)}, nil
                        }</span>
                        <span class="cov8" title="1">r.Log.Info("Removing finalizer")
                        // Once all finalizers have been removed, the object will be deleted
                        instance.ObjectMeta.Finalizers = remove(instance.ObjectMeta.Finalizers, storageClusterFinalizer)
                        if err := r.Client.Update(context.TODO(), instance); err != nil </span><span class="cov0" title="0">{
                                r.Log.Info("Update Error", "MetaUpdateErr", "Failed to remove finalizer from storagecluster")
                                return reconcile.Result{}, err
                        }</span>
                }
                <span class="cov8" title="1">r.Log.Info("Object is terminated, skipping reconciliation")
                return reconcile.Result{}, nil</span>
        }

        <span class="cov8" title="1">if !instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                // Get storage node topology labels
                if err := r.reconcileNodeTopologyMap(instance); err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to set node topology map")
                        return reconcile.Result{}, err
                }</span>

                <span class="cov8" title="1">if instance.Status.FailureDomain == "" </span><span class="cov8" title="1">{
                        instance.Status.FailureDomain = determineFailureDomain(instance)
                }</span>
        }

        // in-memory conditions should start off empty. It will only ever hold
        // negative conditions (!Available, Degraded, Progressing)
        <span class="cov8" title="1">r.conditions = nil
        // Start with empty r.phase
        r.phase = ""
        var objs []resourceManager
        if !instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                // list of default ensure functions
                objs = []resourceManager{
                        &amp;ocsStorageClass{},
                        &amp;ocsSnapshotClass{},
                        &amp;ocsCephObjectStores{},
                        &amp;ocsCephObjectStoreUsers{},
                        &amp;ocsCephBlockPools{},
                        &amp;ocsCephFilesystems{},
                        &amp;ocsCephConfig{},
                        &amp;ocsCephCluster{},
                        &amp;ocsNoobaaSystem{},
                        &amp;ocsJobTemplates{},
                        &amp;ocsQuickStarts{},
                }

        }</span> else<span class="cov8" title="1"> {
                // for external cluster, we have a different set of ensure functions
                objs = []resourceManager{
                        &amp;ocsExternalResources{},
                        &amp;ocsCephCluster{},
                        &amp;ocsSnapshotClass{},
                        &amp;ocsNoobaaSystem{},
                        &amp;ocsQuickStarts{},
                }
        }</span>

        <span class="cov8" title="1">for _, obj := range objs </span><span class="cov8" title="1">{
                err := obj.ensureCreated(r, instance)
                if r.phase == statusutil.PhaseClusterExpanding </span><span class="cov0" title="0">{
                        instance.Status.Phase = statusutil.PhaseClusterExpanding
                }</span> else<span class="cov8" title="1"> if instance.Status.Phase != statusutil.PhaseReady &amp;&amp;
                        instance.Status.Phase != statusutil.PhaseConnecting </span><span class="cov8" title="1">{
                        instance.Status.Phase = statusutil.PhaseProgressing
                }</span>
                <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                        reason := ocsv1.ReconcileFailed
                        message := fmt.Sprintf("Error while reconciling: %v", err)
                        statusutil.SetErrorCondition(&amp;instance.Status.Conditions, reason, message)
                        instance.Status.Phase = statusutil.PhaseError
                        // don't want to overwrite the actual reconcile failure
                        return reconcile.Result{}, err
                }</span>
        }
        // All component operators are in a happy state.
        <span class="cov8" title="1">if r.conditions == nil </span><span class="cov8" title="1">{
                r.Log.Info("No component operator reported negatively")
                reason := ocsv1.ReconcileCompleted
                message := ocsv1.ReconcileCompletedMessage
                statusutil.SetCompleteCondition(&amp;instance.Status.Conditions, reason, message)

                // If no operator whose conditions we are watching reports an error, then it is safe
                // to set readiness.
                readiness := statusutil.NewFileReady()
                if err := readiness.Set(); err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Failed to mark operator ready")
                        return reconcile.Result{}, err
                }</span>
                <span class="cov8" title="1">if instance.Status.Phase != statusutil.PhaseClusterExpanding &amp;&amp;
                        !instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                        instance.Status.Phase = statusutil.PhaseReady
                }</span>
        } else<span class="cov8" title="1"> {
                // If any component operator reports negatively we want to write that to
                // the instance while preserving it's lastTransitionTime.
                // For example, consider the resource has the Available condition
                // type with type "False". When reconciling the resource we would
                // add it to the in-memory representation of OCS's conditions (r.conditions)
                // and here we are simply writing it back to the server.
                // One shortcoming is that only one failure of a particular condition can be
                // captured at one time (ie. if resource1 and resource2 are both reporting !Available,
                // you will only see resource2q as it updates last).
                for _, condition := range r.conditions </span><span class="cov8" title="1">{
                        conditionsv1.SetStatusCondition(&amp;instance.Status.Conditions, condition)
                }</span>
                <span class="cov8" title="1">reason := ocsv1.ReconcileCompleted
                message := ocsv1.ReconcileCompletedMessage
                conditionsv1.SetStatusCondition(&amp;instance.Status.Conditions, conditionsv1.Condition{
                        Type:    ocsv1.ConditionReconcileComplete,
                        Status:  corev1.ConditionTrue,
                        Reason:  reason,
                        Message: message,
                })

                // If for any reason we marked ourselves !upgradeable...then unset readiness
                if conditionsv1.IsStatusConditionFalse(instance.Status.Conditions, conditionsv1.ConditionUpgradeable) </span><span class="cov8" title="1">{
                        readiness := statusutil.NewFileReady()
                        if err := readiness.Unset(); err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, "Failed to mark operator unready")
                                return reconcile.Result{}, err
                        }</span>
                }
                <span class="cov8" title="1">if instance.Status.Phase != statusutil.PhaseClusterExpanding &amp;&amp;
                        !instance.Spec.ExternalStorage.Enable </span><span class="cov8" title="1">{
                        if conditionsv1.IsStatusConditionTrue(instance.Status.Conditions, conditionsv1.ConditionProgressing) </span><span class="cov8" title="1">{
                                instance.Status.Phase = statusutil.PhaseProgressing
                        }</span> else<span class="cov0" title="0"> if conditionsv1.IsStatusConditionFalse(instance.Status.Conditions, conditionsv1.ConditionUpgradeable) </span><span class="cov0" title="0">{
                                instance.Status.Phase = statusutil.PhaseNotReady
                        }</span> else<span class="cov0" title="0"> {
                                instance.Status.Phase = statusutil.PhaseError
                        }</span>
                }
        }

        // enable metrics exporter at the end of reconcile
        // this allows storagecluster to be instantiated before
        // scraping metrics
        <span class="cov8" title="1">if err := r.enableMetricsExporter(instance); err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed to reconcile metrics exporter")
                return reconcile.Result{}, err
        }</span>

        <span class="cov8" title="1">if err := r.enablePrometheusRules(instance.Spec.ExternalStorage.Enable); err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed to reconcile prometheus rules")
                return reconcile.Result{}, err
        }</span>

        <span class="cov8" title="1">return reconcile.Result{}, nil</span>
}

// versionCheck populates the `.Spec.Version` field
func versionCheck(sc *ocsv1.StorageCluster, reqLogger logr.Logger) error <span class="cov8" title="1">{
        if sc.Spec.Version == "" </span><span class="cov8" title="1">{
                sc.Spec.Version = version.Version
        }</span> else<span class="cov8" title="1"> if sc.Spec.Version != version.Version </span><span class="cov8" title="1">{ // check anything else only if the versions mis-match
                storClustSemV1, err := semver.Make(sc.Spec.Version)
                if err != nil </span><span class="cov0" title="0">{
                        reqLogger.Error(err, "Error while parsing Storage Cluster version")
                        return err
                }</span>
                <span class="cov8" title="1">ocsSemV1, err := semver.Make(version.Version)
                if err != nil </span><span class="cov0" title="0">{
                        reqLogger.Error(err, "Error while parsing OCS Operator version")
                        return err
                }</span>
                // if the storage cluster version is higher than the invoking OCS Operator's version,
                // return error
                <span class="cov8" title="1">if storClustSemV1.GT(ocsSemV1) </span><span class="cov8" title="1">{
                        err = fmt.Errorf("Storage cluster version (%s) is higher than the OCS Operator version (%s)",
                                sc.Spec.Version, version.Version)
                        reqLogger.Error(err, "Incompatible Storage cluster version")
                        return err
                }</span>
                // if the storage cluster version is less than the OCS Operator version,
                // just update.
                <span class="cov8" title="1">sc.Spec.Version = version.Version</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// validateStorageDeviceSets checks the StorageDeviceSets of the given
// StorageCluster for completeness and correctness
func (r *StorageClusterReconciler) validateStorageDeviceSets(sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        for i, ds := range sc.Spec.StorageDeviceSets </span><span class="cov8" title="1">{
                if ds.DataPVCTemplate.Spec.StorageClassName == nil || *ds.DataPVCTemplate.Spec.StorageClassName == "" </span><span class="cov8" title="1">{
                        return fmt.Errorf("failed to validate StorageDeviceSet %d: no StorageClass specified", i)
                }</span>
                <span class="cov8" title="1">if ds.MetadataPVCTemplate != nil </span><span class="cov8" title="1">{
                        if ds.MetadataPVCTemplate.Spec.StorageClassName == nil || *ds.MetadataPVCTemplate.Spec.StorageClassName == "" </span><span class="cov8" title="1">{
                                return fmt.Errorf("failed to validate StorageDeviceSet %d: no StorageClass specified for metadataPVCTemplate", i)
                        }</span>
                }
                <span class="cov8" title="1">if ds.WalPVCTemplate != nil </span><span class="cov8" title="1">{
                        if ds.WalPVCTemplate.Spec.StorageClassName == nil || *ds.WalPVCTemplate.Spec.StorageClassName == "" </span><span class="cov8" title="1">{
                                return fmt.Errorf("failed to validate StorageDeviceSet %d: no StorageClass specified for walPVCTemplate", i)
                        }</span>
                }
                <span class="cov8" title="1">if ds.DeviceType != "" </span><span class="cov0" title="0">{
                        if (DeviceTypeSSD != strings.ToLower(ds.DeviceType)) &amp;&amp; (DeviceTypeHDD != strings.ToLower(ds.DeviceType)) &amp;&amp; (DeviceTypeNVMe != strings.ToLower(ds.DeviceType)) </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to validate DeviceType %q: no Device of this type", ds.DeviceType)
                        }</span>
                }
        }

        <span class="cov8" title="1">return nil</span>
}

// ensureCreated ensures that a ConfigMap resource exists with its Spec in
// the desired state.
func (obj *ocsCephConfig) ensureCreated(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        ownerRef := metav1.OwnerReference{
                UID:        sc.UID,
                APIVersion: sc.APIVersion,
                Kind:       sc.Kind,
                Name:       sc.Name,
        }
        cm := &amp;corev1.ConfigMap{
                ObjectMeta: metav1.ObjectMeta{
                        Name:            rookConfigMapName,
                        Namespace:       sc.Namespace,
                        OwnerReferences: []metav1.OwnerReference{ownerRef},
                },
                Data: map[string]string{
                        "config": rookConfigData,
                },
        }

        found := &amp;corev1.ConfigMap{}
        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: rookConfigMapName, Namespace: sc.Namespace}, found)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("Creating Ceph ConfigMap")
                        err = r.Client.Create(context.TODO(), cm)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
                <span class="cov8" title="1">return err</span>
        }

        <span class="cov0" title="0">ownerRefFound := false
        for _, ownerRef := range found.OwnerReferences </span><span class="cov0" title="0">{
                if ownerRef.UID == sc.UID </span><span class="cov0" title="0">{
                        ownerRefFound = true
                }</span>
        }
        <span class="cov0" title="0">val, ok := found.Data["config"]
        if !ok || val != rookConfigData || !ownerRefFound </span><span class="cov0" title="0">{
                r.Log.Info("Updating Ceph ConfigMap")
                return r.Client.Update(context.TODO(), cm)
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// ensureDeleted is dummy func for the ocsCephConfig
func (obj *ocsCephConfig) ensureDeleted(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov0" title="0">{
        return nil
}</span>

func (r *StorageClusterReconciler) isActiveStorageCluster(instance *ocsv1.StorageCluster) (bool, error) <span class="cov8" title="1">{
        storageClusterList := ocsv1.StorageClusterList{}

        // instance is already marked for deletion
        // do not mark it as active
        if !instance.GetDeletionTimestamp().IsZero() </span><span class="cov0" title="0">{
                return false, nil
        }</span>

        <span class="cov8" title="1">err := r.Client.List(context.TODO(), &amp;storageClusterList, client.InNamespace(instance.Namespace))
        if err != nil </span><span class="cov0" title="0">{
                return false, fmt.Errorf("Error fetching StorageClusterList. %+v", err)
        }</span>

        // There is only one StorageCluster i.e. instance
        <span class="cov8" title="1">if len(storageClusterList.Items) == 1 </span><span class="cov8" title="1">{
                return true, nil
        }</span>

        // There are many StorageClusters. Check if this is Active
        <span class="cov8" title="1">for n, storageCluster := range storageClusterList.Items </span><span class="cov8" title="1">{
                if storageCluster.Status.Phase != statusutil.PhaseIgnored &amp;&amp;
                        storageCluster.ObjectMeta.Name != instance.ObjectMeta.Name </span><span class="cov8" title="1">{
                        // Both StorageClusters are in creation phase
                        // Tiebreak using CreationTimestamp and Alphanumeric ordering
                        if storageCluster.Status.Phase == "" </span><span class="cov8" title="1">{
                                if storageCluster.CreationTimestamp.Before(&amp;instance.CreationTimestamp) </span><span class="cov0" title="0">{
                                        return false, nil
                                }</span> else<span class="cov8" title="1"> if storageCluster.CreationTimestamp.Equal(&amp;instance.CreationTimestamp) &amp;&amp; storageCluster.Name &lt; instance.Name </span><span class="cov8" title="1">{
                                        return false, nil
                                }</span>
                                <span class="cov8" title="1">if n == len(storageClusterList.Items)-1 </span><span class="cov8" title="1">{
                                        return true, nil
                                }</span>
                                <span class="cov0" title="0">continue</span>
                        }
                        <span class="cov0" title="0">return false, nil</span>
                }
        }
        <span class="cov8" title="1">return true, nil</span>
}

// Checks whether a string is contained within a slice
func contains(slice []string, s string) bool <span class="cov8" title="1">{
        for _, item := range slice </span><span class="cov8" title="1">{
                if item == s </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

// Removes a given string from a slice and returns the new slice
func remove(slice []string, s string) (result []string) <span class="cov8" title="1">{
        for _, item := range slice </span><span class="cov8" title="1">{
                if item == s </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov0" title="0">result = append(result, item)</span>
        }
        <span class="cov8" title="1">return</span>
}

// ensureCreated ensures if the osd removal job template exists
func (obj *ocsJobTemplates) ensureCreated(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        osdCleanUpTemplate := &amp;openshiftv1.Template{
                ObjectMeta: metav1.ObjectMeta{
                        Name:      "ocs-osd-removal",
                        Namespace: sc.Namespace,
                },
        }
        _, err := controllerutil.CreateOrUpdate(context.TODO(), r.Client, osdCleanUpTemplate, func() error </span><span class="cov8" title="1">{
                osdCleanUpTemplate.Objects = []runtime.RawExtension{
                        {
                                Object: newCleanupJob(sc),
                        },
                }
                osdCleanUpTemplate.Parameters = []openshiftv1.Parameter{
                        {
                                Name:        "FAILED_OSD_IDS",
                                DisplayName: "OSD IDs",
                                Required:    true,
                                Description: `
The parameter OSD IDs needs a comma-separated list of numerical FAILED_OSD_IDs 
when a single job removes multiple OSDs. 
If the expected comma-separated format is not used, 
or an ID cannot be converted to an int, 
or if an OSD ID is not found, errors will be generated in the log and no OSDs would be removed.`,
                        },
                }
                return controllerutil.SetControllerReference(sc, osdCleanUpTemplate, r.Scheme)
        }</span>)
        <span class="cov8" title="1">if err != nil &amp;&amp; !errors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create Template: %v", err.Error())
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted is dummy func for the ocsJobTemplates
func (obj *ocsJobTemplates) ensureDeleted(r *StorageClusterReconciler, sc *ocsv1.StorageCluster) error <span class="cov0" title="0">{
        return nil
}</span>

func newCleanupJob(sc *ocsv1.StorageCluster) *batchv1.Job <span class="cov8" title="1">{
        labels := map[string]string{
                "app": "ceph-toolbox-job",
        }

        // Annotation template.alpha.openshift.io/wait-for-ready ensures template readiness
        annotations := map[string]string{
                "template.alpha.openshift.io/wait-for-ready": "true",
        }

        job := &amp;batchv1.Job{
                TypeMeta: metav1.TypeMeta{
                        Kind:       "Job",
                        APIVersion: "batch/v1",
                },
                ObjectMeta: metav1.ObjectMeta{
                        Name:        "ocs-osd-removal-job",
                        Namespace:   sc.Namespace,
                        Labels:      labels,
                        Annotations: annotations,
                },
                Spec: batchv1.JobSpec{
                        Template: corev1.PodTemplateSpec{
                                Spec: corev1.PodSpec{
                                        RestartPolicy:      corev1.RestartPolicyNever,
                                        ServiceAccountName: "rook-ceph-system",
                                        Volumes: []corev1.Volume{
                                                {
                                                        Name:         "ceph-conf-emptydir",
                                                        VolumeSource: corev1.VolumeSource{EmptyDir: &amp;corev1.EmptyDirVolumeSource{}},
                                                },
                                                {
                                                        Name:         "rook-config",
                                                        VolumeSource: corev1.VolumeSource{EmptyDir: &amp;corev1.EmptyDirVolumeSource{}},
                                                },
                                        },

                                        Containers: []corev1.Container{
                                                {
                                                        Name:  "operator",
                                                        Image: os.Getenv("ROOK_CEPH_IMAGE"),
                                                        Args: []string{
                                                                "ceph",
                                                                "osd",
                                                                "remove",
                                                                "--osd-ids=${FAILED_OSD_IDS}",
                                                        },
                                                        VolumeMounts: []corev1.VolumeMount{
                                                                {
                                                                        Name:      "ceph-conf-emptydir",
                                                                        MountPath: "/etc/ceph",
                                                                },
                                                                {
                                                                        Name:      "rook-config",
                                                                        MountPath: "/var/lib/rook",
                                                                },
                                                        },
                                                        Env: []corev1.EnvVar{
                                                                {
                                                                        Name: "ROOK_MON_ENDPOINTS",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                ConfigMapKeyRef: &amp;corev1.ConfigMapKeySelector{
                                                                                        Key:                  "data",
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon-endpoints"},
                                                                                },
                                                                        },
                                                                },
                                                                {
                                                                        Name:  "POD_NAMESPACE",
                                                                        Value: sc.Namespace,
                                                                },
                                                                {
                                                                        Name: "ROOK_CEPH_USERNAME",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                SecretKeyRef: &amp;corev1.SecretKeySelector{
                                                                                        Key:                  "ceph-username",
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon"},
                                                                                },
                                                                        },
                                                                },
                                                                {
                                                                        Name: "ROOK_CEPH_SECRET",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                SecretKeyRef: &amp;corev1.SecretKeySelector{
                                                                                        Key:                  "ceph-secret",
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon"},
                                                                                },
                                                                        },
                                                                },
                                                                {
                                                                        Name: "ROOK_FSID",
                                                                        ValueFrom: &amp;corev1.EnvVarSource{
                                                                                SecretKeyRef: &amp;corev1.SecretKeySelector{
                                                                                        Key:                  "fsid",
                                                                                        LocalObjectReference: corev1.LocalObjectReference{Name: "rook-ceph-mon"},
                                                                                },
                                                                        },
                                                                },
                                                                {
                                                                        Name:  "ROOK_CONFIG_DIR",
                                                                        Value: "/var/lib/rook",
                                                                },
                                                                {
                                                                        Name:  "ROOK_CEPH_CONFIG_OVERRIDE",
                                                                        Value: "/etc/rook/config/override.conf",
                                                                },
                                                                {
                                                                        Name:  "ROOK_LOG_LEVEL",
                                                                        Value: "DEBUG",
                                                                },
                                                        },
                                                },
                                        },
                                },
                        },
                },
        }

        return job
}</span>

func validateArbiterSpec(sc *ocsv1.StorageCluster, reqLogger logr.Logger) error <span class="cov8" title="1">{

        if sc.Spec.Arbiter.Enable &amp;&amp; sc.Spec.FlexibleScaling </span><span class="cov0" title="0">{
                return fmt.Errorf("arbiter and flexibleScaling both can't be enabled")
        }</span>
        <span class="cov8" title="1">if sc.Spec.Arbiter.Enable &amp;&amp; sc.Spec.NodeTopologies.ArbiterLocation == "" </span><span class="cov0" title="0">{
                return fmt.Errorf("arbiter is set to enable but no arbiterLocation has been provided in the Spec.NodeTopologies.ArbiterLocation")
        }</span>
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file19" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        "reflect"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        corev1 "k8s.io/api/core/v1"
        storagev1 "k8s.io/api/storage/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
)

// StorageClassConfiguration provides configuration options for a StorageClass.
type StorageClassConfiguration struct {
        storageClass      *storagev1.StorageClass
        reconcileStrategy ReconcileStrategy
        disable           bool
}

type ocsStorageClass struct{}

// ensureCreated ensures that StorageClass resources exist in the desired
// state.
func (obj *ocsStorageClass) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        scs, err := r.newStorageClassConfigurations(instance)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">err = r.createStorageClasses(scs)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the storageClasses that the ocs-operator created
func (obj *ocsStorageClass) ensureDeleted(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{

        sccs, err := r.newStorageClassConfigurations(instance)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to determine the StorageClass names")) //nolint:gosimple
                return nil
        }</span>
        <span class="cov8" title="1">for _, scc := range sccs </span><span class="cov8" title="1">{
                sc := scc.storageClass
                existing := storagev1.StorageClass{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: sc.Name, Namespace: sc.Namespace}, &amp;existing)

                switch </span>{
                case err == nil:<span class="cov8" title="1">
                        if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                r.Log.Info(fmt.Sprintf("Uninstall: StorageClass %s is already marked for deletion", existing.Name))
                                break</span>
                        }

                        <span class="cov8" title="1">r.Log.Info(fmt.Sprintf("Uninstall: Deleting StorageClass %s", sc.Name))
                        existing.ObjectMeta.OwnerReferences = sc.ObjectMeta.OwnerReferences
                        sc.ObjectMeta = existing.ObjectMeta

                        err = r.Client.Delete(context.TODO(), sc)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Ignoring error deleting the StorageClass %s", existing.Name))
                        }</span>
                case errors.IsNotFound(err):<span class="cov8" title="1">
                        r.Log.Info(fmt.Sprintf("Uninstall: StorageClass %s not found, nothing to do", sc.Name))</span>
                default:<span class="cov0" title="0">
                        r.Log.Info(fmt.Sprintf("Uninstall: Error while getting StorageClass %s: %v", sc.Name, err))</span>
                }
        }
        <span class="cov8" title="1">return nil</span>
}

func (r *StorageClusterReconciler) createStorageClasses(sccs []StorageClassConfiguration) error <span class="cov8" title="1">{
        for _, scc := range sccs </span><span class="cov8" title="1">{
                if scc.reconcileStrategy == ReconcileStrategyIgnore || scc.disable </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov8" title="1">sc := scc.storageClass
                existing := &amp;storagev1.StorageClass{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: sc.Name, Namespace: sc.Namespace}, existing)

                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        // Since the StorageClass is not found, we will create a new one
                        r.Log.Info(fmt.Sprintf("Creating StorageClass %s", sc.Name))
                        err = r.Client.Create(context.TODO(), sc)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                } else<span class="cov8" title="1"> if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span> else<span class="cov8" title="1"> {
                        if scc.reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                                continue</span>
                        }
                        <span class="cov8" title="1">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to restore storageclass  %s because it is marked for deletion", existing.Name)
                        }</span>
                        <span class="cov8" title="1">if !reflect.DeepEqual(sc.Parameters, existing.Parameters) </span><span class="cov0" title="0">{
                                // Since we have to update the existing StorageClass
                                // So, we will delete the existing storageclass and create a new one
                                r.Log.Info(fmt.Sprintf("StorageClass %s needs to be updated, deleting it", existing.Name))
                                err = r.Client.Delete(context.TODO(), existing)
                                if err != nil </span><span class="cov0" title="0">{
                                        return err
                                }</span>
                                <span class="cov0" title="0">r.Log.Info(fmt.Sprintf("Creating StorageClass %s", sc.Name))
                                err = r.Client.Create(context.TODO(), sc)
                                if err != nil </span><span class="cov0" title="0">{
                                        return err
                                }</span>
                        }
                }
        }
        <span class="cov8" title="1">return nil</span>
}

// newCephFilesystemStorageClassConfiguration generates configuration options for a Ceph Filesystem StorageClass.
func newCephFilesystemStorageClassConfiguration(initData *ocsv1.StorageCluster) StorageClassConfiguration <span class="cov8" title="1">{
        persistentVolumeReclaimDelete := corev1.PersistentVolumeReclaimDelete
        allowVolumeExpansion := true
        managementSpec := initData.Spec.ManagedResources.CephFilesystems
        return StorageClassConfiguration{
                storageClass: &amp;storagev1.StorageClass{
                        ObjectMeta: metav1.ObjectMeta{
                                Name: generateNameForCephFilesystemSC(initData),
                                Annotations: map[string]string{
                                        "description": "Provides RWO and RWX Filesystem volumes",
                                },
                        },
                        Provisioner:   fmt.Sprintf("%s.cephfs.csi.ceph.com", initData.Namespace),
                        ReclaimPolicy: &amp;persistentVolumeReclaimDelete,
                        // AllowVolumeExpansion is set to true to enable expansion of OCS backed Volumes
                        AllowVolumeExpansion: &amp;allowVolumeExpansion,
                        Parameters: map[string]string{
                                "clusterID": initData.Namespace,
                                "fsName":    fmt.Sprintf("%s-cephfilesystem", initData.Name),
                                "csi.storage.k8s.io/provisioner-secret-name":            "rook-csi-cephfs-provisioner",
                                "csi.storage.k8s.io/provisioner-secret-namespace":       initData.Namespace,
                                "csi.storage.k8s.io/node-stage-secret-name":             "rook-csi-cephfs-node",
                                "csi.storage.k8s.io/node-stage-secret-namespace":        initData.Namespace,
                                "csi.storage.k8s.io/controller-expand-secret-name":      "rook-csi-cephfs-provisioner",
                                "csi.storage.k8s.io/controller-expand-secret-namespace": initData.Namespace,
                        },
                },
                reconcileStrategy: ReconcileStrategy(managementSpec.ReconcileStrategy),
                disable:           managementSpec.DisableStorageClass,
        }
}</span>

// newCephBlockPoolStorageClassConfiguration generates configuration options for a Ceph Block Pool StorageClass.
func newCephBlockPoolStorageClassConfiguration(initData *ocsv1.StorageCluster) StorageClassConfiguration <span class="cov8" title="1">{
        persistentVolumeReclaimDelete := corev1.PersistentVolumeReclaimDelete
        allowVolumeExpansion := true
        managementSpec := initData.Spec.ManagedResources.CephBlockPools
        return StorageClassConfiguration{
                storageClass: &amp;storagev1.StorageClass{
                        ObjectMeta: metav1.ObjectMeta{
                                Name: generateNameForCephBlockPoolSC(initData),
                                Annotations: map[string]string{
                                        "description": "Provides RWO Filesystem volumes, and RWO and RWX Block volumes",
                                },
                        },
                        Provisioner:   fmt.Sprintf("%s.rbd.csi.ceph.com", initData.Namespace),
                        ReclaimPolicy: &amp;persistentVolumeReclaimDelete,
                        // AllowVolumeExpansion is set to true to enable expansion of OCS backed Volumes
                        AllowVolumeExpansion: &amp;allowVolumeExpansion,
                        Parameters: map[string]string{
                                "clusterID":                 initData.Namespace,
                                "pool":                      generateNameForCephBlockPool(initData),
                                "imageFeatures":             "layering",
                                "csi.storage.k8s.io/fstype": "ext4",
                                "imageFormat":               "2",
                                "csi.storage.k8s.io/provisioner-secret-name":            "rook-csi-rbd-provisioner",
                                "csi.storage.k8s.io/provisioner-secret-namespace":       initData.Namespace,
                                "csi.storage.k8s.io/node-stage-secret-name":             "rook-csi-rbd-node",
                                "csi.storage.k8s.io/node-stage-secret-namespace":        initData.Namespace,
                                "csi.storage.k8s.io/controller-expand-secret-name":      "rook-csi-rbd-provisioner",
                                "csi.storage.k8s.io/controller-expand-secret-namespace": initData.Namespace,
                        },
                },
                reconcileStrategy: ReconcileStrategy(managementSpec.ReconcileStrategy),
                disable:           managementSpec.DisableStorageClass,
        }
}</span>

// newCephOBCStorageClassConfiguration generates configuration options for a Ceph Object Store StorageClass.
func newCephOBCStorageClassConfiguration(initData *ocsv1.StorageCluster) StorageClassConfiguration <span class="cov8" title="1">{
        reclaimPolicy := corev1.PersistentVolumeReclaimDelete
        managementSpec := initData.Spec.ManagedResources.CephObjectStores
        return StorageClassConfiguration{
                storageClass: &amp;storagev1.StorageClass{
                        ObjectMeta: metav1.ObjectMeta{
                                Name: generateNameForCephRgwSC(initData),
                                Annotations: map[string]string{
                                        "description": "Provides Object Bucket Claims (OBCs)",
                                },
                        },
                        Provisioner:   fmt.Sprintf("%s.ceph.rook.io/bucket", initData.Namespace),
                        ReclaimPolicy: &amp;reclaimPolicy,
                        Parameters: map[string]string{
                                "objectStoreNamespace": initData.Namespace,
                                "region":               "us-east-1",
                                "objectStoreName":      generateNameForCephObjectStore(initData),
                        },
                },
                reconcileStrategy: ReconcileStrategy(managementSpec.ReconcileStrategy),
                disable:           managementSpec.DisableStorageClass,
        }
}</span>

// newStorageClassConfigurations returns the StorageClassConfiguration instances that should be created
// on first run.
func (r *StorageClusterReconciler) newStorageClassConfigurations(initData *ocsv1.StorageCluster) ([]StorageClassConfiguration, error) <span class="cov8" title="1">{
        ret := []StorageClassConfiguration{
                newCephFilesystemStorageClassConfiguration(initData),
                newCephBlockPoolStorageClassConfiguration(initData),
        }
        // OBC storageclass will be returned only in TWO conditions,
        // a. either 'externalStorage' is enabled
        // OR
        // b. current platform is not a cloud-based platform
        platform, err := r.platform.GetPlatform(r.Client)
        if err != nil </span><span class="cov0" title="0">{
                return ret, err
        }</span>
        <span class="cov8" title="1">isIBMWithSecret, err1 := isIBMPlatformWithCosSecret(platform, initData.Namespace, r.Client)
        if err1 != nil </span><span class="cov0" title="0">{
                return ret, err1
        }</span>
        <span class="cov8" title="1">if initData.Spec.ExternalStorage.Enable || (!avoidObjectStore(platform) &amp;&amp; !isIBMWithSecret) </span><span class="cov8" title="1">{
                ret = append(ret, newCephOBCStorageClassConfiguration(initData))
        }</span>
        <span class="cov8" title="1">return ret, nil</span>
}
</pre>
		
		<pre class="file" id="file20" style="display: none">package storagecluster

import (
        "fmt"
        "os"

        "github.com/go-logr/logr"
        nbv1 "github.com/noobaa/noobaa-operator/v2/pkg/apis/noobaa/v1alpha1"
        conditionsv1 "github.com/openshift/custom-resource-status/conditions/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/util"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/runtime"
        "k8s.io/apimachinery/pkg/version"
        "k8s.io/client-go/kubernetes"
        "k8s.io/client-go/tools/record"
        ctrl "sigs.k8s.io/controller-runtime"
        "sigs.k8s.io/controller-runtime/pkg/builder"
        "sigs.k8s.io/controller-runtime/pkg/client"
        "sigs.k8s.io/controller-runtime/pkg/client/config"
        "sigs.k8s.io/controller-runtime/pkg/event"
        "sigs.k8s.io/controller-runtime/pkg/predicate"
)

var (
        log = ctrl.Log.WithName("controllers").WithName("StorageCluster")
)

func (r *StorageClusterReconciler) initializeImageVars() error <span class="cov8" title="1">{
        r.images.Ceph = os.Getenv("CEPH_IMAGE")
        r.images.NooBaaCore = os.Getenv("NOOBAA_CORE_IMAGE")
        r.images.NooBaaDB = os.Getenv("NOOBAA_DB_IMAGE")

        if r.images.Ceph == "" </span><span class="cov8" title="1">{
                err := fmt.Errorf("CEPH_IMAGE environment variable not found")
                r.Log.Error(err, "missing required environment variable for ocs initialization")
                return err
        }</span> else<span class="cov0" title="0"> if r.images.NooBaaCore == "" </span><span class="cov0" title="0">{
                err := fmt.Errorf("NOOBAA_CORE_IMAGE environment variable not found")
                r.Log.Error(err, "missing required environment variable for ocs initialization")
                return err
        }</span> else<span class="cov0" title="0"> if r.images.NooBaaDB == "" </span><span class="cov0" title="0">{
                err := fmt.Errorf("NOOBAA_DB_IMAGE environment variable not found")
                r.Log.Error(err, "missing required environment variable for ocs initialization")
                return err
        }</span>
        <span class="cov0" title="0">return nil</span>
}

func (r *StorageClusterReconciler) initializeServerVersion() error <span class="cov0" title="0">{
        clientset, err := kubernetes.NewForConfig(config.GetConfigOrDie())
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed creation of clientset for determining serverversion")
                return err
        }</span>
        <span class="cov0" title="0">r.serverVersion, err = clientset.Discovery().ServerVersion()
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, "failed getting the serverversion")
                return err
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// ImageMap holds mapping information between component image name and the image url
type ImageMap struct {
        Ceph       string
        NooBaaCore string
        NooBaaDB   string
}

// StorageClusterReconciler reconciles a StorageCluster object
//nolint
type StorageClusterReconciler struct {
        client.Client
        Log            logr.Logger
        Scheme         *runtime.Scheme
        serverVersion  *version.Info
        conditions     []conditionsv1.Condition
        phase          string
        monitoringIP   string
        monitoringPort string
        nodeCount      int
        platform       *Platform
        images         ImageMap
        recorder       record.EventRecorder
}

// SetupWithManager sets up a controller with manager
func (r *StorageClusterReconciler) SetupWithManager(mgr ctrl.Manager) error <span class="cov0" title="0">{
        if err := r.initializeImageVars(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">if err := r.initializeServerVersion(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">r.platform = &amp;Platform{}
        r.recorder = mgr.GetEventRecorderFor("controller_storagecluster")

        // Compose a predicate that is an OR of the specified predicates
        scPredicate := util.ComposePredicates(
                predicate.GenerationChangedPredicate{},
                util.MetadataChangedPredicate{},
        )

        pvcPredicate := predicate.Funcs{
                DeleteFunc: func(e event.DeleteEvent) bool </span><span class="cov0" title="0">{
                        // Evaluates to false if the object has been confirmed deleted.
                        return !e.DeleteStateUnknown
                }</span>,
        }

        <span class="cov0" title="0">return ctrl.NewControllerManagedBy(mgr).
                For(&amp;ocsv1.StorageCluster{}, builder.WithPredicates(scPredicate)).
                Owns(&amp;cephv1.CephCluster{}).
                Owns(&amp;nbv1.NooBaa{}).
                Owns(&amp;corev1.PersistentVolumeClaim{}, builder.WithPredicates(pvcPredicate)).
                Complete(r)</span>
}
</pre>
		
		<pre class="file" id="file21" style="display: none">package storagecluster

import (
        "context"
        "encoding/json"
        "fmt"
        "sort"
        "strings"

        "github.com/openshift/ocs-operator/controllers/defaults"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/strategicpatch"
        "sigs.k8s.io/controller-runtime/pkg/client"

        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        corev1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func (r *StorageClusterReconciler) getStorageClusterEligibleNodes(sc *ocsv1.StorageCluster) (nodes *corev1.NodeList, err error) <span class="cov8" title="1">{
        nodes = &amp;corev1.NodeList{}
        var selector labels.Selector

        labelSelector := &amp;metav1.LabelSelector{
                MatchLabels: map[string]string{defaults.NodeAffinityKey: ""},
        }
        if sc.Spec.LabelSelector != nil </span><span class="cov8" title="1">{
                labelSelector = sc.Spec.LabelSelector
        }</span>

        <span class="cov8" title="1">selector, err = metav1.LabelSelectorAsSelector(labelSelector)
        if err != nil </span><span class="cov0" title="0">{
                return nodes, err
        }</span>
        <span class="cov8" title="1">err = r.Client.List(context.TODO(), nodes, MatchingLabelsSelector{Selector: selector})

        return nodes, err</span>
}

// determineFailureDomain determines the appropriate Ceph failure domain based
// on the storage cluster's topology map
func determineFailureDomain(sc *ocsv1.StorageCluster) string <span class="cov8" title="1">{
        if sc.Status.FailureDomain != "" </span><span class="cov8" title="1">{
                return sc.Status.FailureDomain
        }</span>

        <span class="cov8" title="1">if sc.Spec.FlexibleScaling </span><span class="cov8" title="1">{
                return "host"
        }</span>

        <span class="cov8" title="1">topologyMap := sc.Status.NodeTopologies
        failureDomain := "rack"
        for label, labelValues := range topologyMap.Labels </span><span class="cov8" title="1">{
                if strings.Contains(label, "zone") </span><span class="cov8" title="1">{
                        if (len(labelValues) &gt;= 2 &amp;&amp; arbiterEnabled(sc)) || (len(labelValues) &gt;= 3) </span><span class="cov8" title="1">{
                                failureDomain = "zone"
                        }</span>
                }
        }
        <span class="cov8" title="1">return failureDomain</span>
}

// determinePlacementRack sorts the list of known racks in alphabetical order,
// counts the number of Nodes in each rack, then returns the first rack with
// the fewest number of Nodes. If there are fewer than three racks, define new
// racks so that there are at least three. It also ensures that only racks with
// either no nodes or nodes in the same AZ are considered valid racks.
func determinePlacementRack(
        nodes *corev1.NodeList, node corev1.Node,
        minRacks int, nodeRacks *ocsv1.NodeTopologyMap) string <span class="cov8" title="1">{

        rackList := []string{}

        if len(nodeRacks.Labels) &lt; minRacks </span><span class="cov8" title="1">{
                for i := len(nodeRacks.Labels); i &lt; minRacks; i++ </span><span class="cov8" title="1">{
                        for j := 0; j &lt;= i; j++ </span><span class="cov8" title="1">{
                                newRack := fmt.Sprintf("rack%d", j)
                                if _, ok := nodeRacks.Labels[newRack]; !ok </span><span class="cov8" title="1">{
                                        nodeRacks.Labels[newRack] = ocsv1.TopologyLabelValues{}
                                        break</span>
                                }
                        }
                }
        }

        <span class="cov8" title="1">targetAZ := ""
        for label, value := range node.Labels </span><span class="cov8" title="1">{
                for _, key := range validTopologyLabelKeys </span><span class="cov8" title="1">{
                        if strings.Contains(label, key) &amp;&amp; strings.Contains(label, "zone") </span><span class="cov8" title="1">{
                                targetAZ = value
                                break</span>
                        }
                }
                <span class="cov8" title="1">if targetAZ != "" </span><span class="cov8" title="1">{
                        break</span>
                }
        }

        <span class="cov8" title="1">if len(targetAZ) &gt; 0 </span><span class="cov8" title="1">{
                for rack := range nodeRacks.Labels </span><span class="cov8" title="1">{
                        nodeNames := nodeRacks.Labels[rack]
                        if len(nodeNames) == 0 </span><span class="cov8" title="1">{
                                rackList = append(rackList, rack)
                                continue</span>
                        }

                        <span class="cov8" title="1">validRack := false
                        for _, nodeName := range nodeNames </span><span class="cov8" title="1">{
                                for _, n := range nodes.Items </span><span class="cov8" title="1">{
                                        if n.Name == nodeName </span><span class="cov8" title="1">{
                                                for label, value := range n.Labels </span><span class="cov8" title="1">{
                                                        for _, key := range validTopologyLabelKeys </span><span class="cov8" title="1">{
                                                                if strings.Contains(label, key) &amp;&amp; strings.Contains(label, "zone") &amp;&amp; value == targetAZ </span><span class="cov8" title="1">{
                                                                        validRack = true
                                                                        break</span>
                                                                }
                                                        }
                                                        <span class="cov8" title="1">if validRack </span><span class="cov8" title="1">{
                                                                break</span>
                                                        }
                                                }
                                                <span class="cov8" title="1">break</span>
                                        }
                                }
                                <span class="cov8" title="1">if validRack </span><span class="cov8" title="1">{
                                        break</span>
                                }
                        }
                        <span class="cov8" title="1">if validRack </span><span class="cov8" title="1">{
                                rackList = append(rackList, rack)
                        }</span>
                }
        } else<span class="cov8" title="1"> {
                for rack := range nodeRacks.Labels </span><span class="cov8" title="1">{
                        rackList = append(rackList, rack)
                }</span>
        }

        <span class="cov8" title="1">sort.Strings(rackList)
        rack := rackList[0]

        for _, r := range rackList </span><span class="cov8" title="1">{
                if len(nodeRacks.Labels[r]) &lt; len(nodeRacks.Labels[rack]) </span><span class="cov8" title="1">{
                        rack = r
                }</span>
        }

        <span class="cov8" title="1">return rack</span>
}

func generateStrategicPatch(oldObj, newObj interface{}) (client.Patch, error) <span class="cov8" title="1">{
        oldJSON, err := json.Marshal(oldObj)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">newJSON, err := json.Marshal(newObj)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">patch, err := strategicpatch.CreateTwoWayMergePatch(oldJSON, newJSON, oldObj)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return client.RawPatch(types.StrategicMergePatchType, patch), nil</span>
}

// ensureNodeRacks iterates through the list of storage nodes and ensures
// all nodes have a rack topology label.
func (r *StorageClusterReconciler) ensureNodeRacks(
        nodes *corev1.NodeList, minRacks int,
        nodeRacks, topologyMap *ocsv1.NodeTopologyMap) error <span class="cov8" title="1">{

        for _, node := range nodes.Items </span><span class="cov8" title="1">{
                hasRack := false

                for _, nodeNames := range nodeRacks.Labels </span><span class="cov8" title="1">{
                        for _, nodeName := range nodeNames </span><span class="cov8" title="1">{
                                if nodeName == node.Name </span><span class="cov8" title="1">{
                                        hasRack = true
                                        break</span>
                                }
                        }
                        <span class="cov8" title="1">if hasRack </span><span class="cov8" title="1">{
                                break</span>
                        }
                }

                <span class="cov8" title="1">if !hasRack </span><span class="cov8" title="1">{
                        rack := determinePlacementRack(nodes, node, minRacks, nodeRacks)
                        nodeRacks.Add(rack, node.Name)
                        if !topologyMap.Contains(defaults.RackTopologyKey, rack) </span><span class="cov8" title="1">{
                                r.Log.Info("Adding rack label from node", "Node", node.Name, "Label", defaults.RackTopologyKey, "Value", rack)
                                topologyMap.Add(defaults.RackTopologyKey, rack)
                        }</span>

                        <span class="cov8" title="1">r.Log.Info("Labeling node with rack label", "Node", node.Name, "Label", defaults.RackTopologyKey, "Value", rack)
                        newNode := node.DeepCopy()
                        newNode.Labels[defaults.RackTopologyKey] = rack
                        patch, err := generateStrategicPatch(node, newNode)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov8" title="1">err = r.Client.Patch(context.TODO(), &amp;node, patch)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
        }

        <span class="cov8" title="1">return nil</span>
}

// reconcileNodeTopologyMap builds the map of all topology labels on all nodes
// in the storage cluster
func (r *StorageClusterReconciler) reconcileNodeTopologyMap(sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        minNodes := getMinimumNodes(sc)

        nodes, err := r.getStorageClusterEligibleNodes(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if sc.Status.NodeTopologies == nil || sc.Status.NodeTopologies.Labels == nil </span><span class="cov8" title="1">{
                sc.Status.NodeTopologies = ocsv1.NewNodeTopologyMap()
        }</span>
        <span class="cov8" title="1">topologyMap := sc.Status.NodeTopologies
        nodeRacks := ocsv1.NewNodeTopologyMap()

        r.nodeCount = len(nodes.Items)

        if r.nodeCount &lt; minNodes </span><span class="cov8" title="1">{
                return fmt.Errorf("Not enough nodes found: Expected %d, found %d", minNodes, r.nodeCount)
        }</span>

        <span class="cov8" title="1">for _, node := range nodes.Items </span><span class="cov8" title="1">{
                labels := node.Labels
                for label, value := range labels </span><span class="cov8" title="1">{
                        for _, key := range validTopologyLabelKeys </span><span class="cov8" title="1">{
                                if strings.Contains(label, key) </span><span class="cov8" title="1">{
                                        if !topologyMap.Contains(label, value) </span><span class="cov8" title="1">{
                                                r.Log.Info("Adding topology label from node", "Node", node.Name, "Label", label, "Value", value)
                                                topologyMap.Add(label, value)
                                        }</span>
                                }
                        }
                        <span class="cov8" title="1">if strings.Contains(label, "rack") </span><span class="cov0" title="0">{
                                if !nodeRacks.Contains(value, node.Name) </span><span class="cov0" title="0">{
                                        nodeRacks.Add(value, node.Name)
                                }</span>
                        }
                }

        }

        <span class="cov8" title="1">if determineFailureDomain(sc) == "rack" </span><span class="cov8" title="1">{
                err = r.ensureNodeRacks(nodes, minNodes, nodeRacks, topologyMap)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file22" style="display: none">package storagecluster

import (
        "context"
        "encoding/json"
        "fmt"

        nbv1 "github.com/noobaa/noobaa-operator/v2/pkg/apis/noobaa/v1alpha1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        "github.com/openshift/ocs-operator/controllers/defaults"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        corev1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/strategicpatch"
        "sigs.k8s.io/controller-runtime/pkg/client"
)

// CleanupPolicyType is a string representing cleanup policy
type CleanupPolicyType string

// UninstallModeType is a string representing cleanup mode, it decides whether the deletion is graceful or forced
type UninstallModeType string

const (
        // CleanupPolicyAnnotation defines the cleanup policy for data and metadata during uninstall
        CleanupPolicyAnnotation = "uninstall.ocs.openshift.io/cleanup-policy"
        // CleanupPolicyDelete when set, modifies the cleanup policy for Rook to delete the DataDirHostPath on uninstall
        CleanupPolicyDelete CleanupPolicyType = "delete"
        // CleanupPolicyRetain when set, modifies the cleanup policy for Rook to not cleanup the DataDirHostPath and the disks on uninstall
        CleanupPolicyRetain CleanupPolicyType = "retain"
        // UninstallModeAnnotation defines the uninstall mode
        UninstallModeAnnotation = "uninstall.ocs.openshift.io/mode"
        // UninstallModeForced when set, sets the uninstall mode for Rook and Noobaa to forced.
        UninstallModeForced UninstallModeType = "forced"
        // UninstallModeGraceful when set, sets the uninstall mode for Rook and Noobaa to graceful.
        UninstallModeGraceful UninstallModeType = "graceful"
)

//nolint:unused // func deleteNodeAffinityKeyFromNodes is not used. For Future usuage func is created.
// deleteNodeAffinityKeyFromNodes deletes the default NodeAffinityKey from the OCS nodes
func (r *StorageClusterReconciler) deleteNodeAffinityKeyFromNodes(sc *ocsv1.StorageCluster) (err error) <span class="cov8" title="1">{

        // We should delete the label only when the StorageCluster is using the default NodeAffinityKey
        if sc.Spec.LabelSelector == nil </span><span class="cov8" title="1">{
                nodes, err := r.getStorageClusterEligibleNodes(sc)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to obtain the list of nodes eligible for the Storage Cluster")) //nolint:gosimple
                        return nil
                }</span>
                <span class="cov8" title="1">for _, node := range nodes.Items </span><span class="cov8" title="1">{
                        r.Log.Info(fmt.Sprintf("Uninstall: Deleting OCS label from node %s", node.Name))
                        new := node.DeepCopy()
                        delete(new.ObjectMeta.Labels, defaults.NodeAffinityKey)

                        oldJSON, err := json.Marshal(node)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeAffinityKey from the node %s", node.Name))
                                continue</span>
                        }

                        <span class="cov8" title="1">newJSON, err := json.Marshal(new)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeAffinityKey from the node %s", node.Name))
                                continue</span>
                        }

                        <span class="cov8" title="1">patch, err := strategicpatch.CreateTwoWayMergePatch(oldJSON, newJSON, node)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeAffinityKey from the node %s", node.Name))
                                continue</span>
                        }

                        <span class="cov8" title="1">err = r.Client.Patch(context.TODO(), &amp;node, client.RawPatch(types.StrategicMergePatchType, patch))
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeAffinityKey from the node %s", node.Name))
                                continue</span>
                        }

                }

        }
        <span class="cov8" title="1">return nil</span>
}

// deleteNodeTaint deletes the default NodeTolerationKey from the OCS nodes
func (r *StorageClusterReconciler) deleteNodeTaint(sc *ocsv1.StorageCluster) (err error) <span class="cov8" title="1">{

        nodes, err := r.getStorageClusterEligibleNodes(sc)
        if err != nil </span><span class="cov0" title="0">{
                r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to obtain the list of nodes eligible for the Storage Cluster")) //nolint:gosimple
                return nil
        }</span>
        <span class="cov8" title="1">for _, node := range nodes.Items </span><span class="cov8" title="1">{
                r.Log.Info(fmt.Sprintf("Uninstall: Deleting OCS NodeTolerationKey from the node %s", node.Name))
                new := node.DeepCopy()
                new.Spec.Taints = make([]corev1.Taint, 0)
                for _, taint := range node.Spec.Taints </span><span class="cov8" title="1">{
                        if defaults.NodeTolerationKey == taint.Key </span><span class="cov8" title="1">{
                                continue</span>
                        }
                        <span class="cov0" title="0">new.Spec.Taints = append(new.Spec.Taints, taint)</span>
                }

                <span class="cov8" title="1">oldJSON, err := json.Marshal(node)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeTolerationKey from the node %s", node.Name))
                        continue</span>
                }

                <span class="cov8" title="1">newJSON, err := json.Marshal(new)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeTolerationKey from the node %s", node.Name))
                        continue</span>
                }

                <span class="cov8" title="1">patch, err := strategicpatch.CreateTwoWayMergePatch(oldJSON, newJSON, node)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeTolerationKey from the node %s", node.Name))
                        continue</span>
                }

                <span class="cov8" title="1">err = r.Client.Patch(context.TODO(), &amp;node, client.RawPatch(types.StrategicMergePatchType, patch))
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, fmt.Sprintf("Uninstall: Unable to remove the NodeTolerationKey from the node %s", node.Name))
                        continue</span>
                }

        }

        <span class="cov8" title="1">return nil</span>
}

// setRookUninstallandCleanupPolicy sets the uninstall mode and cleanup policy for rook based on the annotation on the StorageCluster
func (r *StorageClusterReconciler) setRookUninstallandCleanupPolicy(instance *ocsv1.StorageCluster) (err error) <span class="cov8" title="1">{

        cephCluster := &amp;cephv1.CephCluster{}
        var updateRequired bool

        err = r.Client.Get(context.TODO(), types.NamespacedName{Name: generateNameForCephCluster(instance), Namespace: instance.Namespace}, cephCluster)
        if err != nil </span><span class="cov0" title="0">{
                if errors.IsNotFound(err) </span><span class="cov0" title="0">{
                        r.Log.Info("Uninstall: CephCluster not found, can't set the cleanup policy and uninstall mode")
                        return nil
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Unable to retrieve the cephCluster: %v", err)</span>
        }

        <span class="cov8" title="1">if v, found := instance.ObjectMeta.Annotations[CleanupPolicyAnnotation]; found </span><span class="cov8" title="1">{
                if (v == string(CleanupPolicyDelete)) &amp;&amp; (cephCluster.Spec.CleanupPolicy.Confirmation != cephv1.DeleteDataDirOnHostsConfirmation) </span><span class="cov8" title="1">{
                        cephCluster.Spec.CleanupPolicy.Confirmation = cephv1.DeleteDataDirOnHostsConfirmation
                        updateRequired = true
                }</span> else<span class="cov8" title="1"> if (v == string(CleanupPolicyRetain)) &amp;&amp; (cephCluster.Spec.CleanupPolicy.Confirmation != "") </span><span class="cov8" title="1">{
                        cephCluster.Spec.CleanupPolicy.Confirmation = ""
                        updateRequired = true
                }</span>
        }

        <span class="cov8" title="1">if v, found := instance.ObjectMeta.Annotations[UninstallModeAnnotation]; found </span><span class="cov8" title="1">{
                if (v == string(UninstallModeForced)) &amp;&amp; (!cephCluster.Spec.CleanupPolicy.AllowUninstallWithVolumes) </span><span class="cov8" title="1">{
                        cephCluster.Spec.CleanupPolicy.AllowUninstallWithVolumes = true
                        updateRequired = true
                }</span> else<span class="cov8" title="1"> if (v == string(UninstallModeGraceful)) &amp;&amp; (cephCluster.Spec.CleanupPolicy.AllowUninstallWithVolumes) </span><span class="cov8" title="1">{
                        cephCluster.Spec.CleanupPolicy.AllowUninstallWithVolumes = false
                        updateRequired = true
                }</span>
        }

        <span class="cov8" title="1">if updateRequired </span><span class="cov8" title="1">{
                err := r.Client.Update(context.TODO(), cephCluster)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("Uninstall: Unable to update the cephCluster to set uninstall mode and/or cleanup policy: %v", err)
                }</span>
                <span class="cov8" title="1">r.Log.Info("Uninstall: CephCluster uninstall mode and cleanup policy has been set")</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// setNoobaaUninstallMode sets the uninstall mode for Noobaa based on the annotation on the StorageCluster
func (r *StorageClusterReconciler) setNoobaaUninstallMode(sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        // Do this if Noobaa is being managed by the OCS operator
        if sc.Spec.MultiCloudGateway != nil </span><span class="cov0" title="0">{
                reconcileStrategy := ReconcileStrategy(sc.Spec.MultiCloudGateway.ReconcileStrategy)
                if reconcileStrategy == ReconcileStrategyIgnore || reconcileStrategy == ReconcileStrategyStandalone </span><span class="cov0" title="0">{
                        return nil
                }</span>
        }
        <span class="cov8" title="1">noobaa := &amp;nbv1.NooBaa{}
        var updateRequired bool

        err := r.Client.Get(context.TODO(), types.NamespacedName{Name: "noobaa", Namespace: sc.Namespace}, noobaa)
        if err != nil </span><span class="cov8" title="1">{
                if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                        r.Log.Info("Uninstall: NooBaa not found, can't set uninstall mode")
                        return nil
                }</span>
                <span class="cov0" title="0">return fmt.Errorf("Uninstall: Error while getting NooBaa %v", err)</span>
        }

        // The CleanupPolicy attribute in the Noobaa spec decides the uninstall mode.
        // Unlike the Rook CleanupPolicy which decides whether the data needs to be erased.
        <span class="cov8" title="1">if v, found := sc.ObjectMeta.Annotations[UninstallModeAnnotation]; found </span><span class="cov8" title="1">{
                if (v == string(UninstallModeForced)) &amp;&amp; (noobaa.Spec.CleanupPolicy.Confirmation != nbv1.DeleteOBCConfirmation) </span><span class="cov8" title="1">{
                        noobaa.Spec.CleanupPolicy.Confirmation = nbv1.DeleteOBCConfirmation
                        updateRequired = true
                }</span> else<span class="cov8" title="1"> if (v == string(UninstallModeGraceful)) &amp;&amp; (noobaa.Spec.CleanupPolicy.Confirmation != "") </span><span class="cov0" title="0">{
                        noobaa.Spec.CleanupPolicy.Confirmation = ""
                        updateRequired = true
                }</span>
        }

        <span class="cov8" title="1">if updateRequired </span><span class="cov8" title="1">{
                err = r.Client.Update(context.TODO(), noobaa)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("Uninstall: Unable to update NooBaa uninstall mode: %v", err)
                }</span>
                <span class="cov8" title="1">r.Log.Info("Uninstall: NooBaa uninstall mode has been set")</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// reconcileUninstallAnnotations looks at the current uninstall annotations on the StorageCluster and sets defaults if none or unrecognized ones are set.
func (r *StorageClusterReconciler) reconcileUninstallAnnotations(sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        var updateRequired bool

        if v, found := sc.ObjectMeta.Annotations[UninstallModeAnnotation]; !found </span><span class="cov8" title="1">{
                metav1.SetMetaDataAnnotation(&amp;sc.ObjectMeta, string(UninstallModeAnnotation), string(UninstallModeGraceful))
                r.Log.Info("Uninstall: Setting uninstall mode annotation to default", "UninstallMode", UninstallModeGraceful)
                updateRequired = true
        }</span> else<span class="cov8" title="1"> if found &amp;&amp; v != string(UninstallModeGraceful) &amp;&amp; v != string(UninstallModeForced) </span><span class="cov8" title="1">{
                // if wrong value found
                metav1.SetMetaDataAnnotation(&amp;sc.ObjectMeta, string(UninstallModeAnnotation), string(UninstallModeGraceful))
                r.Log.Info("Uninstall: Found unrecognized uninstall mode annotation. Changing it to default",
                        "CurrentUninstallMode", v, "DefaultUninstallMode", UninstallModeGraceful)
                updateRequired = true
        }</span>

        <span class="cov8" title="1">if v, found := sc.ObjectMeta.Annotations[CleanupPolicyAnnotation]; !found </span><span class="cov8" title="1">{
                metav1.SetMetaDataAnnotation(&amp;sc.ObjectMeta, string(CleanupPolicyAnnotation), string(CleanupPolicyDelete))
                r.Log.Info("Uninstall: Setting uninstall cleanup policy annotation to default", "CleanupPolicy", CleanupPolicyDelete)
                updateRequired = true
        }</span> else<span class="cov8" title="1"> if found &amp;&amp; v != string(CleanupPolicyDelete) &amp;&amp; v != string(CleanupPolicyRetain) </span><span class="cov8" title="1">{
                // if wrong value found
                metav1.SetMetaDataAnnotation(&amp;sc.ObjectMeta, string(CleanupPolicyAnnotation), string(CleanupPolicyDelete))
                r.Log.Info("Uninstall: Found unrecognized uninstall cleanup policy annotation.Changing it to default",
                        "CurrentCleanupPolicy", v, "DefaultCleanupPolicy", CleanupPolicyDelete)
                updateRequired = true
        }</span>

        <span class="cov8" title="1">if updateRequired </span><span class="cov8" title="1">{
                oldSc := ocsv1.StorageCluster{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: sc.Name, Namespace: sc.Namespace}, &amp;oldSc)
                if err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Uninstall: Failed to get storagecluster")
                        return err
                }</span>
                <span class="cov8" title="1">sc.ObjectMeta.ResourceVersion = oldSc.ObjectMeta.ResourceVersion
                if err := r.Client.Update(context.TODO(), sc); err != nil </span><span class="cov0" title="0">{
                        r.Log.Error(err, "Uninstall: Failed to update the storagecluster with uninstall defaults")
                        return err
                }</span>
                <span class="cov8" title="1">r.Log.Info("Uninstall: Default uninstall annotations has been set on storagecluster")</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// deleteResources is the function where the storageClusterFinalizer is handled
// Every function that is called within this function should be idempotent
func (r *StorageClusterReconciler) deleteResources(sc *ocsv1.StorageCluster) error <span class="cov8" title="1">{

        var obj ocsQuickStarts
        err := obj.ensureDeleted(r, sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">err = r.setRookUninstallandCleanupPolicy(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">err = r.setNoobaaUninstallMode(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">objs := []resourceManager{
                &amp;ocsNoobaaSystem{},
                &amp;ocsCephCluster{},
                &amp;ocsCephObjectStoreUsers{},
                &amp;ocsCephObjectStores{},
                &amp;ocsCephFilesystems{},
                &amp;ocsCephBlockPools{},
                &amp;ocsSnapshotClass{},
                &amp;ocsStorageClass{},
        }

        for _, obj := range objs </span><span class="cov8" title="1">{
                err = obj.ensureDeleted(r, sc)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">err = r.deleteNodeTaint(sc)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // TODO: skip the deletion of these labels till we figure out a way to wait
        // for the cleanup jobs
        //err = r.deleteNodeAffinityKeyFromNodes(sc)
        //if err != nil {
        //        return err
        //}

        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file23" style="display: none">package storagecluster

import (
        "context"
        "fmt"
        "reflect"

        "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/types"

        snapapi "github.com/kubernetes-csi/external-snapshotter/v2/pkg/apis/volumesnapshot/v1beta1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// SnapshotterType represents a snapshotter type
type SnapshotterType string

type ocsSnapshotClass struct{}

const (
        rbdSnapshotter    SnapshotterType = "rbd"
        cephfsSnapshotter SnapshotterType = "cephfs"
)

// secret name and namespace for snapshotter class
const (
        snapshotterSecretName      = "csi.storage.k8s.io/snapshotter-secret-name"
        snapshotterSecretNamespace = "csi.storage.k8s.io/snapshotter-secret-namespace"
)

// SnapshotClassConfiguration provides configuration options for a SnapshotClass.
type SnapshotClassConfiguration struct {
        snapshotClass     *snapapi.VolumeSnapshotClass
        reconcileStrategy ReconcileStrategy
        disable           bool
}

// newVolumeSnapshotClass returns a new VolumeSnapshotter class backed by provided snapshotter type
// available 'snapShotterType' values are 'rbd' and 'cephfs'
func newVolumeSnapshotClass(instance *ocsv1.StorageCluster, snapShotterType SnapshotterType) *snapapi.VolumeSnapshotClass <span class="cov8" title="1">{
        retSC := &amp;snapapi.VolumeSnapshotClass{
                ObjectMeta: metav1.ObjectMeta{
                        Name: generateNameForSnapshotClass(instance, snapShotterType),
                },
                Driver: generateNameForSnapshotClassDriver(instance, snapShotterType),
                Parameters: map[string]string{
                        "clusterID":                instance.Namespace,
                        snapshotterSecretName:      generateNameForSnapshotClassSecret(snapShotterType),
                        snapshotterSecretNamespace: instance.Namespace,
                },
                DeletionPolicy: snapapi.VolumeSnapshotContentDelete,
        }
        return retSC
}</span>

func newCephFilesystemSnapshotClassConfiguration(instance *ocsv1.StorageCluster) SnapshotClassConfiguration <span class="cov8" title="1">{
        return SnapshotClassConfiguration{
                snapshotClass:     newVolumeSnapshotClass(instance, cephfsSnapshotter),
                reconcileStrategy: ReconcileStrategy(instance.Spec.ManagedResources.CephFilesystems.ReconcileStrategy),
                disable:           instance.Spec.ManagedResources.CephFilesystems.DisableSnapshotClass,
        }
}</span>

func newCephBlockPoolSnapshotClassConfiguration(instance *ocsv1.StorageCluster) SnapshotClassConfiguration <span class="cov8" title="1">{
        return SnapshotClassConfiguration{
                snapshotClass:     newVolumeSnapshotClass(instance, rbdSnapshotter),
                reconcileStrategy: ReconcileStrategy(instance.Spec.ManagedResources.CephFilesystems.ReconcileStrategy),
                disable:           instance.Spec.ManagedResources.CephFilesystems.DisableSnapshotClass,
        }
}</span>

// newSnapshotClassConfigurations generates configuration options for Ceph SnapshotClasses.
func newSnapshotClassConfigurations(instance *ocsv1.StorageCluster) []SnapshotClassConfiguration <span class="cov8" title="1">{
        vsccs := []SnapshotClassConfiguration{
                newCephFilesystemSnapshotClassConfiguration(instance),
                newCephBlockPoolSnapshotClassConfiguration(instance),
        }
        return vsccs
}</span>

func (r *StorageClusterReconciler) createSnapshotClasses(vsccs []SnapshotClassConfiguration) error <span class="cov8" title="1">{

        for _, vscc := range vsccs </span><span class="cov8" title="1">{
                if vscc.reconcileStrategy == ReconcileStrategyIgnore || vscc.disable </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov8" title="1">vsc := vscc.snapshotClass
                existing := &amp;snapapi.VolumeSnapshotClass{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: vsc.Name, Namespace: vsc.Namespace}, existing)
                if err != nil </span><span class="cov8" title="1">{
                        if errors.IsNotFound(err) </span><span class="cov8" title="1">{
                                // Since the SnapshotClass is not found, we will create a new one
                                r.Log.Info(fmt.Sprintf("creating SnapshotClass %q", vsc.Name))
                                err = r.Client.Create(context.TODO(), vsc)
                                if err != nil </span><span class="cov0" title="0">{
                                        r.Log.Error(err, fmt.Sprintf("failed to create SnapshotClass %q", vsc.Name))
                                        return err
                                }</span>
                                // no error, continue with the next iteration
                                <span class="cov8" title="1">continue</span>
                        } else<span class="cov0" title="0"> {
                                r.Log.Error(err, fmt.Sprintf("failed to 'Get' SnapshotClass %q", vsc.Name))
                                return err
                        }</span>
                }
                <span class="cov8" title="1">if vscc.reconcileStrategy == ReconcileStrategyInit </span><span class="cov0" title="0">{
                        return nil
                }</span>
                <span class="cov8" title="1">if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to restore snapshotclass %q because it is marked for deletion", existing.Name)
                }</span>
                // if there is a mis-match in the parameters of existing vs created resources,
                <span class="cov8" title="1">if !reflect.DeepEqual(vsc.Parameters, existing.Parameters) </span><span class="cov0" title="0">{
                        // we have to update the existing SnapshotClass
                        r.Log.Info(fmt.Sprintf("SnapshotClass %q needs to be updated", existing.Name))
                        existing.ObjectMeta.OwnerReferences = vsc.ObjectMeta.OwnerReferences
                        vsc.ObjectMeta = existing.ObjectMeta
                        if err := r.Client.Update(context.TODO(), vsc); err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("SnapshotClass %q updation failed", existing.Name))
                                return err
                        }</span>
                }
        }
        <span class="cov8" title="1">return nil</span>
}

// ensureCreated functions ensures that snpashotter classes are created
func (obj *ocsSnapshotClass) ensureCreated(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{
        vsccs := newSnapshotClassConfigurations(instance)

        err := r.createSnapshotClasses(vsccs)
        if err != nil </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// ensureDeleted deletes the SnapshotClasses that the ocs-operator created
func (obj *ocsSnapshotClass) ensureDeleted(r *StorageClusterReconciler, instance *ocsv1.StorageCluster) error <span class="cov8" title="1">{

        vsccs := newSnapshotClassConfigurations(instance)
        for _, vscc := range vsccs </span><span class="cov8" title="1">{
                sc := vscc.snapshotClass
                existing := snapapi.VolumeSnapshotClass{}
                err := r.Client.Get(context.TODO(), types.NamespacedName{Name: sc.Name, Namespace: sc.Namespace}, &amp;existing)

                switch </span>{
                case err == nil:<span class="cov8" title="1">
                        if existing.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                r.Log.Info(fmt.Sprintf("Uninstall: SnapshotClass %s is already marked for deletion", existing.Name))
                                break</span>
                        }

                        <span class="cov8" title="1">r.Log.Info(fmt.Sprintf("Uninstall: Deleting SnapshotClass %s", sc.Name))
                        existing.ObjectMeta.OwnerReferences = sc.ObjectMeta.OwnerReferences
                        sc.ObjectMeta = existing.ObjectMeta

                        err = r.Client.Delete(context.TODO(), sc)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Log.Error(err, fmt.Sprintf("Uninstall: Ignoring error deleting the SnapshotClass %s", existing.Name))
                        }</span>
                case errors.IsNotFound(err):<span class="cov0" title="0">
                        r.Log.Info(fmt.Sprintf("Uninstall: SnapshotClass %s not found, nothing to do", sc.Name))</span>
                default:<span class="cov0" title="0">
                        r.Log.Info(fmt.Sprintf("Uninstall: Error while getting SnapshotClass %s: %v", sc.Name, err))</span>
                }
        }
        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file24" style="display: none">package util

import (
        "fmt"
        "os"
)

// WatchNamespaceEnvVar is the constant for env variable WATCH_NAMESPACE
// which is the namespace where the watch activity happens.
// this value is empty if the operator is running with clusterScope.
const WatchNamespaceEnvVar = "WATCH_NAMESPACE"

// GetWatchNamespace returns the namespace the operator should be watching for changes
func GetWatchNamespace() (string, error) <span class="cov0" title="0">{
        ns, found := os.LookupEnv(WatchNamespaceEnvVar)
        if !found </span><span class="cov0" title="0">{
                return "", fmt.Errorf("%s must be set", WatchNamespaceEnvVar)
        }</span>
        <span class="cov0" title="0">return ns, nil</span>
}
</pre>
		
		<pre class="file" id="file25" style="display: none">package util

import (
        "reflect"

        "sigs.k8s.io/controller-runtime/pkg/event"
        "sigs.k8s.io/controller-runtime/pkg/predicate"
)

// ComposePredicates will compose a variable number of predicates and return a predicate that
// will allow events that are allowed by any of the given predicates.
func ComposePredicates(predicates ...predicate.Predicate) predicate.Predicate <span class="cov8" title="1">{
        return predicate.Funcs{
                CreateFunc: func(e event.CreateEvent) bool </span><span class="cov0" title="0">{
                        for _, p := range predicates </span><span class="cov0" title="0">{
                                if p != nil &amp;&amp; p.Create(e) </span><span class="cov0" title="0">{
                                        return true
                                }</span>
                        }
                        <span class="cov0" title="0">return false</span>
                },
                DeleteFunc: func(e event.DeleteEvent) bool <span class="cov0" title="0">{
                        for _, p := range predicates </span><span class="cov0" title="0">{
                                if p != nil &amp;&amp; p.Delete(e) </span><span class="cov0" title="0">{
                                        return true
                                }</span>
                        }
                        <span class="cov0" title="0">return false</span>
                },
                UpdateFunc: func(e event.UpdateEvent) bool <span class="cov8" title="1">{
                        for _, p := range predicates </span><span class="cov8" title="1">{
                                if p != nil &amp;&amp; p.Update(e) </span><span class="cov8" title="1">{
                                        return true
                                }</span>
                        }
                        <span class="cov8" title="1">return false</span>
                },
                GenericFunc: func(e event.GenericEvent) bool <span class="cov0" title="0">{
                        for _, p := range predicates </span><span class="cov0" title="0">{
                                if p != nil &amp;&amp; p.Generic(e) </span><span class="cov0" title="0">{
                                        return true
                                }</span>
                        }
                        <span class="cov0" title="0">return false</span>
                },
        }
}

// MetadataChangedPredicate will only allow events that changed labels,
// annotations, or finalizers
type MetadataChangedPredicate struct {
        predicate.Funcs
}

// Update implements the update event trap for StorageClusterChangedPredicate
func (p MetadataChangedPredicate) Update(e event.UpdateEvent) bool <span class="cov8" title="1">{
        metaChanged := !reflect.DeepEqual(e.MetaOld.GetLabels(), e.MetaNew.GetLabels()) ||
                !reflect.DeepEqual(e.MetaOld.GetAnnotations(), e.MetaNew.GetAnnotations()) ||
                !reflect.DeepEqual(e.MetaOld.GetFinalizers(), e.MetaNew.GetFinalizers())

        return e.MetaOld != nil &amp;&amp;
                e.MetaNew != nil &amp;&amp;
                metaChanged
}</span>
</pre>
		
		<pre class="file" id="file26" style="display: none">package util

import (
        "os"
)

// FileName represent the full path to the file for determining ready status
const FileName = "/tmp/operator-sdk-ready"

// Ready holds state about whether the operator is ready and communicates that
// to a Kubernetes readiness probe.
type Ready interface {
        // Set ensures that future readiness probes will indicate that the operator
        // is ready.
        Set() error

        // Unset ensures that future readiness probes will indicate that the
        // operator is not ready.
        Unset() error
}

// NewFileReady returns a Ready that uses the presence of a file on disk to
// communicate whether the operator is ready. The operator's Pod definition
// should include a readinessProbe of "exec" type that calls
// "stat /tmp/operator-sdk-ready".
func NewFileReady() Ready <span class="cov0" title="0">{
        return fileReady{}
}</span>

type fileReady struct{}

// Set creates a file on disk whose presence can be used by a readiness probe
// to determine that the operator is ready.
func (r fileReady) Set() error <span class="cov0" title="0">{
        f, err := os.Create(FileName)
        if err != nil </span><span class="cov0" title="0">{
                if os.IsExist(err) </span><span class="cov0" title="0">{
                        return nil
                }</span>
                <span class="cov0" title="0">return err</span>
        }
        <span class="cov0" title="0">return f.Close()</span>
}

// Unset removes the file on disk that was created by Set().
func (r fileReady) Unset() error <span class="cov0" title="0">{
        if _, err := os.Stat(FileName); os.IsNotExist(err) </span><span class="cov0" title="0">{
                return nil
        }</span>
        <span class="cov0" title="0">return os.Remove(FileName)</span>
}
</pre>
		
		<pre class="file" id="file27" style="display: none">package util

import (
        "fmt"

        nbv1 "github.com/noobaa/noobaa-operator/v2/pkg/apis/noobaa/v1alpha1"
        conditionsv1 "github.com/openshift/custom-resource-status/conditions/v1"
        ocsv1 "github.com/openshift/ocs-operator/api/v1"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        corev1 "k8s.io/api/core/v1"
)

// These constants represent the overall Phase as used by .Status.Phase
var (
        // PhaseIgnored is used when a resource is ignored
        PhaseIgnored = "Ignored"
        // PhaseProgressing is used when SetProgressingCondition is called
        PhaseProgressing = "Progressing"
        // PhaseError is used when SetErrorCondition is called
        PhaseError = "Error"
        // PhaseReady is used when SetCompleteCondition is called
        PhaseReady = "Ready"
        // PhaseNotReady is used when waiting for system to be ready
        // after reconcile is successful
        PhaseNotReady = "Not Ready"
        // PhaseClusterExpanding is used when cluster is expanding capacity
        PhaseClusterExpanding = "Expanding Capacity"
        // PhaseDeleting is used when cluster is deleting
        PhaseDeleting = "Deleting"
        // PhaseConnecting is used when cluster is connecting to external cluster
        PhaseConnecting = "Connecting"
)

const (
        // ExternalClusterConnectingReason indicates the storage cluster is trying to connect to an external one.
        ExternalClusterConnectingReason = "ExternalClusterStateConnecting"
        // ExternalClusterUnknownReason is for unknown cluster condition/state
        ExternalClusterUnknownReason = "ExternalClusterStateUnknownCondition"
        // ExternalClusterErrorReason indicates an error state
        ExternalClusterErrorReason = "ExternalClusterStateError"
)

// SetProgressingCondition sets the ProgressingCondition to True and other conditions to
// false or Unknown. Used when we are just starting to reconcile, and there are no existing
// conditions.
func SetProgressingCondition(conditions *[]conditionsv1.Condition, reason string, message string) <span class="cov0" title="0">{
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    ocsv1.ConditionReconcileComplete,
                Status:  corev1.ConditionUnknown,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionAvailable,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionProgressing,
                Status:  corev1.ConditionTrue,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionDegraded,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionUpgradeable,
                Status:  corev1.ConditionUnknown,
                Reason:  reason,
                Message: message,
        })
}</span>

// SetErrorCondition sets the ConditionReconcileComplete to False in case of any errors
// during the reconciliation process.
func SetErrorCondition(conditions *[]conditionsv1.Condition, reason string, message string) <span class="cov0" title="0">{
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    ocsv1.ConditionReconcileComplete,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
}</span>

// SetCompleteCondition sets the ConditionReconcileComplete to True and other Conditions
// to indicate that the reconciliation process has completed successfully.
func SetCompleteCondition(conditions *[]conditionsv1.Condition, reason string, message string) <span class="cov0" title="0">{
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    ocsv1.ConditionReconcileComplete,
                Status:  corev1.ConditionTrue,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionAvailable,
                Status:  corev1.ConditionTrue,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionProgressing,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionDegraded,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionUpgradeable,
                Status:  corev1.ConditionTrue,
                Reason:  reason,
                Message: message,
        })
}</span>

// MapCephClusterNegativeConditions maps the status states from CephCluster resource into ocs status conditions.
// This will only look for negative conditions: !Available, Degraded, Progressing
func MapCephClusterNegativeConditions(conditions *[]conditionsv1.Condition, found *cephv1.CephCluster) <span class="cov0" title="0">{
        switch found.Status.State </span>{
        case cephv1.ClusterStateCreating:<span class="cov0" title="0">
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionProgressing,
                        Status:  corev1.ConditionTrue,
                        Reason:  "ClusterStateCreating",
                        Message: fmt.Sprintf("CephCluster is creating: %v", string(found.Status.Message)),
                })
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionUpgradeable,
                        Status:  corev1.ConditionFalse,
                        Reason:  "ClusterStateCreating",
                        Message: fmt.Sprintf("CephCluster is creating: %v", string(found.Status.Message)),
                })</span>
        case cephv1.ClusterStateUpdating:<span class="cov0" title="0">
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionProgressing,
                        Status:  corev1.ConditionTrue,
                        Reason:  "ClusterStateUpdating",
                        Message: fmt.Sprintf("CephCluster is updating: %v", string(found.Status.Message)),
                })
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionUpgradeable,
                        Status:  corev1.ConditionFalse,
                        Reason:  "ClusterStateUpdating",
                        Message: fmt.Sprintf("CephCluster is updating: %v", string(found.Status.Message)),
                })</span>
        case cephv1.ClusterStateError:<span class="cov0" title="0">
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionAvailable,
                        Status:  corev1.ConditionFalse,
                        Reason:  "ClusterStateError",
                        Message: fmt.Sprintf("CephCluster error: %v", string(found.Status.Message)),
                })
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionDegraded,
                        Status:  corev1.ConditionTrue,
                        Reason:  "ClusterStateError",
                        Message: fmt.Sprintf("CephCluster error: %v", string(found.Status.Message)),
                })</span>
        }
}

// MapExternalCephClusterNegativeConditions maps the status states from CephCluster resource into ocs status conditions.
// This will only look for negative conditions: !Available, Degraded, Progressing
func MapExternalCephClusterNegativeConditions(conditions *[]conditionsv1.Condition, found *cephv1.CephCluster) <span class="cov0" title="0">{
        switch found.Status.State </span>{
        case cephv1.ClusterStateConnecting:<span class="cov0" title="0">

                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    ocsv1.ConditionExternalClusterConnecting,
                        Status:  corev1.ConditionTrue,
                        Reason:  ExternalClusterConnectingReason,
                        Message: fmt.Sprintf("External CephCluster is trying to connect: %v", found.Status.Message),
                })
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    ocsv1.ConditionExternalClusterConnected,
                        Status:  corev1.ConditionFalse,
                        Reason:  ExternalClusterConnectingReason,
                        Message: fmt.Sprintf("External CephCluster is trying to connect: %v", found.Status.Message),
                })</span>
        case cephv1.ClusterStateError:<span class="cov0" title="0">
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    ocsv1.ConditionExternalClusterConnected,
                        Status:  corev1.ConditionFalse,
                        Reason:  ExternalClusterErrorReason,
                        Message: fmt.Sprintf("External CephCluster error: %v", string(found.Status.Message)),
                })
                conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                        Type:    ocsv1.ConditionExternalClusterConnecting,
                        Status:  corev1.ConditionFalse,
                        Reason:  ExternalClusterErrorReason,
                        Message: fmt.Sprintf("External CephCluster error: %v", string(found.Status.Message)),
                })</span>
        }
}

// MapCephClusterNoConditions sets status conditions to progressing. Used when component operator isn't
// reporting any status, and we have to assume progress.
func MapCephClusterNoConditions(conditions *[]conditionsv1.Condition, reason string, message string) <span class="cov0" title="0">{
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionAvailable,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionProgressing,
                Status:  corev1.ConditionTrue,
                Reason:  reason,
                Message: message,
        })
        conditionsv1.SetStatusCondition(conditions, conditionsv1.Condition{
                Type:    conditionsv1.ConditionUpgradeable,
                Status:  corev1.ConditionFalse,
                Reason:  reason,
                Message: message,
        })
}</span>

// won't override a status condition of the same type and status
func setStatusConditionIfNotPresent(conditions *[]conditionsv1.Condition, condition conditionsv1.Condition) <span class="cov0" title="0">{

        foundCondition := conditionsv1.FindStatusCondition(*conditions, condition.Type)
        if foundCondition != nil &amp;&amp; foundCondition.Status == condition.Status </span><span class="cov0" title="0">{
                // already exists
                return
        }</span>

        <span class="cov0" title="0">conditionsv1.SetStatusCondition(conditions, condition)</span>
}

// MapNoobaaNegativeConditions records noobaa related conditions
// This will only look for negative conditions: !Available, Degraded, Progressing
func MapNoobaaNegativeConditions(conditions *[]conditionsv1.Condition, found *nbv1.NooBaa) <span class="cov0" title="0">{

        if found == nil </span><span class="cov0" title="0">{
                setStatusConditionIfNotPresent(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionDegraded,
                        Status:  corev1.ConditionTrue,
                        Reason:  "NoobaaNotFound",
                        Message: fmt.Sprintf("Waiting on Nooba instance creation"), //nolint:gosimple
                })
                return
        }</span>

        <span class="cov0" title="0">switch found.Status.Phase </span>{
        case nbv1.SystemPhaseRejected:<span class="cov0" title="0">
                setStatusConditionIfNotPresent(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionDegraded,
                        Status:  corev1.ConditionTrue,
                        Reason:  "NoobaaSpecRejected",
                        Message: fmt.Sprintf("Noobaa object's configuration is rejected by the noobaa operator"), //nolint:gosimple
                })</span>
        case "", nbv1.SystemPhaseVerifying, nbv1.SystemPhaseCreating, nbv1.SystemPhaseConnecting, nbv1.SystemPhaseConfiguring:<span class="cov0" title="0">
                setStatusConditionIfNotPresent(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionProgressing,
                        Status:  corev1.ConditionTrue,
                        Reason:  "NoobaaInitializing",
                        Message: fmt.Sprintf("Waiting on Nooba instance to finish initialization"), //nolint:gosimple
                })</span>
        case nbv1.SystemPhaseReady:<span class="cov0" title="0"></span>
                // no-op. Ready isn't a negative case
        default:<span class="cov0" title="0">
                setStatusConditionIfNotPresent(conditions, conditionsv1.Condition{
                        Type:    conditionsv1.ConditionDegraded,
                        Status:  corev1.ConditionTrue,
                        Reason:  "NoobaaPhaseUnknown",
                        Message: fmt.Sprintf("Noobaa phase %s is unknown", found.Status.Phase),
                })</span>
        }

}
</pre>
		
		<pre class="file" id="file28" style="display: none">package collectors

import (
        "github.com/openshift/ocs-operator/metrics/internal/options"
        "github.com/prometheus/client_golang/prometheus"
        cephv1 "github.com/rook/rook/pkg/apis/ceph.rook.io/v1"
        rookclient "github.com/rook/rook/pkg/client/clientset/versioned"
        cephv1listers "github.com/rook/rook/pkg/client/listers/ceph.rook.io/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/fields"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/client-go/tools/cache"
        "k8s.io/klog"
)

const (
        // name of the project/exporter
        namespace = "ocs"
        // component within the project/exporter
        subsystem = "rgw"
)

var _ prometheus.Collector = &amp;CephObjectStoreCollector{}

// CephObjectStoreCollector is a custom collector for CephObjectStore Custom Resource
type CephObjectStoreCollector struct {
        RGWHealthStatus   *prometheus.Desc
        Informer          cache.SharedIndexInformer
        AllowedNamespaces []string
}

// NewCephObjectStoreCollector constructs a collector
func NewCephObjectStoreCollector(opts *options.Options) *CephObjectStoreCollector <span class="cov8" title="1">{
        client, err := rookclient.NewForConfig(opts.Kubeconfig)
        if err != nil </span><span class="cov0" title="0">{
                klog.Error(err)
        }</span>

        <span class="cov8" title="1">lw := cache.NewListWatchFromClient(client.CephV1().RESTClient(), "cephobjectstores", metav1.NamespaceAll, fields.Everything())
        sharedIndexInformer := cache.NewSharedIndexInformer(lw, &amp;cephv1.CephObjectStore{}, 0, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})

        return &amp;CephObjectStoreCollector{
                RGWHealthStatus: prometheus.NewDesc(
                        prometheus.BuildFQName(namespace, subsystem, "health_status"),
                        `Health Status of RGW Endpoint. 0=Connected, 1=Progressing &amp; 2=Failure`,
                        []string{"name", "namespace", "rgw_endpoint"},
                        nil,
                ),
                Informer:          sharedIndexInformer,
                AllowedNamespaces: opts.AllowedNamespaces,
        }</span>
}

// Run starts CephObjectStore informer
func (c *CephObjectStoreCollector) Run(stopCh &lt;-chan struct{}) <span class="cov0" title="0">{
        go c.Informer.Run(stopCh)
}</span>

// Describe implements prometheus.Collector interface
func (c *CephObjectStoreCollector) Describe(ch chan&lt;- *prometheus.Desc) <span class="cov0" title="0">{
        ds := []*prometheus.Desc{
                c.RGWHealthStatus,
        }

        for _, d := range ds </span><span class="cov0" title="0">{
                ch &lt;- d
        }</span>
}

// Collect implements prometheus.Collector interface
func (c *CephObjectStoreCollector) Collect(ch chan&lt;- prometheus.Metric) <span class="cov0" title="0">{
        cephObjectStoreLister := cephv1listers.NewCephObjectStoreLister(c.Informer.GetIndexer())
        cephObjectStores := getAllObjectStores(cephObjectStoreLister, c.AllowedNamespaces)

        if len(cephObjectStores) &gt; 0 </span><span class="cov0" title="0">{
                c.collectObjectStoreHealth(cephObjectStores, ch)
        }</span>
}

func getAllObjectStores(lister cephv1listers.CephObjectStoreLister, namespaces []string) (cephObjectStores []*cephv1.CephObjectStore) <span class="cov8" title="1">{
        var tempCephObjectStores []*cephv1.CephObjectStore
        var err error
        if len(namespaces) == 0 </span><span class="cov0" title="0">{
                cephObjectStores, err = lister.List(labels.Everything())
                if err != nil </span><span class="cov0" title="0">{
                        klog.Errorf("couldn't list CephObjectStores. %v", err)
                }</span>
                <span class="cov0" title="0">return</span>
        }
        <span class="cov8" title="1">for _, namespace := range namespaces </span><span class="cov8" title="1">{
                tempCephObjectStores, err = lister.CephObjectStores(namespace).List(labels.Everything())
                if err != nil </span><span class="cov0" title="0">{
                        klog.Errorf("couldn't list CephObjectStores in namespace %s. %v", namespace, err)
                        continue</span>
                }
                <span class="cov8" title="1">cephObjectStores = append(cephObjectStores, tempCephObjectStores...)</span>
        }
        <span class="cov8" title="1">return</span>
}

func (c *CephObjectStoreCollector) collectObjectStoreHealth(cephObjectStores []*cephv1.CephObjectStore, ch chan&lt;- prometheus.Metric) <span class="cov8" title="1">{
        for _, cephObjectStore := range cephObjectStores </span><span class="cov8" title="1">{
                switch cephObjectStore.Status.Phase </span>{
                case cephv1.ConditionConnected:<span class="cov8" title="1">
                        ch &lt;- prometheus.MustNewConstMetric(c.RGWHealthStatus,
                                prometheus.GaugeValue, 0,
                                cephObjectStore.Name,
                                cephObjectStore.Namespace,
                                cephObjectStore.Status.Info["endpoint"])</span>
                case cephv1.ConditionProgressing:<span class="cov8" title="1">
                        ch &lt;- prometheus.MustNewConstMetric(c.RGWHealthStatus,
                                prometheus.GaugeValue, 1,
                                cephObjectStore.Name,
                                cephObjectStore.Namespace,
                                cephObjectStore.Status.Info["endpoint"])</span>
                case cephv1.ConditionFailure:<span class="cov8" title="1">
                        ch &lt;- prometheus.MustNewConstMetric(c.RGWHealthStatus,
                                prometheus.GaugeValue, 2,
                                cephObjectStore.Name,
                                cephObjectStore.Namespace,
                                cephObjectStore.Status.Info["endpoint"])</span>
                default:<span class="cov8" title="1">
                        klog.Errorf("CephObjectStore in unexpected phase. Must be %q, %q or %q",
                                cephv1.ConditionConnected, cephv1.ConditionProgressing, cephv1.ConditionFailure)</span>
                }
        }
}
</pre>
		
		<pre class="file" id="file29" style="display: none">package collectors

import (
        "github.com/openshift/ocs-operator/metrics/internal/options"
        "github.com/prometheus/client_golang/prometheus"
)

// RegisterCustomResourceCollectors registers the custom resource collectors
// in the given prometheus.Registry
// This is used to expose metrics about the Custom Resources
func RegisterCustomResourceCollectors(registry *prometheus.Registry, opts *options.Options) <span class="cov0" title="0">{
        cephObjectStoreCollector := NewCephObjectStoreCollector(opts)
        cephObjectStoreCollector.Run(opts.StopCh)
        registry.MustRegister(
                cephObjectStoreCollector,
        )
}</span>
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
